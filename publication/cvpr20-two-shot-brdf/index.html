<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&family=Cutive+Mono&family=Raleway:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&family=Cutive+Mono&family=Raleway:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="S4KoopgNiTG4YgetELfTyLoFaDS9bHbC3gJBOdfft3o"><meta name=msvalidate.01 content="2625B23426FAE2D4025878BAF046F9A0"><meta name=author content="Mark Boss"><meta name=description content="Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach."><link rel=alternate hreflang=en-us href=https://markboss.me/publication/cvpr20-two-shot-brdf/><meta name=theme-color content="#009688"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=/css/wowchemy.6afeb9a4c9fd434ee6cfaa6ff54e51c4.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://markboss.me/publication/cvpr20-two-shot-brdf/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@markb_boss"><meta property="twitter:creator" content="@markb_boss"><meta property="og:site_name" content="Mark Boss"><meta property="og:url" content="https://markboss.me/publication/cvpr20-two-shot-brdf/"><meta property="og:title" content="Two-shot Spatially-varying BRDF and Shape Estimation | Mark Boss"><meta property="og:description" content="Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach."><meta property="og:image" content="https://markboss.me/publication/cvpr20-two-shot-brdf/featured.jpg"><meta property="twitter:image" content="https://markboss.me/publication/cvpr20-two-shot-brdf/featured.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2017-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-06-16T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://markboss.me/publication/cvpr20-two-shot-brdf/"},"headline":"Two-shot Spatially-varying BRDF and Shape Estimation","image":["https://markboss.me/publication/cvpr20-two-shot-brdf/featured.jpg"],"datePublished":"2017-01-01T00:00:00Z","dateModified":"2020-06-16T00:00:00Z","author":{"@type":"Person","name":"Mark Boss"},"publisher":{"@type":"Organization","name":"Mark Boss","logo":{"@type":"ImageObject","url":"https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_192x192_fill_lanczos_center_3.png"}},"description":"Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach."}</script><title>Two-shot Spatially-varying BRDF and Shape Estimation | Mark Boss</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=876f8c22bd2dde8478d06100c11e546b><script src=/js/wowchemy-init.min.e359544023addbb017e703b8a0938d12.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Mark Boss</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Mark Boss</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publication/><span>All Publications</span></a>
<a class=dropdown-item href=/#featured_publications><span>Featured Publications</span></a></div></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/files/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Two-shot Spatially-varying BRDF and Shape Estimation</h1><div class=article-metadata><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span>, <span><a href=/author/kihwan-kim/>Kihwan Kim</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span>, <span><a href=/author/jan-kautz/>Jan Kautz</a></span></div><span class=article-date>June, 2020</span></div><div class="btn-links mb-3"><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/cvpr20-two-shot-brdf/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=/files/cvpr20-two-shot-brdf.pdf><i class="fas fa-file-pdf mr-1"></i>Paper + Supplementary</a>
<a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2004.00403 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>arXiv</a>
<a class="btn btn-outline-primary btn-page-header" href=https://youtu.be/CyC6PutoJO8 target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/NVlabs/two-shot-brdf-shape target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a>
<a class="btn btn-outline-primary btn-page-header" href="https://drive.google.com/file/d/14mou3Va65deimPYE5GtFdK8OS3I0BSzq/view?usp=sharing" target=_blank rel=noopener><i class="fas fa-database mr-1"></i>Dataset</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:313px><div style=position:relative><img src=/publication/cvpr20-two-shot-brdf/featured_hu30910665284931ace4f57faa1e01d828_139209_720x2500_fit_q75_h2_lanczos.webp width=720 height=313 alt="Overview of the network architecture" class=featured-image>
<span class=article-header-caption>The proposed cascaded network architectures.</span></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>IEEE Conference on Computer Vision and Pattern Recognition</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/CyC6PutoJO8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=results>Results</h3><div class="alert alert-note"><div>Click the images for an interactive 3D visualization.</div></div><h4 id=aksoy-et-al---a-dataset-of-flash-and-ambient-illumination-pairs-from-the-crowd>Aksoy et al. - A Dataset of Flash and Ambient Illumination Pairs from the Crowd</h4><div class=gallery><a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf0" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf0/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf1" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf1/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf2" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf2/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf3" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf3/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf4" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf4/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf5" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf5/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf6" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf6/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf7" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf7/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf8" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf8/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf9" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf9/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf10" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf10/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf11" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf11/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf12" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf12/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf13" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf13/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf14" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf14/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=fnf15" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/fnf15/input_flash.jpg alt></a></div><h5 id=real-world-examples>Real-world Examples</h5><div class=gallery><a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=rw0" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/rw0/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=rw1" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/rw1/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=rw2" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/rw2/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=rw3" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/rw3/input_flash.jpg alt></a></div><h5 id=synthetic-examples>Synthetic Examples</h5><div class=gallery><a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=syn0" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/syn0/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=syn1" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/syn1/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=syn2" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/syn2/input_flash.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/cvpr20-results/renderer.html?mat=syn3" href=javascript:;><img class=fixed-width-img src=/files/cvpr20-results/predictions/syn3/input_flash.jpg alt></a></div><h3 id=copyright>Copyright</h3><p>This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to <a href=mailto:pubs-permissions@ieee.org>pubs-permissions@ieee.org</a>.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/material-acquisition/>Material Acquisition</a>
<a class="badge badge-light" href=/tag/machine-learning/>Machine Learning</a>
<a class="badge badge-light" href=/tag/two-shot/>Two-Shot</a>
<a class="badge badge-light" href=/tag/shape/>Shape</a>
<a class="badge badge-light" href=/tag/depth-map/>Depth Map</a>
<a class="badge badge-light" href=/tag/svbrdf/>SVBRDF</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://markboss.me/publication/cvpr20-two-shot-brdf/&text=Two-shot%20Spatially-varying%20BRDF%20and%20Shape%20Estimation" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://markboss.me/publication/cvpr20-two-shot-brdf/&t=Two-shot%20Spatially-varying%20BRDF%20and%20Shape%20Estimation" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Two-shot%20Spatially-varying%20BRDF%20and%20Shape%20Estimation&body=https://markboss.me/publication/cvpr20-two-shot-brdf/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://markboss.me/publication/cvpr20-two-shot-brdf/&title=Two-shot%20Spatially-varying%20BRDF%20and%20Shape%20Estimation" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Two-shot%20Spatially-varying%20BRDF%20and%20Shape%20Estimation%20https://markboss.me/publication/cvpr20-two-shot-brdf/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://markboss.me/publication/cvpr20-two-shot-brdf/&title=Two-shot%20Spatially-varying%20BRDF%20and%20Shape%20Estimation" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://markboss.me/><img class="avatar mr-3 avatar-circle" src=/author/mark-boss/avatar_hu8228efeb083742449721d94a9befbadd_1942861_270x270_fill_q75_lanczos_center.jpg alt="Mark Boss"></a><div class=media-body><h5 class=card-title><a href=https://markboss.me/>Mark Boss</a></h5><h6 class=card-subtitle>Ph.D. Student</h6><p class=card-text>I&rsquo;m a Ph.D. student at the University of Tübingen with research interests in the intersection of machine learning and computer graphics.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:hello@markboss.me><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/markb_boss target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=y23cQ6wAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/vork target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/markbboss target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=/files/cv.pdf><i class="ai ai-cv"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/privacy/>Privacy Policy</a>
&#183;
<a href=/terms/>Legal details</a></p><p class="powered-by copyright-license-text">© 2022 Mark Boss. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.3d946de2e8784a477845261d87025092.js></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/elm.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/kotlin.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/rust.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.e8fd2d733eef6a8bbbe0539398fc0547.js type=module></script>
<script src=/en/js/wowchemy.min.de33c90527762392d440555c36739dd9.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.b0d291ed6d27eacec233e6cf5204f99a.js type=module></script></body></html>