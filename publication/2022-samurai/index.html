<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&family=Cutive+Mono&family=Raleway:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&family=Cutive+Mono&family=Raleway:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="S4KoopgNiTG4YgetELfTyLoFaDS9bHbC3gJBOdfft3o"><meta name=msvalidate.01 content="2625B23426FAE2D4025878BAF046F9A0"><meta name=author content="Mark Boss"><meta name=description content="Inverse rendering of an object under entirely unknown capture conditions is a fundamental challenge in computer vision and graphics. Neural approaches such as NeRF have achieved photorealistic results on novel view synthesis, but they require known camera poses. Solving this problem with unknown camera poses is highly challenging as it requires joint optimization over shape, radiance, and pose. This problem is exacerbated when the input images are captured in the wild with varying backgrounds and illuminations. In such image collections in the wild, standard pose estimation techniques fail due to very few estimated correspondences across images. Furthermore, NeRF cannot relight a scene under any illumination, as it operates on radiance (the product of reflectance and illumination). We propose a joint optimization framework to estimate the shape,  BRDF, and per-image camera pose and illumination. Our method works on in-the-wild online image collections of an object and produces relightable 3D assets for several use-cases such as AR/VR. To our knowledge, our method is the first to tackle this severely unconstrained task with minimal user interaction."><link rel=alternate hreflang=en-us href=https://markboss.me/publication/2022-samurai/><meta name=theme-color content="#009688"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.51552f52995db288c096f6cea62018c9.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://markboss.me/publication/2022-samurai/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@markb_boss"><meta property="twitter:creator" content="@markb_boss"><meta property="og:site_name" content="Mark Boss"><meta property="og:url" content="https://markboss.me/publication/2022-samurai/"><meta property="og:title" content="SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections | Mark Boss"><meta property="og:description" content="Inverse rendering of an object under entirely unknown capture conditions is a fundamental challenge in computer vision and graphics. Neural approaches such as NeRF have achieved photorealistic results on novel view synthesis, but they require known camera poses. Solving this problem with unknown camera poses is highly challenging as it requires joint optimization over shape, radiance, and pose. This problem is exacerbated when the input images are captured in the wild with varying backgrounds and illuminations. In such image collections in the wild, standard pose estimation techniques fail due to very few estimated correspondences across images. Furthermore, NeRF cannot relight a scene under any illumination, as it operates on radiance (the product of reflectance and illumination). We propose a joint optimization framework to estimate the shape,  BRDF, and per-image camera pose and illumination. Our method works on in-the-wild online image collections of an object and produces relightable 3D assets for several use-cases such as AR/VR. To our knowledge, our method is the first to tackle this severely unconstrained task with minimal user interaction."><meta property="og:image" content="https://markboss.me/publication/2022-samurai/featured.jpg"><meta property="twitter:image" content="https://markboss.me/publication/2022-samurai/featured.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-05-26T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-31T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://markboss.me/publication/2022-samurai/"},"headline":"SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections","image":["https://markboss.me/publication/2022-samurai/featured.jpg"],"datePublished":"2022-05-26T00:00:00Z","dateModified":"2022-05-31T00:00:00Z","author":{"@type":"Person","name":"Mark Boss"},"publisher":{"@type":"Organization","name":"Mark Boss","logo":{"@type":"ImageObject","url":"https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_192x192_fill_lanczos_center_3.png"}},"description":"Inverse rendering of an object under entirely unknown capture conditions is a fundamental challenge in computer vision and graphics. Neural approaches such as NeRF have achieved photorealistic results on novel view synthesis, but they require known camera poses. Solving this problem with unknown camera poses is highly challenging as it requires joint optimization over shape, radiance, and pose. This problem is exacerbated when the input images are captured in the wild with varying backgrounds and illuminations. In such image collections in the wild, standard pose estimation techniques fail due to very few estimated correspondences across images. Furthermore, NeRF cannot relight a scene under any illumination, as it operates on radiance (the product of reflectance and illumination). We propose a joint optimization framework to estimate the shape,  BRDF, and per-image camera pose and illumination. Our method works on in-the-wild online image collections of an object and produces relightable 3D assets for several use-cases such as AR/VR. To our knowledge, our method is the first to tackle this severely unconstrained task with minimal user interaction."}</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=all onload='this.media="all"'><title>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections | Mark Boss</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=26a5c0a05e4a19ccc5e0d8813f08f8eb><script src=/js/wowchemy-init.min.e359544023addbb017e703b8a0938d12.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Mark Boss</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Mark Boss</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Publications</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/publication/><span>All Publications</span></a>
<a class=dropdown-item href=/#featured_publications><span>Featured Publications</span></a></div></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/files/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</h1><div class=article-metadata><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/andreas-engelhardt/>Andreas Engelhardt</a></span>, <span><a href=/author/abhishek-kar/>Abhishek Kar</a></span>, <span><a href=/author/yuanzhen-li/>Yuanzhen Li</a></span>, <span><a href=/author/deqing-sun/>Deqing Sun</a></span>, <span><a href=/author/jonathan-t.-barron/>Jonathan T. Barron</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span></div><span class=article-date>May, 2022</span></div><div class="btn-links mb-3"><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/2022-samurai/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=/files/samurai.pdf><i class="fas fa-file-pdf mr-1"></i>Paper + Supplementary</a>
<a class="btn btn-outline-primary btn-page-header" href=https://youtu.be/LlYuGDjXp-8 target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:265px><div style=position:relative><img src=/publication/2022-samurai/featured_hueebc0576a6a37e9404d18d9e27e81796_130114_720x2500_fit_q75_h2_lanczos.webp width=720 height=265 alt="Overview of the method. SAMURAI decomposes a scene from multiple roughly posed input images into a neural volume with explicit BRDFs. Having an explicit decomposition allows us several applications such as material editing, AR, and levearging our objects in games or movies." class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Inverse rendering of an object under entirely unknown capture conditions is a fundamental challenge in computer vision and graphics. Neural approaches such as NeRF have achieved photorealistic results on novel view synthesis, but they require known camera poses. Solving this problem with unknown camera poses is highly challenging as it requires joint optimization over shape, radiance, and pose. This problem is exacerbated when the input images are captured in the wild with varying backgrounds and illuminations. In such image collections in the wild, standard pose estimation techniques fail due to very few estimated correspondences across images. Furthermore, NeRF cannot relight a scene under any illumination, as it operates on radiance (the product of reflectance and illumination). We propose a joint optimization framework to estimate the shape, BRDF, and per-image camera pose and illumination. Our method works on in-the-wild online image collections of an object and produces relightable 3D assets for several use-cases such as AR/VR. To our knowledge, our method is the first to tackle this severely unconstrained task with minimal user interaction.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#3>Preprint</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/LlYuGDjXp-8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=introduction>Introduction</h3><p>Our previous methods such as <a href=/publication/2021-nerd/>NeRD</a> and <a href=/publication/2021-neural-pil/>Neural-PIL</a> achieve the decomposition of images under varying illumination into shape, BRDF, and illumination. However, both methods require near-perfect known poses. In challenging scenes recovering poses is challenging and traditional methods fail with objects captured under varying illuminations and locations.</p><h2 id=method>Method</h2><figure id=figure-the-samurai-architecture><div class="d-flex justify-content-center"><div class=w-100><img alt="The SAMURAI architecture." srcset="/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_f0fbbf61d7097a49e26b7975fffcfb94.webp 400w,
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_9e4d72244c945061262711b0f3a8d3cd.webp 760w,
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_f0fbbf61d7097a49e26b7975fffcfb94.webp width=760 height=248 loading=lazy data-zoomable></div></div><figcaption data-pre=Figure&nbsp; data-post=:&nbsp; class=numbered>The SAMURAI architecture.</figcaption></figure><p>In <a href=#figure-the-samurai-architecture><strong>FIGURE 1</strong></a> we visualize the SAMURAI architecture, which jointly optimizes the camera extrinsic and intrinsic parameters per image, the global shape and BRDF, as well as the per-image illumination latent variables. Here, we leverage <a href=/publication/2021-neural-pil/>Neural-PIL</a> for the rendering and prior on natural illuminations.</p><h2 id=results>Results</h2><div class="alert alert-note"><div>Click the images for an interactive 3D visualization.</div></div><h4 id=samurai-dataset>SAMURAI-Dataset</h4><div class=gallery><a data-fancybox data-type=iframe data-src="/files/samurai-results/render.html?scene=duck" href=javascript:;><img class=fixed-width-img src=/files/samurai-results/assets/models/duck.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/samurai-results/render.html?scene=fireengine" href=javascript:;><img class=fixed-width-img src=/files/samurai-results/assets/models/fireengine.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/samurai-results/render.html?scene=garbagetruck" href=javascript:;><img class=fixed-width-img src=/files/samurai-results/assets/models/garbagetruck.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/samurai-results/render.html?scene=keywest" href=javascript:;><img class=fixed-width-img src=/files/samurai-results/assets/models/keywest.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/samurai-results/render.html?scene=pumpkin" href=javascript:;><img class=fixed-width-img src=/files/samurai-results/assets/models/pumpkin.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/samurai-results/render.html?scene=rccar" href=javascript:;><img class=fixed-width-img src=/files/samurai-results/assets/models/rccar.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/samurai-results/render.html?scene=robot" href=javascript:;><img class=fixed-width-img src=/files/samurai-results/assets/models/robot.jpg alt></a></div></div><div class=article-tags><a class="badge badge-light" href=/tag/material-acquisition/>Material Acquisition</a>
<a class="badge badge-light" href=/tag/shape/>Shape</a>
<a class="badge badge-light" href=/tag/machine-learning/>Machine Learning</a>
<a class="badge badge-light" href=/tag/optimization/>Optimization</a>
<a class="badge badge-light" href=/tag/svbrdf/>SVBRDF</a>
<a class="badge badge-light" href=/tag/neural-rendering/>Neural Rendering</a>
<a class="badge badge-light" href=/tag/camera-pose-estimation/>Camera Pose Estimation</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://markboss.me/publication/2022-samurai/&text=SAMURAI:%20Shape%20And%20Material%20from%20Unconstrained%20Real-world%20Arbitrary%20Image%20collections" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://markboss.me/publication/2022-samurai/&t=SAMURAI:%20Shape%20And%20Material%20from%20Unconstrained%20Real-world%20Arbitrary%20Image%20collections" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=SAMURAI:%20Shape%20And%20Material%20from%20Unconstrained%20Real-world%20Arbitrary%20Image%20collections&body=https://markboss.me/publication/2022-samurai/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://markboss.me/publication/2022-samurai/&title=SAMURAI:%20Shape%20And%20Material%20from%20Unconstrained%20Real-world%20Arbitrary%20Image%20collections" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=SAMURAI:%20Shape%20And%20Material%20from%20Unconstrained%20Real-world%20Arbitrary%20Image%20collections%20https://markboss.me/publication/2022-samurai/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://markboss.me/publication/2022-samurai/&title=SAMURAI:%20Shape%20And%20Material%20from%20Unconstrained%20Real-world%20Arbitrary%20Image%20collections" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://markboss.me/><img class="avatar mr-3 avatar-circle" src=/author/mark-boss/avatar_hu8228efeb083742449721d94a9befbadd_1942861_270x270_fill_q75_lanczos_center.jpg alt="Mark Boss"></a><div class=media-body><h5 class=card-title><a href=https://markboss.me/>Mark Boss</a></h5><h6 class=card-subtitle>Ph.D. Student</h6><p class=card-text>I&rsquo;m a Ph.D. student at the University of Tübingen with research interests in the intersection of machine learning and computer graphics.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:hello@markboss.me><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/markb_boss target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=y23cQ6wAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/vork target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/markbboss target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=/files/cv.pdf><i class="ai ai-cv"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/privacy/>Privacy Policy</a>
&#183;
<a href=/terms/>Legal details</a></p><p class="powered-by copyright-license-text">© 2022 Mark Boss. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.92d2024afaa4dce0cad42ba360879ce9.js></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.e8fd2d733eef6a8bbbe0539398fc0547.js type=module></script>
<script src=/en/js/wowchemy.min.671654b80131033fb70f23abba52991a.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script></body></html>