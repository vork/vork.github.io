<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.2 for Hugo"><meta name=author content="Mark Boss"><meta name=description content="Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed to high-quality models."><link rel=alternate hreflang=en-us href=https://markboss.me/publication/2021-nerd/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#009688"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload="this.media='all'"><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Raleway:400,700%7CBitter:400,400italic,700%7CCutive+Mono&display=swap"><link rel=stylesheet href=/css/wowchemy.4d30c87659a1163e2d055fee89851971.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hube1743d0a4940c76c300ec8349475861_6659_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hube1743d0a4940c76c300ec8349475861_6659_180x180_fill_lanczos_center_2.png><link rel=canonical href=https://markboss.me/publication/2021-nerd/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Mark Boss"><meta property="og:url" content="https://markboss.me/publication/2021-nerd/"><meta property="og:title" content="NeRD: Neural Reflectance Decomposition from Image Collections | Mark Boss"><meta property="og:description" content="Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed to high-quality models."><meta property="og:image" content="https://markboss.me/publication/2021-nerd/featured.jpg"><meta property="twitter:image" content="https://markboss.me/publication/2021-nerd/featured.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-11-27T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://markboss.me/publication/2021-nerd/"},"headline":"NeRD: Neural Reflectance Decomposition from Image Collections","image":["https://markboss.me/publication/2021-nerd/featured.jpg"],"datePublished":"2020-01-01T00:00:00Z","dateModified":"2020-11-27T00:00:00Z","author":{"@type":"Person","name":"Mark Boss"},"publisher":{"@type":"Organization","name":"Mark Boss","logo":{"@type":"ImageObject","url":"https://markboss.me/images/icon_hube1743d0a4940c76c300ec8349475861_6659_192x192_fill_lanczos_center_2.png"}},"description":"Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed to high-quality models."}</script><meta name=google-site-verification content="S4KoopgNiTG4YgetELfTyLoFaDS9bHbC3gJBOdfft3o"><title>NeRD: Neural Reflectance Decomposition from Image Collections | Mark Boss</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.962cb51d7217082ade044488dc9fb47e.js></script><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Mark Boss</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Mark Boss</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured_publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>NeRD: Neural Reflectance Decomposition from Image Collections</h1><div class=article-metadata><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/raphael-braun/>Raphael Braun</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span>, <span><a href=/author/jonathan-t.-barron/>Jonathan T. Barron</a></span>, <span><a href=/author/ce-liu/>Ce Liu</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span></div><span class=article-date>November 2020</span></div><div class="btn-links mb-3"><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/2021-nerd/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2012.03918 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>arXiv</a>
<a class="btn btn-outline-primary btn-page-header" href=https://youtu.be/JL-qMTXw9VU target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:242px><div style=position:relative><img src=/publication/2021-nerd/featured_hu9ce3dee256724e8b0641511e91e45d8f_249226_720x0_resize_q80_lanczos.jpg alt="Overview of the method. NeRD decomposes a scene from multiple input images into a neural volume with explicit BRDFs. The information can be extracted to traditional textured meshes." class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed to high-quality models.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#3>Preprint</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>ArXiv</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/JL-qMTXw9VU style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=introduction>Introduction</h3><p>NeRD is a novel method that can decompose image collections from multiple views taken under varying or fixed illumination conditions. The object can be rotated, or the camera can turn around the object. The result is a neural volume with an explicit representation of the appearance and illumination in the form of the BRDF and Spherical Gaussian (SG) environment illumination.</p><p>The method is based on the general structure of <a href=https://www.matthewtancik.com/nerf target=_blank rel=noopener>NeRF</a>. However, NeRF encodes the scene to an implicit BRDF representation where a Multi-Layer-Perceptron (MLP) is queried for outgoing view directions at every point. Extracting information from NeRF is therefore not easily done, and the inference time for novel views takes around 30 seconds. Also, NeRF is not capable of relighting an object under any illumination. By introducing physically-based representations for lighting and appearance, NeRD can relighting an object, and information can be extracted from the neural volume. After our extraction process, the result is a regular texture mesh that can be rendered in real-time. See our <a href=#results>results</a> where we provide a web-based interactive renderer.</p><h3 id=method>Method</h3><p>Decomposing the scene requires that the integral over the hemisphere from the rendering equation is decomposed into its parts. Here, we use a simplified version without self-emittance.
$$L_o(x,\omega_o) = \int_\Omega L_i(x,\omega_i) f_r(x,\omega_i,\omega_o) (\omega_i \cdot n) d\omega_i$$
Here, $L_o$ is the outgoing radiance for a point $x$ in the direction $\omega_o$. This radiance is calculated by integrating all influences over the hemisphere $\Omega$, which are based on the incoming light $L_i$ for each direction $\omega_i$. The surface behavior is expressed as the BRDF $f_r$, which describes how incoming light $\omega_i$ is directed to the outgoing direction $\omega_o$. Lastly, a cosine term is used $(\omega_i \cdot n)$, which reduces the received light based on the angle towards the light source.</p><p>The inverse of this integral is highly ambiguous, and we use several approximations to solve it. We do not use any interreflections or shadowing, which means that we do not compute the incoming radiance recursively. Additionally, our illumination is expressed as SG, which reduces a full continuous integral to - in our case - 24 evaluation of the environment SGs.</p><figure id=figure-steps-of-the-query-process-in-a-the-volume-is-constructed-by-tracing-rays-from-each-camera-into-the-scene-then-samples-are-placed-along-with-the-rays-in-b-and-based-on-the-density-at-each-sampling-point-additional-samples-are-placed-in-c-the-samples-are-then-evaluated-into-a-brdf-which-is-re-rendered-using-the-jointly-optimized-illumination-in-d><a data-fancybox href=/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_2000x2000_fit_q80_lanczos.jpg data-caption="Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d)."><img data-src=/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_2000x2000_fit_q80_lanczos.jpg class=lazyload alt width=960 height=540></a><figcaption data-pre=Figure&nbsp; data-post=:&nbsp; class=numbered>Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d).</figcaption></figure><p>Inspired by NeRF the method uses two MLPs, which encode the each position in the volume $\textbf{x} = (x,y,z)$ to a volume density $\sigma$ and a color or BRDF parameters. <a href=#figure-steps-of-the-query-process-in-a-the-volume-is-consturcted-by-tracing-rays-from-each-camera-into-the-scene-then-samples-are-placed-along-the-rays-in-b-and-based-on-the-density-at-each-sampling-point-additional-samples-are-placed-in-c-the-samples-are-then-evaluated-into-a-brdf-which-is-re-rendered-using-the-jointly-optimized-illumination-in-d>Figure 1</a> shows an overview of this optimization process.</p><h3 id=results>Results</h3><div class="alert alert-note"><div>Click the images for an interactive 3D visualization.</div></div><h4 id=real-world>Real-world</h4><div class=gallery><a data-fancybox data-type=iframe data-src="/files/nerd-results/render.html?scene=gnome" href=javascript:;><img class=fixed-width-img src=/files/nerd-results/assets/models/gnome/input.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/nerd-results/render.html?scene=goldcape" href=javascript:;><img class=fixed-width-img src=/files/nerd-results/assets/models/goldcape/input.jpg alt></a></div><h4 id=real-world-preliminary-in-the-wild>Real-world (Preliminary in the wild)</h4><p>The images from the Statue of Liberty are collected from Flickr, Unsplash, Youtube and Vimeo videos. In total about 120 images are used for training from various phones, cameras and drones. Overall even the COLMAP registration is not perfect due to the simplistic shared camera model.</p><div class=gallery><a data-fancybox data-type=iframe data-src="/files/nerd-results/render.html?scene=statue" href=javascript:;><img class=fixed-width-img src=/files/nerd-results/assets/models/statue/input.jpg alt></a></div><h5 id=synthetic-examples>Synthetic Examples</h5><div class=gallery><a data-fancybox data-type=iframe data-src="/files/nerd-results/render.html?scene=car" href=javascript:;><img class=fixed-width-img src=/files/nerd-results/assets/models/car/input.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/nerd-results/render.html?scene=chair" href=javascript:;><img class=fixed-width-img src=/files/nerd-results/assets/models/chair/input.jpg alt></a>
<a data-fancybox data-type=iframe data-src="/files/nerd-results/render.html?scene=globe" href=javascript:;><img class=fixed-width-img src=/files/nerd-results/assets/models/globe/input.jpg alt></a></div></div><div class=article-tags><a class="badge badge-light" href=/tag/material-acquisition/>Material Acquisition</a>
<a class="badge badge-light" href=/tag/shape/>Shape</a>
<a class="badge badge-light" href=/tag/machine-learning/>Machine Learning</a>
<a class="badge badge-light" href=/tag/optimization/>Optimization</a>
<a class="badge badge-light" href=/tag/svbrdf/>SVBRDF</a>
<a class="badge badge-light" href=/tag/neural-rendering/>Neural Rendering</a></div><div class="media author-card content-widget-hr"><a href=https://markboss.me/><img class="avatar mr-3 avatar-circle" src=/author/mark-boss/avatar_hu8228efeb083742449721d94a9befbadd_1942861_270x270_fill_q80_lanczos_center.jpg alt="Mark Boss"></a><div class=media-body><h5 class=card-title><a href=https://markboss.me/>Mark Boss</a></h5><h6 class=card-subtitle>Ph.D. Student</h6><p class=card-text>I&rsquo;m a Ph.D. student at the University of Tübingen with research interests in the intersection of machine learning and computer graphics.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:hello@markboss.me><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/markb_boss target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=y23cQ6wAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/vork target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/mark-boss-4a8a568b target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=/files/cv.pdf><i class="ai ai-cv"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/publication/cvpr20-two-shot-brdf/>Two-shot Spatially-varying BRDF and Shape Estimation</a></li><li><a href=/publication/master_thesis/>CNN-based BRDF parameter estimation</a></li><li><a href=/publication/deep-dual-loss-brdf-parameter-estimation/>Deep Dual Loss BRDF Parameter Estimation</a></li><li><a href=/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/>Single Image BRDF Parameter Estimation with a Conditional Adversarial Network</a></li></ul></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/privacy/>Privacy Policy</a>
&#183;
<a href=/terms/>Legal details</a></p><p class=powered-by></p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=/js/_vendor/bootstrap.bundle.min.29d2abb07db775505d3a2efc824e7b29.js></script><script src=/en/js/wowchemy.min.26c75ee9769bc167bdc8d8fe0c74dcfd.js></script></body></html>