[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. student under the supervision of Prof. Hendrik P. A. Lensch in the Computer Graphics Group at the University of Tübingen. My research interests lie at the intersection of machine learning and computer graphics. The main research question is how to perform inverse rendering on sparse and casual captured images. Here, I primarily focus on enabling efficient material appearance acquisition.\n Download my CV ","date":1659571200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1659571200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://markboss.me/author/mark-boss/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mark-boss/","section":"authors","summary":"I am a Ph.D. student under the supervision of Prof. Hendrik P. A. Lensch in the Computer Graphics Group at the University of Tübingen. My research interests lie at the intersection of machine learning and computer graphics.","tags":null,"title":"Mark Boss","type":"authors"},{"authors":null,"categories":null,"content":"","date":1659603443,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659603443,"objectID":"3112356626e4050884ded2b6feba7a65","permalink":"https://markboss.me/news/scirep22-tlc/","publishdate":"2022-08-04T09:57:23+01:00","relpermalink":"/news/scirep22-tlc/","section":"news","summary":"**[Scientific Report](https://doi.org/10.1038/s41598-022-17527-y)** article and **[Project page](/publication/2022-tlcyzer/)** online","tags":null,"title":"An open-source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening","type":"news"},{"authors":["Cathrin Hauk","Mark Boss","Julia Gabel","Simon Schäfermann","Hendrik P. A. Lensch","Lutz Heide"],"categories":null,"content":"","date":1659571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659571200,"objectID":"89be311f7d6815f8941aff1052cb1d31","permalink":"https://markboss.me/publication/2022-tlcyzer/","publishdate":"2022-08-04T00:00:00Z","relpermalink":"/publication/2022-tlcyzer/","section":"publication","summary":"Substandard and falsified medicines present a serious threat to public health. Simple, low-cost screening tools are important in the identification of such products in low- and middle-income countries. In the present study, a smartphone-based imaging software was developed for the quantification of thin-layer chromatographic (TLC) analyses. A performance evaluation of this tool in the TLC analysis of 14 active pharmaceutical ingredients according to the procedures of the Global Pharma Health Fund (GPHF) Minilab was carried out, following international guidelines and assessing accuracy, repeatability, intermediate precision, specificity, linearity, range and robustness of the method. Relative standard deviations of 2.79% and 4.46% between individual measurements were observed in the assessments of repeatability and intermediate precision, respectively. Small deliberate variations of the conditions hardly affected the results. A locally producible wooden box was designed which ensures TLC photography under standardized conditions and shielding from ambient light. Photography and image analysis were carried out with a low-cost Android-based smartphone. The app allows to share TLC photos and quantification results using messaging apps, e-mail, cable or Bluetooth connections, or to upload them to a cloud. The app is available free of charge as General Public License (GPL) open-source software, and interested individuals or organizations are welcome to use and/or to further improve this software.","tags":["Android","Medicine","Optimization"],"title":"An open-source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening","type":"publication"},{"authors":null,"categories":null,"content":"","date":1659571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659571200,"objectID":"4470c9ac5318bb93f33765a63ecd20b7","permalink":"https://markboss.me/project/tlcyzer/","publishdate":"2022-08-04T00:00:00Z","relpermalink":"/project/tlcyzer/","section":"project","summary":"TLCyzer is a free and open source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening, described and validated in a scientific study.","tags":["Android","Rust","Kotlin","Open Source"],"title":"TLCyzer","type":"project"},{"authors":[],"categories":null,"content":"","date":1658800800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658800800,"objectID":"5890cd731d83b4079e84816f28e8b015","permalink":"https://markboss.me/talk/neural-reflectance-decomposition/","publishdate":"2022-07-14T13:48:52+02:00","relpermalink":"/talk/neural-reflectance-decomposition/","section":"event","summary":"In this talk, I will present our recent work on decomposing an object into its shape, reflectance, and illumination. This highly ill-posed problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is an unconstrained environmental illumination. Decomposing an object under this ambiguous setup enables the automated creation of relightable 3D assets for AR/VR applications, enhanced shopping experiences, games, and movies from online images.In this talk, I will present our recent methods in the field of reflectance decomposition using Neural Fields. Our methods are capable of building a neural volumetric reflectance decomposition from unconstrained image collections. Contrary to most recent works that require images to be captured under the same illumination, our input images are taken under varying illuminations. This practical setup enables the decomposition of images gathered from online searches and the automated creation of relightable 3D assets. Our techniques handle complex geometries with non-Lambertian surfaces, and we also extract 3D meshes with material properties from the learned reflectance volumes enabling their use in existing graphics engines. In our last method, we also enable the decomposition of unposed image collections. Most recent reconstruction methods require posed collections. However, common pose recovery methods fail under highly varying illuminations or locations.","tags":["Material Acquisition","Shape","Machine Learning","Optimization","SVBRDF","Neural Rendering","Camera Pose Estimation"],"title":"Neural Reflectance Decomposition","type":"event"},{"authors":null,"categories":null,"content":"","date":1653987443,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653987443,"objectID":"bc88864ab1fa10658d355ed70fa46a43","permalink":"https://markboss.me/news/arxiv22-samurai/","publishdate":"2022-05-31T09:57:23+01:00","relpermalink":"/news/arxiv22-samurai/","section":"news","summary":"**[ArXiv](https://arxiv.org/abs/2205.15768)** submission and **[Project page](/publication/2022-samurai/)** online","tags":null,"title":"SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections","type":"news"},{"authors":["Mark Boss","Andreas Engelhardt","Abhishek Kar","Yuanzhen Li","Deqing Sun","Jonathan T. Barron","Hendrik P. A. Lensch","Varun Jampani"],"categories":null,"content":"   Introduction Our previous methods such as NeRD and Neural-PIL achieve the decomposition of images under varying illumination into shape, BRDF, and illumination. However, both methods require near-perfect known poses. In challenging scenes recovering poses is challenging and traditional methods fail with objects captured under varying illuminations and locations.\nMethod  The SAMURAI architecture.  In FIGURE 1 we visualize the SAMURAI architecture, which jointly optimizes the camera extrinsic and intrinsic parameters per image, the global shape and BRDF, as well as the per-image illumination latent variables. Here, we leverage Neural-PIL for the rendering and prior on natural illuminations.\nResults  Click the images for an interactive 3D visualization.   SAMURAI-Dataset         ","date":1653955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653955200,"objectID":"26a5c0a05e4a19ccc5e0d8813f08f8eb","permalink":"https://markboss.me/publication/2022-samurai/","publishdate":"2022-05-26T00:00:00Z","relpermalink":"/publication/2022-samurai/","section":"publication","summary":"Inverse rendering of an object under entirely unknown capture conditions is a fundamental challenge in computer vision and graphics. Neural approaches such as NeRF have achieved photorealistic results on novel view synthesis, but they require known camera poses. Solving this problem with unknown camera poses is highly challenging as it requires joint optimization over shape, radiance, and pose. This problem is exacerbated when the input images are captured in the wild with varying backgrounds and illuminations. In such image collections in the wild, standard pose estimation techniques fail due to very few estimated correspondences across images. Furthermore, NeRF cannot relight a scene under any illumination, as it operates on radiance (the product of reflectance and illumination). We propose a joint optimization framework to estimate the shape,  BRDF, and per-image camera pose and illumination. Our method works on in-the-wild online image collections of an object and produces relightable 3D assets for several use-cases such as AR/VR. To our knowledge, our method is the first to tackle this severely unconstrained task with minimal user interaction.","tags":["Material Acquisition","Shape","Machine Learning","Optimization","SVBRDF","Neural Rendering","Camera Pose Estimation"],"title":"SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections","type":"publication"},{"authors":null,"categories":null,"content":"","date":1634641043,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634641043,"objectID":"8b73b52261d726bf9363119ff166d5a9","permalink":"https://markboss.me/news/neurips21-neuralpil/","publishdate":"2021-10-19T11:57:23+01:00","relpermalink":"/news/neurips21-neuralpil/","section":"news","summary":"Accepted at NeurIPS 2021 Poster Presentation, **[arXiv](https://arxiv.org/abs/2110.14373)** and **[Project page](/publication/2021-neural-pil/)** online","tags":null,"title":"Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition","type":"news"},{"authors":["Mark Boss","Varun Jampani","Raphael Braun","Ce Liu","Jonathan T. Barron","Hendrik P. A. Lensch"],"categories":null,"content":"   Introduction Besides the general NeRF Explosion of 2020, a subfield of introducing explicit material representations in to neural volume representation emerged with papers such as NeRD, NeRV, Neural Reflectance Fields for Appearance Acquisition, PhySG or NeRFactor. The way illumination is represented varies drastically between the methods. Either the methods focus on single-point lights such as in Neural Reflectance Fields for Appearance Acquisition, it is assumed to be known (NeRV), it is extracted from a trained NeRF as an illumination map (NeRFactor), or it is represented as Spherical Gaussians (NeRD and PhySG). It is also worth pointing out that nearly all methods focus on a single illumination per scene, except NeRD.\nWhile NeRD enabled decomposition from multiple views under different illumination, SGs only allowed for rather diffuse illuminations. Inspired from Pre-integrated Lighting from real-time rendering, we transfer this concept to a neural network, which handles the integration and can represent illuminations from a manifold of natural illuminations.\nMethod  The Neural-PIL architecture.  In FIGURE 1 visualizes the Neural-PIL architecture, which is inspired by pi-GAN. As seen, the mapping networks are used on the embedding $z^l$, which describes the general content of the environment map and the roughness $b_r$, which defines how rough and therefore how blurry the environment should be.\n Pre-integrated Lighting visualzed.  Visually this can be seen in FIGURE 2. If the BRDF, shown on the left for each pair, becomes rougher, the illuminations from a larger area get integrated. The result is a blurrier environment map.\nAs a joint decomposition of illumination, shape, and appearance is a challenging, ill-posed task, we introduce priors to the BRDF and illumination. The illumination should only lie on a smooth manifold of natural illumination and the BRDF on possible materials. Here, we introduce a Smooth Manifold Auto-Encoder (SMAE).\n Smooth-Manifold-Auto-Encoder.  Inspired by Berthelot et al. - Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer, we introduce the interpolation in latent space during training, we further introduce three additional losses which further aid in a smooth manifold formation. The smoothness loss encourages a smooth gradient w.r.t. to the interpolation factor and therefore achieves a smooth interpolation between two points in the latent space. The cyclic loss enforces that the encoder and decoder perform the same step by re-encoding the decoded interpolated embeddings and ensuring the re-encoded latent vectors are the same as the initial ones. Lastly, we add a discriminator trained on the examples from the dataset as real ones and the interpolated ones as fake and try to fool it with our interpolated embeddings. With these three losses, a smooth latent space is formed, which allows for an easy introduction in our framework, where the corresponding networks are frozen, and only the latent space is optimized.\n","date":1634601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634601600,"objectID":"6add6d439d4627f4ea9c14675576eb6e","permalink":"https://markboss.me/publication/2021-neural-pil/","publishdate":"2021-10-19T00:00:00Z","relpermalink":"/publication/2021-neural-pil/","section":"publication","summary":"Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural approaches such as NeRF have achieved remarkable success in view synthesis, but do not explicitly perform decomposition and instead operate exclusively on radiance (the product of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform decomposition but struggle to accurately recover detailed illumination, thereby significantly limiting realism. We propose a novel reflectance decomposition network that can estimate shape, BRDF, and per-image illumination given a set of object images captured under varying illumination. Our key technique is a novel illumination integration network called Neural-PIL that replaces a costly illumination integral operation in the rendering with a simple network query. In addition, we also learn deep low-dimensional priors on BRDF and illumination representations using novel smooth manifold auto-encoders. Our decompositions can result in considerably better BRDF and light estimates enabling more accurate novel view-synthesis and relighting compared to prior art.","tags":["Material Acquisition","Shape","Machine Learning","Optimization","SVBRDF","Neural Rendering"],"title":"Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition","type":"publication"},{"authors":null,"categories":null,"content":"","date":1629244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629244800,"objectID":"017b3141785e1798f981d5b5ac182850","permalink":"https://markboss.me/news/iccv21-accept/","publishdate":"2021-08-18T00:00:00Z","relpermalink":"/news/iccv21-accept/","section":"news","summary":"Accepted for a ICCV 2021 Poster Presentation","tags":null,"title":"NeRD: Neural Reflectance Decomposition from Image Collections","type":"news"},{"authors":null,"categories":null,"content":"","date":1624838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624838400,"objectID":"ca2aaae623c7a0530f021ca2ffd27321","permalink":"https://markboss.me/project/farrow-and-ball/","publishdate":"2021-06-28T00:00:00Z","relpermalink":"/project/farrow-and-ball/","section":"project","summary":"A set of British paint manufacturer Farrow \u0026 Ball inspired color maps for matplotlib. The maps are sorted in categories for different plotting needs (Divergent, Spectral, Base Colors, Misc).","tags":["Python","Matplotlib","Open Source"],"title":"farrow-and-ball","type":"project"},{"authors":null,"categories":null,"content":"","date":1607252243,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607252243,"objectID":"9cb88f7fd23e1f438ba83fc5b86a6d21","permalink":"https://markboss.me/news/arxiv20-nerd/","publishdate":"2020-12-06T11:57:23+01:00","relpermalink":"/news/arxiv20-nerd/","section":"news","summary":"**[ArXiv](https://arxiv.org/abs/2012.03918)** submission and **[Project page](/publication/2021-nerd/)** online","tags":null,"title":"NeRD: Neural Reflectance Decomposition from Image Collections","type":"news"},{"authors":[],"categories":[],"content":"A bidirectional reflectance distribution function (BRDF) is a function which defines how incoming light $\\omega_i$ is reflected to every outgoing direction $\\omega_o$ on the hemisphere $f_r(\\omega_i, \\omega_o)$. A BRDF can be measured by capturing a surface from various known view and light positions. The information can then be stored in a texture where the x and y coordinates represent the corresponding view and light direction. This is mostly only done for materials that do not vary over the surface due to the information density. Therefore, often analytical BRDF models are used. Parameters are then used to build a function that can be queried for all incoming and outgoing directions. One popular model is the Cook-Torrance model, which parametrizes the BRDF with a diffuse and specular color plus a roughness value. The model is split into a diffuse and specular lobe. Below is an interactive visualizer, where you can change the diffuse, specular, and roughness parameters.\n  Interpolating between random BRDFs is also quite soothing:\n  If you want to use this interactive visualizer for teaching, feel free to use it: link.\n","date":1606815159,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606815159,"objectID":"f71f70059247e8ea0ed80da74f8ecfe5","permalink":"https://markboss.me/project/web_brdf_viz/","publishdate":"2020-12-01T10:32:39+01:00","relpermalink":"/project/web_brdf_viz/","section":"project","summary":"The bidirectional reflectance distribution function (BRDF) is a function that defines how incoming light is reflected outward. This web-based visualizer uses the Cook-Torrance model and shows how the material looks on a sphere, and simultaneously displays its evaluated BRDF. Especially the different reflective behavior of specular and diffuse materials are easily visible.","tags":["Teaching"],"title":"Online BRDF Visualization","type":"project"},{"authors":["Mark Boss","Raphael Braun","Varun Jampani","Jonathan T. Barron","Ce Liu","Hendrik P. A. Lensch"],"categories":null,"content":"   Introduction NeRD is a novel method that can decompose image collections from multiple views taken under varying or fixed illumination conditions. The object can be rotated, or the camera can turn around the object. The result is a neural volume with an explicit representation of the appearance and illumination in the form of the BRDF and Spherical Gaussian (SG) environment illumination.\nThe method is based on the general structure of NeRF. However, NeRF encodes the scene to an implicit BRDF representation where a Multi-Layer-Perceptron (MLP) is queried for outgoing view directions at every point. Extracting information from NeRF is therefore not easily done, and the inference time for novel views takes around 30 seconds. Also, NeRF is not capable of relighting an object under any illumination. By introducing physically-based representations for lighting and appearance, NeRD can relighting an object, and information can be extracted from the neural volume. After our extraction process, the result is a regular texture mesh that can be rendered in real-time. See our results where we provide a web-based interactive renderer.\nMethod Decomposing the scene requires that the integral over the hemisphere from the rendering equation is decomposed into its parts. Here, we use a simplified version without self-emittance. $$L_o(x,\\omega_o) = \\int_\\Omega L_i(x,\\omega_i) f_r(x,\\omega_i,\\omega_o) (\\omega_i \\cdot n) d\\omega_i$$ Here, $L_o$ is the outgoing radiance for a point $x$ in the direction $\\omega_o$. This radiance is calculated by integrating all influences over the hemisphere $\\Omega$, which are based on the incoming light $L_i$ for each direction $\\omega_i$. The surface behavior is expressed as the BRDF $f_r$, which describes how incoming light $\\omega_i$ is directed to the outgoing direction $\\omega_o$. Lastly, a cosine term is used $(\\omega_i \\cdot n)$, which reduces the received light based on the angle towards the light source.\nThe inverse of this integral is highly ambiguous, and we use several approximations to solve it. We do not use any interreflections or shadowing, which means that we do not compute the incoming radiance recursively. Additionally, our illumination is expressed as SG, which reduces a full continuous integral to - in our case - 24 evaluation of the environment SGs.\n Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d).  Inspired by NeRF the method uses two MLPs, which encode the each position in the volume $\\textbf{x} = (x,y,z)$ to a volume density $\\sigma$ and a color or BRDF parameters. Figure 1 shows an overview of this optimization process.\n Overview of the sampling network.   Overview of the decomposition network.  --   -- Results  Click the images for an interactive 3D visualization.   Real-world    Real-world (Preliminary in the wild) The images from the Statue of Liberty are collected from Flickr, Unsplash, Youtube and Vimeo videos. In total about 120 images are used for training from various phones, cameras and drones. Overall even the COLMAP registration is not perfect due to the simplistic shared camera model.\n  Synthetic Examples     Copyright This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to pubs-permissions@ieee.org.\n","date":1606435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606435200,"objectID":"b3f35247536c5e8294a8640a508e3b70","permalink":"https://markboss.me/publication/2021-nerd/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2021-nerd/","section":"publication","summary":"Decomposing a scene into its shape, reflectance, and illumination is a challenging but important problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, most of these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. We propose a neural reflectance decomposition (NeRD) technique that uses physically-based rendering to decompose the scene into spatially varying BRDF material properties. In contrast to existing techniques, our input images can be captured under different illumination conditions. In addition, we also propose techniques to convert the learned reflectance volume into a relightable textured mesh enabling fast real-time rendering with novel illuminations. We demonstrate the potential of the proposed approach with experiments on both synthetic and real datasets, where we are able to obtain high-quality relightable 3D assets from image collections.","tags":["Material Acquisition","Shape","Machine Learning","Optimization","SVBRDF","Neural Rendering"],"title":"NeRD: Neural Reflectance Decomposition from Image Collections","type":"publication"},{"authors":null,"categories":null,"content":"","date":1599782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599782400,"objectID":"30952f371f78725639cac67837164e65","permalink":"https://markboss.me/news/cvpr20-code/","publishdate":"2020-09-11T00:00:00Z","relpermalink":"/news/cvpr20-code/","section":"news","summary":"Source code and dataset released: **[Github](https://github.com/NVlabs/two-shot-brdf-shape)**","tags":null,"title":"Two-shot Spatially-varying BRDF and Shape Estimation","type":"news"},{"authors":["Mark Boss","Varun Jampani","Kihwan Kim","Hendrik P. A. Lensch","Jan Kautz"],"categories":null,"content":"   Results  Click the images for an interactive 3D visualization.   Aksoy et al. - A Dataset of Flash and Ambient Illumination Pairs from the Crowd                  Real-world Examples      Synthetic Examples      Copyright This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to pubs-permissions@ieee.org.\n","date":1592265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592265600,"objectID":"876f8c22bd2dde8478d06100c11e546b","permalink":"https://markboss.me/publication/cvpr20-two-shot-brdf/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/cvpr20-two-shot-brdf/","section":"publication","summary":"Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach.","tags":["Material Acquisition","Machine Learning","Two-Shot","Shape","Depth Map","SVBRDF"],"title":"Two-shot Spatially-varying BRDF and Shape Estimation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"6082eb6237dcf86bdb87a6e456af7380","permalink":"https://markboss.me/news/cvpr20-accept/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/news/cvpr20-accept/","section":"news","summary":"Accepted for a CVPR 2020 Poster Presentation","tags":null,"title":"Two-shot Spatially-varying BRDF and Shape Estimation","type":"news"},{"authors":null,"categories":null,"content":"","date":1577491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577491200,"objectID":"d648370abc4a7f8a9867bdf500fbd632","permalink":"https://markboss.me/project/infomark/","publishdate":"2019-12-28T00:00:00Z","relpermalink":"/project/infomark/","section":"project","summary":"A free, scalable, modern and open source solution for programming lectures supporting auto-testing/grading of programming assignments scaling to thousands of students and several courses.","tags":["Teaching"],"title":"Infomark","type":"project"},{"authors":["Mark Boss","Hendrik P. A. Lensch"],"categories":null,"content":"","date":1570752e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570752e3,"objectID":"b5e50bbae57ddc67784104b3cac55f18","permalink":"https://markboss.me/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/","section":"publication","summary":"Creating plausible surfaces is an essential component in achieving a high degree of realism in rendering. To relieve artists, who create these surfaces in a time-consuming, manual process, automated retrieval of the spatially-varying Bidirectional Reflectance Distribution Function (SVBRDF) from a single mobile phone image is desirable. By leveraging a deep neural network, this casual capturing method can be achieved. The trained network can estimate per pixel normal, base color, metallic and roughness parameters from the Disney BRDF. The input image is taken with a mobile phone lit by the camera flash. The network is trained to compensate for environment lighting and thus learned to reduce artifacts introduced by other light sources. These losses contain a multi-scale discriminator with an additional perceptual loss, a rendering loss using a differentiable renderer, and a parameter loss. Besides the local precision, this loss formulation generates material texture maps which are globally more consistent. The network is set up as a generator network trained in an adversarial fashion to ensure that only plausible maps are produced. The estimated parameters not only reproduce the material faithfully in rendering but capture the style of hand-authored materials due to the more global loss terms compared to previous works without requiring additional post-processing. Both the resolution and the quality is improved.","tags":["Material Acquisition","Machine Learning","Single-Shot","Flat Surface","SVBRDF"],"title":"Single Image BRDF Parameter Estimation with a Conditional Adversarial Network","type":"publication"},{"authors":["Mark Boss","Fabian Groh","Sebastian Herholz","Hendrik P. A. Lensch"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"2c17505f22b1b0ca69d3e04d1557f1d5","permalink":"https://markboss.me/publication/deep-dual-loss-brdf-parameter-estimation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/deep-dual-loss-brdf-parameter-estimation/","section":"publication","summary":"Surface parameter estimation is an essential field in computer games and movies. An exact representation of a real-world surface allows for a higher degree of realism. Capturing or artistically creating these materials is a time-consuming process. We propose a method which utilizes an encoder-decoder Convolutional Neural Network (CNN) to extract parameters for the Bidirectional Reflectance Distribution Function (BRDF) automatically from a sparse sample set. This is done by implementing a differentiable renderer, which allows for a loss backpropagation of rendered images. This photometric loss is essential because defining a numerical BRDF distance metric is difficult. A second loss is added, which compares the parameters maps directly. Therefore, the statistical properties of the BRDF model are learned, which reduces artifacts in the predicted parameters. This dual loss principal improves the result of the network significantly. Opposed to previous means this method retrieves information of the whole surface as spatially varying BRDF (SVBRDF) parameters with a sufficiently high resolution for intended real-world usage. The capture process for materials only requires five known light positions with a fixed camera position. This reduces the scanning time drastically, and a material sample can be obtained in seconds with an automated system.","tags":["Material Acquisition","Machine Learning","Multi-Shot","Flat Surface","SVBRDF"],"title":"Deep Dual Loss BRDF Parameter Estimation","type":"publication"},{"authors":null,"categories":null,"content":"Legal Disclosure Information in accordance with Section 5 TMG\nMark Boss\nBismarckstr. 114\n72072 Tübingen\nContact Information E-Mail: markboss@mailbox.org\nInternet address: markboss.me\nDisclaimer Accountability for content The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents’ accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this matter, please note that we are not obliged to monitor the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per §§ 8 to 10 of the Telemedia Act (TMG).\nAccountability for links Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately.\nCopyright Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law, every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are only allowed for private use. The materials from these pages are copyrighted and any unauthorized use may violate copyright laws.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://markboss.me/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"Legal Disclosure Information in accordance with Section 5 TMG\nMark Boss\nBismarckstr. 114\n72072 Tübingen\nContact Information E-Mail: markboss@mailbox.org\nInternet address: markboss.me\nDisclaimer Accountability for content The contents of our pages have been created with the utmost care.","tags":null,"title":"Legal details","type":"page"},{"authors":null,"categories":null,"content":"Your privacy is important to us. It is Mark Boss’ policy to respect your privacy regarding any information we may collect from you across our website, https://www.markboss.me, and other sites we own and operate.\nWe only ask for personal information when we truly need it to provide a service to you. We collect it by fair and lawful means, with your knowledge and consent. We also let you know why we’re collecting it and how it will be used.\nWe only retain collected information for as long as necessary to provide you with your requested service. What data we store, we’ll protect within commercially acceptable means to prevent loss and theft, as well as unauthorized access, disclosure, copying, use or modification.\nWe don’t share any personally identifying information publicly or with third-parties, except when required to by law.\nOur website may link to external sites that are not operated by us. Please be aware that we have no control over the content and practices of these sites, and cannot accept responsibility or liability for their respective privacy policies.\nYou are free to refuse our request for your personal information, with the understanding that we may be unable to provide you with some of your desired services.\nYour continued use of our website will be regarded as acceptance of our practices around privacy and personal information. If you have any questions about how we handle user data and personal information, feel free to contact us.\nThis policy is effective as of 1 April 2020.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://markboss.me/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"Your privacy is important to us. It is Mark Boss’ policy to respect your privacy regarding any information we may collect from you across our website, https://www.markboss.me, and other sites we own and operate.","tags":null,"title":"Privacy Policy","type":"page"},{"authors":["Mark Boss","Hendrik P. A. Lensch"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"3a9afb08e7d0d94f6bb3e8fb2337a5ad","permalink":"https://markboss.me/publication/master_thesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/master_thesis/","section":"publication","summary":"The behavior of surfaces is an essential field in computer games and movies. An exact representation of a real-world surface allows for a higher degree of realism. Capturing or artistically creating these materials is a time-consuming process. Thus, in this thesis a method which utilizes an encoder-decoder Convolutional Neural Networks (CNN) to extract information of the Bidirectional Reflectance Distribution Function (BRDF) automatically is proposed. Opposed to previous means this method retrieves information of the whole surface as spatially varying BRDF-parameters with a sufficiently high resolution for real-world usage. The capture process for materials only requires five known light positions with a fixed camera position and thus can be acquired even in a mobile setup. This reduces the scanning time drastically and a material sample can be obtained in seconds with an automated system.","tags":["Material Acquisition","Machine Learning","Multi-Shot","Flat Surface","SVBRDF"],"title":"CNN-based BRDF parameter estimation","type":"publication"}]