<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mark Boss</title><link>https://markboss.me/</link><atom:link href="https://markboss.me/index.xml" rel="self" type="application/rss+xml"/><description>Mark Boss</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Oct 2022 18:24:54 +0200</lastBuildDate><image><url>https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png</url><title>Mark Boss</title><link>https://markboss.me/</link></image><item><title>NeRF at ECCV 2022</title><link>https://markboss.me/post/nerf_at_eccv22/</link><pubDate>Sat, 01 Oct 2022 18:24:54 +0200</pubDate><guid>https://markboss.me/post/nerf_at_eccv22/</guid><description>&lt;p>I recently went through the &lt;a href="https://eccv2022.ecva.net/program/provisional-program/" target="_blank" rel="noopener">the provisional programm&lt;/a> of ECCV 2022. After my last &lt;a href="https://markboss.me/post/nerf_at_neurips22/">post on &amp;ldquo;NeRF at NeurIPS&amp;rdquo;&lt;/a> got such great feedback, and I anyway compiled a list of all NeRFy things, I decided to do it all again.&lt;/p>
&lt;p>I again tried to find all papers by parsing the titles of the provisional program. A brief scan through the paper or abstract was then done to confirm if it is NeRFy and get a rough idea about the paper. If I have mischaracterized or missed any paper, please send me a DM on Twitter &lt;a href="https://twitter.com/markb_boss" target="_blank" rel="noopener">@markb_boss&lt;/a> or via mail.&lt;/p>
&lt;p>Here we go again!&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: &lt;em>images below belong to the cited papers, and the copyright belongs to the authors or the organization that published these papers. Below I use key figures or videos from some papers under the fair use clause of copyright law.&lt;/em>&lt;/p>
&lt;h2 id="nerf">NeRF&lt;/h2>
&lt;p>Mildenhall &lt;em>et al.&lt;/em> introduced NeRF at ECCV 2020 in the now seminal &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">Neural Radiance Field paper&lt;/a>. As mentioned in the introduction, there has been fantastic progress in research in this field. NeurIPS this year is no exception to this trend.&lt;/p>
&lt;p>What NeRF does, in short, is similar to the task of computed tomography scans (CT). In CT scans, a rotating head measures densities from x-rays. However, knowing where that measurement should be placed in 3D is challenging. NeRF also tries to estimate where a surface and view-dependent radiance is located in space. This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images. With many posed images, photorealistic novel view synthesis is possible.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
Rotate by clicking and dragging, zoom via mouse wheel, and use the buttons (Teal indicates active) to visualize the NeRF process.
&lt;/div>
&lt;/div>
&lt;iframe
src="https://markboss.me/talks/viz/nerf_explainer/"
width="100%"
height="500px"
style="border:none;">
&lt;/iframe>
&lt;h3 id="fundamentals">Fundamentals&lt;/h3>
&lt;figure>
&lt;video autoplay loop >
&lt;source src="https://snap-research.github.io/R2L/static/videos/DONeRF/forest.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
&lt;p>Light field distillation with &lt;a href="https://arxiv.org/abs/2203.17261" target="_blank" rel="noopener">R2L&lt;/a>. Here, on the left, the regular NeRF rendering is compared with the distilled light field. The quality improves with the light field (NeRF PSNR: 28.11 vs. R2L PSNR: 34.18).&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>These papers address more fundamental problems of view-synthesis with NeRF methods.&lt;/p>
&lt;p>Typically in streaming, the quality of the signal is adjusted depending on the connection quality. NeRFs, on the other hand, encode the entire signal in the weights. &lt;strong>&lt;a href="https://arxiv.org/abs/2207.09663" target="_blank" rel="noopener">Streamable Neural Fields&lt;/a>&lt;/strong> aims to fix that by encoding everything into smaller sub-networks that can be streamed over time.&lt;/p>
&lt;p>While NeRFs provide photorealistic novel view synthesis results, they do not offer a way to estimate the certainty of their reconstruction. Especially in medical or autonomous driving, this is a highly desirable property. &lt;strong>&lt;a href="https://arxiv.org/abs/2203.10192" target="_blank" rel="noopener">Conditional-Flow NeRF&lt;/a>&lt;/strong> aims to alleviate this by leveraging a flexible data-driven approach with conditional normalizing flows coupled with latent variable modeling.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.17261" target="_blank" rel="noopener">R2L&lt;/a>&lt;/strong> aims to directly learn a surface light field from a pre-trained NeRF. This way, only a single point per ray needs to be evaluated, drastically improving the inference performance.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.10312" target="_blank" rel="noopener">AdaNeRF&lt;/a>&lt;/strong> aims to speed up NeRF rendering by learning to reduce the sample count drastically using a split network architecture. One network predicts the sampling, and the other the shading. By enforcing sparsity during training, the number of samples can be lowered.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.07967" target="_blank" rel="noopener">Intrinsic Neural Fields&lt;/a>&lt;/strong> proposes a new representation for neural fields on manifolds using the spectral properties of the Laplace-Beltrami operator.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2111.15135" target="_blank" rel="noopener">Beyond Periodicity&lt;/a>&lt;/strong> examines the activation functions of NeRFs and tests several novel non-periodic activation functions which enable high-frequency signal encoding.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.14455" target="_blank" rel="noopener">NeDDF&lt;/a>&lt;/strong> proposes a novel 3D representation that reciprocally constrains the distance and density fields. This enables the definition of a distance field for objects with indefinite boundaries (smoke, hairballs, glass, etc.) without losing density information.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2112.05504" target="_blank" rel="noopener">BungeeNeRF&lt;/a>&lt;/strong> enables training NeRFs on large-scale scenes by growing the network during training with a residual block structure. Each block progressively encodes a finer scale.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.06340" target="_blank" rel="noopener">SNeS&lt;/a>&lt;/strong> proposes to leverage symmetry during NeRF training for unseen areas of objects. Geometry and material are often symmetrical, but illumination and shadowing are not symmetric. Therefore, the method introduces a soft symmetry constraint for geometry and materials and includes a global illumination model which can break the symmetry.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2208.06787" target="_blank" rel="noopener">HDR-Plenoxels&lt;/a>&lt;/strong> learns an HDR radiance field with multiple LDR images under different camera settings. The method achieves this by modeling the tone mapping of camera imaging pipelines.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2202.03532" target="_blank" rel="noopener">MINER&lt;/a>&lt;/strong> enables gigapixel image or large-scale point cloud learning with Laplacian pyramids to learn multi-scale signal decompositions. Small MLPs learn disjointed patches in each pyramid scale, and the scales allow the network to grow in capacity during training.&lt;/p>
&lt;h3 id="priors-and-generative">Priors and Generative&lt;/h3>
&lt;!-- &lt;figure>
&lt;video autoplay loop >
&lt;source src="https://markomih.github.io/KeypointNeRF/images/facedome_results.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
Generalization to novel avatars with [KeypointNeRF](https://arxiv.org/abs/2205.04992).
&lt;/figcaption>
&lt;/figure>
-->
&lt;p>
&lt;figure id="figure-generalizable-patch-based-neural-renderinghttpsarxivorgabs220710662-uses-transformers-to-generalize-the-3d-reconstruction-to-novel-views-using-transformers-on-epipolar-geometry-patches">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://mohammedsuhail.net/gen_patch_neural_rendering/img/model_animation.gif" alt="Genearlizable patch-based neural rendering with transformers." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://arxiv.org/abs/2207.10662" target="_blank" rel="noopener">Generalizable Patch-Based Neural Rendering&lt;/a> uses transformers to generalize the 3D reconstruction to novel views using transformers on epipolar geometry patches.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Priors can either aid in the reconstruction or can be used in a generative manner. For example, in the reconstruction, priors either increase the quality of neural view synthesis or enable reconstructions from sparse image collections.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.13691" target="_blank" rel="noopener">ShAPO&lt;/a>&lt;/strong> aims to solve the challenge of 3D understanding from a single RGB-D image by posing the problem as a latent space regression of shape, appearance, and pose latent codes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.04992" target="_blank" rel="noopener">KeypointNeRF&lt;/a>&lt;/strong> aims to represent general human avatars with NeRFs by encoding relative spatial 3D information from sparse 3D keypoints. These keypoints are easily applicable with human priors to novel avatars or capture setups.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.11770" target="_blank" rel="noopener">DFRF&lt;/a>&lt;/strong> uses NeRFs to tackle talking head synthesis, which is the task of animating a head from audio. Here, the NeRF is conditioned on 2D appearance and audio features. This also acts as a prior and enables fast adjustments to novel identities.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.05737" target="_blank" rel="noopener">SparseNeuS&lt;/a>&lt;/strong> enables reconstructions from sparse images by learning generalizable priors from image features by introducing geometry encoding volumes for generic surface prediction.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2112.04481" target="_blank" rel="noopener">Scene-DRDF&lt;/a>&lt;/strong> predicts a full 3D scene from a single unseen image. For this, the method is trained on a dataset of realistic non-watertight scans of scenes and predicts the Directed Ray Distance Function (DRDF).&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://arxiv.org/abs/2207.10662" target="_blank" rel="noopener">Generalizable Patch-Based Neural Rendering&lt;/a>&lt;/strong>, the color of a ray in a novel scene is predicted directly from a collection of patches sampled from the scene. The method leverages epipolar geometry to extract patches along the epipolar line, linearly projects them into a 1D feature vector, and transformers process the collection.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.10157" target="_blank" rel="noopener">ViewFormer&lt;/a>&lt;/strong> proposes tackling the novel view synthesis task from 2D images in a single pass network. Here, a two-stage architecture consisting of a codebook and a transformer model is used. The former is used to embed individual images into a smaller latent space, and the transformer solves the view synthesis task in this more compact space.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2208.02801" target="_blank" rel="noopener">Transformers as Meta-Learners for Implicit Neural Representations&lt;/a>&lt;/strong> proposes to leverage transformers as hyper networks for neural fields, which generate the weights of the NeRF. This circumvents the information bottleneck of only conditioning the field based on a single latent vector.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.10821" target="_blank" rel="noopener">Sem2NeRF&lt;/a>&lt;/strong> reconstructs a 3D scene from a single-view semantic mask. This is achieved by encoding the mask into a latent code that controls the scene representation of a pre-trained decoder.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.11467" target="_blank" rel="noopener">CompNVS&lt;/a>&lt;/strong> proposes to perform novel view synthesis from RGB-D images with largely incomplete scene coverage. The method works on a grid-based representation and completes unobserved parts with pre-trained networks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2204.00928" target="_blank" rel="noopener">SiNeRF&lt;/a>&lt;/strong> trains a NeRF from a single view by applying geometric and semantic regularizations. Here, image warping is used to obtain geometry pseudo labels, and adversarial training, as well as a pre-trained ViT, are utilized for semantic pseudo labels&lt;/p>
&lt;h3 id="articulated">Articulated&lt;/h3>
&lt;figure>
&lt;video autoplay loop >
&lt;source src="https://lsongx.github.io/projects/images/pref-overview.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
&lt;p>Motion estimation with &lt;a href="https://arxiv.org/abs/2209.10691" target="_blank" rel="noopener">PREF&lt;/a>. PREF is capable of motion estimation even under topological changes.&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Capturing dynamic objects is a trend that started in previous conference years. However, this year I noticed a trend in adding neural priors.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2209.10691" target="_blank" rel="noopener">PREF&lt;/a>&lt;/strong> learns a neural motion field by introducing regularization on the predictability of motion based on previous frames using a predictor network.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://neuralbodies.github.io/arah" target="_blank" rel="noopener">ARAH&lt;/a>&lt;/strong> enables learning animated clothed human avatars from multi-view videos. These avatars have pose-dependent geometry and appearance and generalize to out-of-distribution poses.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.12575" target="_blank" rel="noopener">NeuMan&lt;/a>&lt;/strong> proposes a framework to reconstruct humans and scenes from a single video with a moving camera. Separate training for the scene and the human is performed, and a warping field to the canonical dynamic human is learned.&lt;/p>
&lt;p>Finding a correspondence between two non-rigidly deforming shapes is a challenging task, which &lt;strong>&lt;a href="https://arxiv.org/abs/2203.07694" target="_blank" rel="noopener">Implicit field supervision for robust non-rigid shape matching&lt;/a>&lt;/strong> proposes to solve with an auto-decoder framework, which learns a continuous shape-wise deformation field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.13817" target="_blank" rel="noopener">AutoAvatar&lt;/a>&lt;/strong> extends recent avatar modeling with autoregressive modeling to express dynamic effects such as soft-tissue deformation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.13807" target="_blank" rel="noopener">Pose-NDF&lt;/a>&lt;/strong> learns a prior of human motion as a field of high-dimensional SO(3)$^K$ pose definition with $K$ quaternions. The resulting manifold can then be easily interpolated and create novel poses.&lt;/p>
&lt;h3 id="editable-and-composable">Editable and Composable&lt;/h3>
&lt;p>
&lt;figure id="figure-object-compositions-with-object-compositional-neural-implicit-surfaceshttpsarxivorgabs220709686">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://wuqianyi.top/objectsdf/img/teaser.gif" alt="Object compositions with Object-Compositional Neural Implicit Surfaces." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Object compositions with &lt;a href="https://arxiv.org/abs/2207.09686" target="_blank" rel="noopener">Object-Compositional Neural Implicit Surfaces&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>This section covers NeRFs that propose composing, controlling, or editing methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.12298" target="_blank" rel="noopener">Deforming Radiance Fields with Cages&lt;/a>&lt;/strong> uses low-poly cages around the object to create a simple deformation target. Deforming the cage is then simple with manual editing. The deformation can warp the previously trained NeRF with a dense warping field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.09686" target="_blank" rel="noopener">Object-Compositional Neural Implicit Surfaces&lt;/a>&lt;/strong> models a scene as a combination of Signed Distance Functions (SDF) of individual objects. For this, the strong association between an object&amp;rsquo;s SDF and semantic label is used, and the semantics are tied to the SDF from each object.&lt;/p>
&lt;h3 id="pose-estimation">Pose Estimation&lt;/h3>
&lt;p>
&lt;figure id="figure-neural-correspondence-field-for-object-pose-estimationhttpsarxivorgabs220800113-find-3d-to-3d-correspondences-in-complex-scenes">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Neural Correspondence Field for Object Pose Estimation." srcset="
/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_2d12e5364440d0e4ce39369d3ac45f54.webp 400w,
/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_339b7686e502915bccfbed0f3b372600.webp 760w,
/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_2d12e5364440d0e4ce39369d3ac45f54.webp"
width="760"
height="324"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://arxiv.org/abs/2208.00113" target="_blank" rel="noopener">Neural Correspondence Field for Object Pose Estimation&lt;/a> find 3D to 3D correspondences in complex scenes.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Estimating the pose of objects or the camera is a fundamental problem in computer vision.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2208.00113" target="_blank" rel="noopener">Neural Correspondence Field for Object Pose Estimation&lt;/a>&lt;/strong> estimates the pose of an object with a known 3D model by constructing neural correspondence fields, which create a 3D mapping between query points and the object space.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.13296" target="_blank" rel="noopener">RayTran&lt;/a>&lt;/strong> estimates the pose of objects and the shape from RGB videos. Here, a global 3D grid of features and an array of view-specific 2D grids is used. A progressive exchange of information is performed with a bidirectional attention mechanism.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2204.05735" target="_blank" rel="noopener">GARF&lt;/a>&lt;/strong> proposes to leverage Gaussian activation functions for pose estimation. Similar to &lt;a href="https://arxiv.org/abs/2104.06405" target="_blank" rel="noopener">BARF&lt;/a>, the method also increases the bandwidth of the Gaussian function over time.&lt;/p>
&lt;h3 id="decomposition">Decomposition&lt;/h3>
&lt;figure>
&lt;video autoplay loop >
&lt;source src="https://akshatdave.github.io/pandora/video/teaser_animation.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
&lt;p>&lt;a href="https://arxiv.org/abs/2203.13458" target="_blank" rel="noopener">PANDORA&lt;/a> enables decompositions into shape, diffuse, and specular using polarization cues.&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>In this section, the radiance of NeRF is split into geometry, BRDF, and illumination. This enables consistent relighting under any illumination.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2112.05140" target="_blank" rel="noopener">NeRF-OSR&lt;/a>&lt;/strong> enables relighting of outdoor scenes by decomposing the radiance to an albedo, Spherical Harmonics (SH) illumination model and explicitly modeling the shadowing from the SH illumination.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.13458" target="_blank" rel="noopener">PANDORA&lt;/a>&lt;/strong> leverages polarization cues to accurately predict shapes and decompositions into diffuse and specular components.&lt;/p>
&lt;p>Relightable dynamic humans are enabled with &lt;strong>&lt;a href="https://arxiv.org/abs/2207.07104" target="_blank" rel="noopener">Relighting4D&lt;/a>&lt;/strong>. Here, the human body is decomposed into surface normals, occlusion, diffuse, and specular with neural fields and rendered with a physically-based renderer.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.11406" target="_blank" rel="noopener">PS-NeRF&lt;/a>&lt;/strong> learns a decomposition from images under sparse views, where each view is illuminated by multiple unknown directional lights. The method can produce detailed surfaces from sparse viewpoints with a shadow-aware renderer and supervision from the multiple illumination images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.07182" target="_blank" rel="noopener">NeILF&lt;/a>&lt;/strong> represents scene lighting as a neural incident light field, which handles occlusions and indirect illumination. With this illumination representation, a decomposition into BRDFs is performed, which a regularized with a Lambertian assumption and bilateral smoothness.&lt;/p>
&lt;h3 id="other">Other&lt;/h3>
&lt;figure>
&lt;video autoplay loop >
&lt;source src="https://www.cs.cornell.edu/projects/arf/videos/Playground_14.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
&lt;p>&lt;a href="https://arxiv.org/abs/2206.06360" target="_blank" rel="noopener">ARF&lt;/a> enables simple stylization of NeRF reconstructions.&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Several works with excellent results in various fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.03949" target="_blank" rel="noopener">RC-MVSNet&lt;/a>&lt;/strong> introduces neural rendering to Multi-View Stereo (MVS) to reduce ambiguity in correspondences on non-Lambertian surfaces.&lt;/p>
&lt;p>What if we do not reconstruct photorealistic 3D scenes but style them according to paintings? &lt;strong>&lt;a href="https://arxiv.org/abs/2206.06360" target="_blank" rel="noopener">ARF&lt;/a>&lt;/strong> creates these highly stylized novel views from regular radiance fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.01831" target="_blank" rel="noopener">LTEW&lt;/a>&lt;/strong> introduces continuous neural fields to image warping. The method enables learning high-frequency content with a Local Texture Estimator instead of the classical NeRF Fourier embedding.&lt;/p>
&lt;p>Periodic patterns appear in many man-made scenes. NeRF-style methods do not capture these periodic patterns. &lt;strong>&lt;a href="https://arxiv.org/abs/2208.12278" target="_blank" rel="noopener">NPP&lt;/a>&lt;/strong> introduces a periodicity-aware warping module in front of the neural field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2112.04267" target="_blank" rel="noopener">Implicit Neural Representations for Image Compression&lt;/a>&lt;/strong> investigates using neural fields for image compression tasks by introducing quantization, quantization-aware retraining, and entropy coding to neural fields. Meta-learned initializations from MAML also enable shorter training times.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.14067" target="_blank" rel="noopener">Neural Strands&lt;/a>&lt;/strong> focuses on hair representation based on a neural scalp texture that encodes the geometry and appearance of individual strands at each texel location. The rendering is performed based on rasterization of the learned hair strands.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.15065" target="_blank" rel="noopener">DeepShadow&lt;/a>&lt;/strong> learns a 3D reconstruction (depth and normals) based on shadowing. For this, the shadow map generated from a neural field that learns the depth map and a light position is compared to the actual captured one.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.01583" target="_blank" rel="noopener">LaTeRF&lt;/a>&lt;/strong> is a method for extracting an object from a neural field given a natural language description of the object and a set of point-labels of object and non-object points in the input images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.15946" target="_blank" rel="noopener">Towards Learning Neural Representations from Shadows&lt;/a>&lt;/strong> estimates the shape from shadows using a neural shadow field. Here, a shadow mapping approach is used to render the shadows, which can be compared to the ground truth shadows.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.14782" target="_blank" rel="noopener">Minimal Neural Atlas&lt;/a>&lt;/strong> learns an explicit neural surface representation from a minimal atlas of 3 charts. With a distortion-minimal parameterization for surfaces, arbitrary topology can be represented.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2204.01943" target="_blank" rel="noopener">Unified Implicit Neural Stylization&lt;/a>&lt;/strong> enables stylized neural fields by learning the style of a painting and the content of the scene separately and combining them in an Amalgamation module.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>This is my second time doing a conference roundup, and the number of papers is quite stunning. This time I gathered 49 papers from just a single conference. &lt;a href="https://markboss.me/post/nerf_at_neurips22/">NeurIPS&lt;/a> had 36. If we also take &lt;a href="https://dellaert.github.io/NeRF22/" target="_blank" rel="noopener">CVPR&lt;/a> into account (57 papers), the 3 conferences alone this year produced over 140 NeRF-related papers.&lt;/p>
&lt;p>There are several trends in this year&amp;rsquo;s ECCV. One major one is combining transformers with NeRFs and, in general, adding neural priors. This trend was also visible in my &lt;a href="https://markboss.me/post/nerf_at_neurips22/">NeurIPS roundup&lt;/a>. Another large one is examining NeRF and finding ways to express high-frequency data more easily or capture vast scenes. In my opinion, it is also amazing to see several works which decompose into BRDF, shape, and illumination or to see techniques such as shadow mapping being used in NeRFs.&lt;/p></description></item><item><title>NeRF at NeurIPS 2022</title><link>https://markboss.me/post/nerf_at_neurips22/</link><pubDate>Sun, 18 Sep 2022 22:31:51 +0200</pubDate><guid>https://markboss.me/post/nerf_at_neurips22/</guid><description>&lt;p>Inspired by &lt;a href="https://dellaert.github.io" target="_blank" rel="noopener">Frank Dellaert&lt;/a> and his excellent series on the original &lt;a href="https://dellaert.github.io/NeRF/" target="_blank" rel="noopener">NeRF Explosion&lt;/a> and the following &lt;a href="https://dellaert.github.io/NeRF21/" target="_blank" rel="noopener">ICCV&lt;/a>/&lt;a href="https://dellaert.github.io/NeRF22/" target="_blank" rel="noopener">CVPR&lt;/a> conference gatherings, I decided to look into creating a NeurIPS 22 rundown myself.&lt;/p>
&lt;p>The papers below are all the papers I could gather by browsing through the extensive list of &lt;a href="https://nips.cc/Conferences/2022/Schedule?type=Poster" target="_blank" rel="noopener">accepted NeurIPS papers&lt;/a>. I mainly collected all papers where the titles fit and did a brief scan through the paper or only the abstract if the paper wasn&amp;rsquo;t published at the time of writing. If I have mischaracterized or missed any paper, please send me a DM on Twitter &lt;a href="https://twitter.com/markb_boss" target="_blank" rel="noopener">@markb_boss&lt;/a> or via mail.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: &lt;em>images below belong to the cited papers, and the copyright belongs to the authors or the organization that published these papers. Below I use key figures or videos from some papers under the fair use clause of copyright law.&lt;/em>&lt;/p>
&lt;h2 id="nerf">NeRF&lt;/h2>
&lt;p>Mildenhall &lt;em>et al.&lt;/em> introduced NeRF at ECCV 2020 in the now seminal &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">Neural Radiance Field paper&lt;/a>. As mentioned in the introduction, there has been fantastic progress in research in this field. NeurIPS this year is no exception to this trend.&lt;/p>
&lt;p>What NeRF does, in short, is similar to the task of computed tomography scans (CT). In CT scans, a rotating head measures densities from x-rays. However, knowing where that measurement should be placed in 3D is challenging. NeRF also tries to estimate where a surface and view-dependent radiance is located in space. This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images. With many posed images, photorealistic novel view synthesis is possible.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
Rotate by clicking and dragging, zoom via mouse wheel, and use the buttons (Teal indicates active) to visualize the NeRF process.
&lt;/div>
&lt;/div>
&lt;iframe
src="https://markboss.me/talks/viz/nerf_explainer/"
width="100%"
height="500px"
style="border:none;">
&lt;/iframe>
&lt;h3 id="fundamentals">Fundamentals&lt;/h3>
&lt;p>
&lt;figure id="figure-surface-reconstruction-improvements-with-improved-surface-reconstruction-using-high-frequency-detailshttpsarxivorgabs220607850">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Surface reconstructions improvements with a high-frequency deformation field" srcset="
/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_2d95c11ceca55453108f8ae0f2f49134.webp 400w,
/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_8f44542db3618cb5e9f001558da0d7f1.webp 760w,
/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_2d95c11ceca55453108f8ae0f2f49134.webp"
width="760"
height="148"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Surface reconstruction improvements with: &lt;a href="https://arxiv.org/abs/2206.07850" target="_blank" rel="noopener">Improved surface reconstruction using high-frequency details&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>These address more fundamental areas of view-synthesis with NeRF methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55157" target="_blank" rel="noopener">NTRF&lt;/a>&lt;/strong> extends NeRF to improve transmission and reflections, using neural transmittance fields and edge constraints.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55396" target="_blank" rel="noopener">PNF&lt;/a>&lt;/strong> introduces a new class of neural fields using basis-encoded polynomials. These can represent the signal as a composition of manipulable and interpretable components.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=52805" target="_blank" rel="noopener">LoENerf&lt;/a>&lt;/strong> introduces a Levels-of-Experts (LoE) framework to create a novel coordinate-based representation with an MLP. The weights of the MLP are hierarchical, periodic, and position-dependent. Each layer of the MLP has multiple candidate values of the weight matrix, which are individually tiled across the input space.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.07850" target="_blank" rel="noopener">Improved surface reconstruction using high-frequency details&lt;/a>&lt;/strong> can be achieved by splitting the base low-frequency shape into signed distance fields and the high-frequent details in a deformation field on the base shape.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15674" target="_blank" rel="noopener">Generalised Implicit Neural Representations&lt;/a>&lt;/strong> venture far beyond the realm of euclidean coordinate systems and propose to observe the continuous high dimensional signal as a discrete graph and perform a spectral embedding on each node to establish the input for the neural field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15848" target="_blank" rel="noopener">Geo-Neus&lt;/a>&lt;/strong> introduces explicit multi-view geometry constraints to generate geometry consistent surface reconstructions. These losses include ones for signed distance function (SDF) from sparse structure-from-motion (SFM) point clouds and ones for photometric consistency.&lt;/p>
&lt;p>While SDFs are often used to express geometry in NeRFs, they can only represent closed shapes. Using unsigned distance functions (UDF) can express open and watertight surfaces, but they can pose challenges in meshing. &lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55176" target="_blank" rel="noopener">HSDF&lt;/a>&lt;/strong> presents a learnable representation that combines the benefits of each.&lt;/p>
&lt;h3 id="audio">Audio&lt;/h3>
&lt;p>
&lt;figure id="figure-results-from-nafhttpsarxivorgabs220400628-showing-the-sound-propagation-in-the-rooms-shown-above-from-the-emitter-indicated-in-red">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Sound propagation modeling results from NAF" srcset="
/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_66bf9ccee1aa19dbeb9938565b0413c5.webp 400w,
/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_35598e6df4c252308dda01e296ff9fa3.webp 760w,
/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_66bf9ccee1aa19dbeb9938565b0413c5.webp"
width="760"
height="214"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Results from &lt;a href="https://arxiv.org/abs/2204.00628" target="_blank" rel="noopener">NAF&lt;/a> showing the sound propagation in the rooms shown above from the emitter, indicated in red.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>As sound propagates similar to rays in a volume, NeRF also found usage in this area.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55190" target="_blank" rel="noopener">INRAS&lt;/a>&lt;/strong> stores high-fidelity time domain impulse responses at arbitrary positions using neural fields. This allows modeling spatial acoustic for a scene efficiently.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2204.00628" target="_blank" rel="noopener">NAF&lt;/a>&lt;/strong> follow a similar approach to INRAS, learning impulse response functions in spatially varying scene.&lt;/p>
&lt;h3 id="priors-and-generative">Priors and Generative&lt;/h3>
&lt;p>
&lt;figure id="figure-overview-of-neuformhttpsarxivorgabs220708890-which-combines-generalizable-priors-with-the-overfitting-of-typical-nerf-model-adaptively">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Visualization of prior-based generalization and NeRF-style overfitting combination in NeuForm." srcset="
/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_19972b94f1141cac9c8daa0b8d07fe46.webp 400w,
/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_b393e8ad9ea1ed746d58736be1c44bc0.webp 760w,
/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_19972b94f1141cac9c8daa0b8d07fe46.webp"
width="681"
height="281"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of &lt;a href="https://arxiv.org/abs/2207.08890" target="_blank" rel="noopener">NeuForm&lt;/a> which combines generalizable priors with the overfitting of typical NeRF model adaptively.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Priors can either aid in the reconstruction or can be used in a generative manner. For example, in the reconstruction, priors either increase the quality of neural view synthesis or enable reconstructions from sparse image collections.&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55117" target="_blank" rel="noopener">CoCo-INR&lt;/a>&lt;/strong>, two attention modules are used. One to extract useful information from the prior codebook and the other to encode these entries into the scene. With this prior, the method is capable of working on sparse images.&lt;/p>
&lt;p>In sparse image collections, several areas are often rarely observed. To circumvent this issue, &lt;strong>&lt;a href="https://arxiv.org/abs/2207.08890" target="_blank" rel="noopener">NeuForm&lt;/a>&lt;/strong> relies on generalizable category-specific representation in less observed areas. In well-observed areas, the method uses the accurate overfitting of NeRF.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.00665" target="_blank" rel="noopener">MonoSDF&lt;/a>&lt;/strong> integrates recent monocular depth and normal prediction networks as priors. Given these additional priors, the authors show performance improvement under either MLP neural fields or voxel-based grid methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.07695" target="_blank" rel="noopener">VoxGRAF&lt;/a>&lt;/strong> combines recent methods for speeding up NeRFs using voxel-based structures and 3D convolutions to generate novel objects in a single forward pass. These scenes can then be rendered from any viewpoint.&lt;/p>
&lt;h3 id="articulated">Articulated&lt;/h3>
&lt;p>
&lt;figure id="figure-overview-of-nemfhttpsarxivorgabs220603287-which-generates-novel-motions-from-a-learned-prior-with-a-continuous-temporal-field">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="NeMF motion prior visualized" srcset="
/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_83625c3cdb2e154623f52edaab166192.webp 400w,
/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_ac37d238defd72e34ef6c07538026258.webp 760w,
/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_83625c3cdb2e154623f52edaab166192.webp"
width="760"
height="223"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of &lt;a href="https://arxiv.org/abs/2206.03287" target="_blank" rel="noopener">NeMF&lt;/a> which generates novel motions from a learned prior with a continuous temporal field.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Capturing dynamic objects is a trend that started in previous conference years. However, this year I noticed a trend in adding priors.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54986" target="_blank" rel="noopener">Neural Shape Deformation Priors&lt;/a>&lt;/strong> uses a transformer trained on large datasets to learn a prior of non-rigid transformations. A source mesh can be morphed with a continuous neural deformation field to a target mesh with a partially described surface deformation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.03287" target="_blank" rel="noopener">NeMF&lt;/a>&lt;/strong> learns a prior of human and quadruped motion and uses it generatively in a neural motion field. The authors show use cases for motion interpolation or in-betweening.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15723" target="_blank" rel="noopener">DeVRF&lt;/a>&lt;/strong> proposes to use 3D spatial voxel grids alongside a 4D deformation field to model dynamic scenes in a highly efficient manner.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54805" target="_blank" rel="noopener">FNeVR&lt;/a>&lt;/strong> enables animating faces with NeRFs by combining 2D motion warping with neural rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.15258" target="_blank" rel="noopener">NDR&lt;/a>&lt;/strong> jointly optimizes the surface and deformation from a dynamic scene using monocular RGB-D cameras. They propose an invertible deformation network to provide cycle consistency between frames. As the topology might change in dynamic scenes, they also employ a strategy to generate topology variants.&lt;/p>
&lt;h3 id="editable-and-composable">Editable and Composable&lt;/h3>
&lt;p>
&lt;figure id="figure-ccnerfhttpsarxivorgabs220514870-enables-compression-and-composition-of--nerfs">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Examples of compression and composition with CCNeRF" srcset="
/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_16364f0c3087528b920dca3fe481a91f.webp 400w,
/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_dc0ab6ad297ad599d0e1beb132739229.webp 760w,
/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_16364f0c3087528b920dca3fe481a91f.webp"
width="760"
height="258"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://arxiv.org/abs/2205.14870" target="_blank" rel="noopener">CCNeRF&lt;/a> enables compression and composition of NeRFs.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>This section covers NeRFs that provide methods for composing, controlling, or editing.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15838v2" target="_blank" rel="noopener">D&lt;sup>2&lt;/sup>NeRF&lt;/a>&lt;/strong> learns a decoupling of static and dynamic objects from monocular video. Here, two networks are trained separately, which handle the respective areas.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://ylqiao.net/publication/2022nerf/" target="_blank" rel="noopener">NeuPhysics&lt;/a>&lt;/strong> allows editing a dynamic scene by editing physics parameters. This editing is performed on a hexahedra mesh tied to an underlying neural field in a two-way conversion. The physics simulation is done differently on the mesh and can propagate back to the neural field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54786" target="_blank" rel="noopener">CageNeRF&lt;/a>&lt;/strong> uses low-poly cages around the object to create a simple deformation target. Deforming the cage is then simple in manual editing. The deformation can then be used to warp the previously trained NeRF.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55279" target="_blank" rel="noopener">INSP-Net&lt;/a>&lt;/strong> introduces signal processing into neural fields by introducing a differential operator framework, which can directly work on the fields without discretization. The authors even propose a CovnNet running on the neural representation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15585" target="_blank" rel="noopener">Decomposing NeRF for Editing via Feature Field Distillation&lt;/a>&lt;/strong> uses existing 2D feature extractors such as CLIP-LSeg to provide additional supervision to detect semantics in the 3D volume. Users can query based on text, image patches, or direct pixel selection to allow semantic editing.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.14870" target="_blank" rel="noopener">CCNeRF&lt;/a>&lt;/strong> uses a tensor rank decomposition to express the neural fields in a highly efficient and compressible manner. As the representation is explicit, the learned models are also composable.&lt;/p>
&lt;h3 id="decomposition">Decomposition&lt;/h3>
&lt;p>
&lt;figure id="figure-overview-of-samuraihttpsarxivorgabs220515768-shows-the-applications-of-shape-brdf-and-illumination-decomposition-with-a-simultaneous-pose-recovery">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Shape, BRDF, and illumination decomposition with a simultaneous pose recovery with SAMURAI." srcset="
/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_aa542951c0ad27f1a98034a6abc4c123.webp 400w,
/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_276ba311288569682a06b1e6c576e6ab.webp 760w,
/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_aa542951c0ad27f1a98034a6abc4c123.webp"
width="760"
height="280"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of &lt;a href="https://arxiv.org/abs/2205.15768" target="_blank" rel="noopener">Samurai&lt;/a> shows the applications of shape, BRDF, and illumination decomposition with a simultaneous pose recovery.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>In this section, the radiance of NeRF is split into geometry, BRDF, and illumination. This enables consistent relighting under any illumination.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.03858" target="_blank" rel="noopener">RENI&lt;/a>&lt;/strong> uses &lt;a href="https://www.vincentsitzmann.com/siren/" target="_blank" rel="noopener">SIREN&lt;/a> and vector neurons to learn a neural prior on natural HDR illuminations. They extend the vector neurons to handle rotation equivariance for spherical images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://github.com/neuralps3d/neuralps3d" target="_blank" rel="noopener">Neural Reflectance Field from Shading and Shadow under a Fixed Viewpoint&lt;/a>&lt;/strong> leverages a fixed viewpoint with a moving light source. This way, the geometry, and BRDF have to be recovered based on shading cues. Due to the neural fields, the 3D scene is recovered even from the fixed viewpoint.&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://arxiv.org/abs/2206.03380" target="_blank" rel="noopener">Shape, Light &amp;amp; Material Decomposition from Images using Monte Carlo Rendering and Denoising&lt;/a>&lt;/strong> no previously used low-frequency basis or pre-integration is used to express the environment illumination. Instead, regular Monte Carlo integration is used, which is passed through a denoising network to ease the learning process from noisy gradients.&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://arxiv.org/abs/2205.15768" target="_blank" rel="noopener">SAMURAI&lt;/a>&lt;/strong> the model can decompose coarsely posed image collections into shape, BRDF, and illumination. The cameras are also optimized during training with a camera multiplex. Especially, datasets of challenging scenes where traditional SFM methods fail can be decomposed with this method. &lt;em>For transparency, I&amp;rsquo;m the author of the paper&lt;/em>&lt;/p>
&lt;h3 id="other">Other&lt;/h3>
&lt;p>
&lt;figure id="figure-overview-of-the-perfceptionhttpsarxivorgabs220811537-dataset-with-its-applications">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Overview of the new PeRFception dataset." srcset="
/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_e5c475dca3ff95f87b124468ea564adf.webp 400w,
/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_0a9781da2c6107fcd23e432c3a6608ff.webp 760w,
/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_e5c475dca3ff95f87b124468ea564adf.webp"
width="760"
height="333"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of the &lt;a href="https://arxiv.org/abs/2208.11537" target="_blank" rel="noopener">PeRFception&lt;/a> dataset with its applications.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Several works with excellent results in various fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55004" target="_blank" rel="noopener">NeMF&lt;/a>&lt;/strong> proposes to leverage neural fields for semantic correspondence matching. A coarse cost volume is used to guide a high-precision neural matching field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.01724" target="_blank" rel="noopener">SNAKE&lt;/a>&lt;/strong> proposes introducing shape reconstruction with neural fields to aid in 3D keypoint detection for point clouds. No supervision for the detection is required.&lt;/p>
&lt;p>Segmentation and concept learning in NeRFs can benefit from &lt;strong>&lt;a href="https://arxiv.org/abs/2207.06403" target="_blank" rel="noopener">3D Concept Grounding on Neural Fields&lt;/a>&lt;/strong>, where a high-dimensional feature descriptor at each spatial location is taken from language embeddings.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=53336" target="_blank" rel="noopener">ULNef&lt;/a>&lt;/strong> specifically targets virtual try-on settings where multiple garments are layered. These layers are modeled as neural fields, and the collision handling is directly performed on the fields.&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://arxiv.org/abs/2206.01634" target="_blank" rel="noopener">NeRF-RL&lt;/a>&lt;/strong>, a neural field is used to learn a latent space as the state representation for the reinforcement learning algorithm.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2208.11537" target="_blank" rel="noopener">PeRFception&lt;/a>&lt;/strong> establishes new large-scale datasets for perception-related tasks such as segmentation, classification, etc. They also evaluate a plenoxels variant on these datasets.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>This was quite the amount of work, and I have enormous respect for Frank Dellaert for having it done thrice before. The field of NeRF moves incredibly fast, and even for a single conference, there exists a massive amount of reading material.&lt;/p>
&lt;p>It is also interesting to see NeRF find new applications in other fields such as &lt;a href="#audio">audio&lt;/a> and that sorting papers into a single category becomes challenging. This might be related to fast progress in the field and the high risk of getting scooped.&lt;/p></description></item><item><title>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</title><link>https://markboss.me/news/neurips22-samurai/</link><pubDate>Thu, 15 Sep 2022 09:57:23 +0100</pubDate><guid>https://markboss.me/news/neurips22-samurai/</guid><description/></item><item><title>An open-source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening</title><link>https://markboss.me/news/scirep22-tlc/</link><pubDate>Thu, 04 Aug 2022 09:57:23 +0100</pubDate><guid>https://markboss.me/news/scirep22-tlc/</guid><description/></item><item><title>An open-source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening</title><link>https://markboss.me/publication/2022-tlcyzer/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2022-tlcyzer/</guid><description>&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>TLCyzer</title><link>https://markboss.me/project/tlcyzer/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://markboss.me/project/tlcyzer/</guid><description/></item><item><title>Neural Reflectance Decomposition</title><link>https://markboss.me/talk/neural-reflectance-decomposition/</link><pubDate>Tue, 26 Jul 2022 10:00:00 +0800</pubDate><guid>https://markboss.me/talk/neural-reflectance-decomposition/</guid><description/></item><item><title>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</title><link>https://markboss.me/news/arxiv22-samurai/</link><pubDate>Tue, 31 May 2022 09:57:23 +0100</pubDate><guid>https://markboss.me/news/arxiv22-samurai/</guid><description/></item><item><title>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</title><link>https://markboss.me/publication/2022-samurai/</link><pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2022-samurai/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/LlYuGDjXp-8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Our previous methods such as &lt;a href="https://markboss.me/publication/2021-nerd/">NeRD&lt;/a> and &lt;a href="https://markboss.me/publication/2021-neural-pil/">Neural-PIL&lt;/a> achieve the decomposition of images under varying illumination into shape, BRDF, and illumination. However, both methods require near-perfect known poses. In challenging scenes recovering poses is challenging and traditional methods fail with objects captured under varying illuminations and locations.&lt;/p>
&lt;h2 id="method">Method&lt;/h2>
&lt;figure id="figure-the-samurai-architecture">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The SAMURAI architecture." srcset="
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_f0fbbf61d7097a49e26b7975fffcfb94.webp 400w,
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_9e4d72244c945061262711b0f3a8d3cd.webp 760w,
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_f0fbbf61d7097a49e26b7975fffcfb94.webp"
width="760"
height="248"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
The SAMURAI architecture.
&lt;/figcaption>&lt;/figure>
&lt;p>In &lt;a href="#figure-the-samurai-architecture">&lt;strong>FIGURE 1&lt;/strong>&lt;/a> we visualize the SAMURAI architecture, which jointly optimizes the camera extrinsic and intrinsic parameters per image, the global shape and BRDF, as well as the per-image illumination latent variables. Here, we leverage &lt;a href="https://markboss.me/publication/2021-neural-pil/">Neural-PIL&lt;/a> for the rendering and prior on natural illuminations.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="samurai-dataset">SAMURAI-Dataset&lt;/h4>
&lt;div class="gallery">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=duck" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/samurai-results/assets/models/duck.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=fireengine" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/samurai-results/assets/models/fireengine.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=garbagetruck" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/samurai-results/assets/models/garbagetruck.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=keywest" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/samurai-results/assets/models/keywest.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=pumpkin" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/samurai-results/assets/models/pumpkin.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=rccar" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/samurai-results/assets/models/rccar.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=robot" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/samurai-results/assets/models/robot.jpg" alt="">
&lt;/a>
&lt;/div>
&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</title><link>https://markboss.me/news/neurips21-neuralpil/</link><pubDate>Tue, 19 Oct 2021 11:57:23 +0100</pubDate><guid>https://markboss.me/news/neurips21-neuralpil/</guid><description/></item><item><title>Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</title><link>https://markboss.me/publication/2021-neural-pil/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2021-neural-pil/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/p5cKaNwVp4M" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Besides the general &lt;a href="https://dellaert.github.io/NeRF/" target="_blank" rel="noopener">NeRF Explosion&lt;/a> of 2020, a subfield of introducing explicit material representations in to neural volume representation emerged with papers such as &lt;a href="https://markboss.me/publication/2021-nerd/">NeRD&lt;/a>, &lt;a href="https://pratulsrinivasan.github.io/nerv/" target="_blank" rel="noopener">NeRV&lt;/a>, &lt;a href="https://arxiv.org/abs/2008.03824" target="_blank" rel="noopener">Neural Reflectance Fields for Appearance Acquisition&lt;/a>, &lt;a href="https://kai-46.github.io/PhySG-website/" target="_blank" rel="noopener">PhySG&lt;/a> or &lt;a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/" target="_blank" rel="noopener">NeRFactor&lt;/a>. The way illumination is represented varies drastically between the methods. Either the methods focus on single-point lights such as in &lt;a href="https://arxiv.org/abs/2008.03824" target="_blank" rel="noopener">Neural Reflectance Fields for Appearance Acquisition&lt;/a>, it is assumed to be known (NeRV), it is extracted from a trained NeRF as an illumination map (NeRFactor), or it is represented as Spherical Gaussians (NeRD and PhySG). It is also worth pointing out that nearly all methods focus on a single illumination per scene, except NeRD.&lt;/p>
&lt;p>While NeRD enabled decomposition from multiple views under different illumination, SGs only allowed for rather diffuse illuminations. Inspired from &lt;a href="https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf" target="_blank" rel="noopener">Pre-integrated Lighting&lt;/a> from real-time rendering, we transfer this concept to a neural network, which handles the integration and can represent illuminations from a manifold of natural illuminations.&lt;/p>
&lt;h2 id="method">Method&lt;/h2>
&lt;figure id="figure-the-neural-pil-architecture">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The Neural-PIL architecture." srcset="
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_f65ed7f0eae768f8d2fe04858fedb038.webp 400w,
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_c8d406d520e0d639062f4ba09d83e4b1.webp 760w,
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_f65ed7f0eae768f8d2fe04858fedb038.webp"
width="375"
height="253"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
The Neural-PIL architecture.
&lt;/figcaption>&lt;/figure>
&lt;p>In &lt;a href="#figure-the-neural-pil-architecture">&lt;strong>FIGURE 1&lt;/strong>&lt;/a> visualizes the Neural-PIL architecture, which is inspired by &lt;a href="https://marcoamonteiro.github.io/pi-GAN-website/" target="_blank" rel="noopener">pi-GAN&lt;/a>. As seen, the mapping networks are used on the embedding $z^l$, which describes the general content of the environment map and the roughness $b_r$, which defines how rough and therefore how blurry the environment should be.&lt;/p>
&lt;figure id="figure-pre-integrated-lighting-visualzed">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Pre-integrated Lighting visualzed." srcset="
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_9b482253ec45a6df9381bef693190cf0.webp 400w,
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_71abc23863cc4b37e7472a0d5c76b9fa.webp 760w,
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_9b482253ec45a6df9381bef693190cf0.webp"
width="760"
height="108"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Pre-integrated Lighting visualzed.
&lt;/figcaption>&lt;/figure>
&lt;p>Visually this can be seen in &lt;a href="#figure-pre-integrated-lighting-visualzed">&lt;strong>FIGURE 2&lt;/strong>&lt;/a>. If the BRDF, shown on the left for each pair, becomes rougher, the illuminations from a larger area get integrated. The result is a blurrier environment map.&lt;/p>
&lt;p>As a joint decomposition of illumination, shape, and appearance is a challenging, ill-posed task, we introduce priors to the BRDF and illumination. The illumination should only lie on a smooth manifold of natural illumination and the BRDF on possible materials. Here, we introduce a Smooth Manifold Auto-Encoder (SMAE).&lt;/p>
&lt;figure id="figure-smooth-manifold-auto-encoder">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Smooth-Manifold-Auto-Encoder." srcset="
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_712e77003ea0bdb06796212f28cc6030.webp 400w,
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_ee450154c756a0c55d12b6eab27b418a.webp 760w,
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_712e77003ea0bdb06796212f28cc6030.webp"
width="408"
height="249"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Smooth-Manifold-Auto-Encoder.
&lt;/figcaption>&lt;/figure>
&lt;p>Inspired by Berthelot et al. - &lt;a href="https://arxiv.org/abs/1807.07543" target="_blank" rel="noopener">Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer&lt;/a>, we introduce the interpolation in latent space during training, we further introduce three additional losses which further aid in a smooth manifold formation. The smoothness loss encourages a smooth gradient w.r.t. to the interpolation factor and therefore achieves a smooth interpolation between two points in the latent space. The cyclic loss enforces that the encoder and decoder perform the same step by re-encoding the decoded interpolated embeddings and ensuring the re-encoded latent vectors are the same as the initial ones. Lastly, we add a discriminator trained on the examples from the dataset as real ones and the interpolated ones as fake and try to fool it with our interpolated embeddings. With these three losses, a smooth latent space is formed, which allows for an easy introduction in our framework, where the corresponding networks are frozen, and only the latent space is optimized.&lt;/p>
&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>NeRD: Neural Reflectance Decomposition from Image Collections</title><link>https://markboss.me/news/iccv21-accept/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://markboss.me/news/iccv21-accept/</guid><description/></item><item><title>farrow-and-ball</title><link>https://markboss.me/project/farrow-and-ball/</link><pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate><guid>https://markboss.me/project/farrow-and-ball/</guid><description/></item><item><title>NeRD: Neural Reflectance Decomposition from Image Collections</title><link>https://markboss.me/news/arxiv20-nerd/</link><pubDate>Sun, 06 Dec 2020 11:57:23 +0100</pubDate><guid>https://markboss.me/news/arxiv20-nerd/</guid><description/></item><item><title>Online BRDF Visualization</title><link>https://markboss.me/project/web_brdf_viz/</link><pubDate>Tue, 01 Dec 2020 10:32:39 +0100</pubDate><guid>https://markboss.me/project/web_brdf_viz/</guid><description>&lt;p>A bidirectional reflectance distribution function (BRDF) is a function which defines how incoming light $\omega_i$ is reflected to every outgoing direction $\omega_o$ on the hemisphere $f_r(\omega_i, \omega_o)$. A BRDF can be measured by capturing a surface from various known view and light positions. The information can then be stored in a texture where the x and y coordinates represent the corresponding view and light direction. This is mostly only done for materials that do not vary over the surface due to the information density. Therefore, often analytical BRDF models are used. Parameters are then used to build a function that can be queried for all incoming and outgoing directions. One popular model is the &lt;a href="https://dl.acm.org/doi/10.1145/357290.357293" target="_blank" rel="noopener">Cook-Torrance&lt;/a> model, which parametrizes the BRDF with a diffuse and specular color plus a roughness value. The model is split into a diffuse and specular lobe. Below is an interactive visualizer, where you can change the diffuse, specular, and roughness parameters.&lt;/p>
&lt;div style="width: 100%; padding-top: 100%; position: relative;">
&lt;iframe src="https://markboss.me/files/brdf_viz/brdf_eval.html" width="100%" height="100%" frameborder=" 0 " style="position: absolute; top: 0;
left: 0;
bottom: 0;
right: 0;">&lt;/iframe>
&lt;/div>
&lt;p>Interpolating between random BRDFs is also quite soothing:&lt;/p>
&lt;div style="width: 100%; padding-top: 100%; position: relative;">
&lt;iframe src="https://markboss.me/files/brdf_viz/brdf_eval.html?rngesus=1" width="100%" height="100%" frameborder=" 0 " style="position: absolute; top: 0;
left: 0;
bottom: 0;
right: 0;">&lt;/iframe>
&lt;/div>
&lt;p>If you want to use this interactive visualizer for teaching, feel free to use it: &lt;a href="https://markboss.me/files/brdf_viz/brdf_eval.html">link&lt;/a>.&lt;/p></description></item><item><title>NeRD: Neural Reflectance Decomposition from Image Collections</title><link>https://markboss.me/publication/2021-nerd/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2021-nerd/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/IM9OgMwHNTI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>NeRD is a novel method that can decompose image collections from multiple views taken under varying or fixed illumination conditions. The object can be rotated, or the camera can turn around the object. The result is a neural volume with an explicit representation of the appearance and illumination in the form of the BRDF and Spherical Gaussian (SG) environment illumination.&lt;/p>
&lt;p>The method is based on the general structure of &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">NeRF&lt;/a>. However, NeRF encodes the scene to an implicit BRDF representation where a Multi-Layer-Perceptron (MLP) is queried for outgoing view directions at every point. Extracting information from NeRF is therefore not easily done, and the inference time for novel views takes around 30 seconds. Also, NeRF is not capable of relighting an object under any illumination. By introducing physically-based representations for lighting and appearance, NeRD can relighting an object, and information can be extracted from the neural volume. After our extraction process, the result is a regular texture mesh that can be rendered in real-time. See our &lt;a href="#results">results&lt;/a> where we provide a web-based interactive renderer.&lt;/p>
&lt;h3 id="method">Method&lt;/h3>
&lt;p>Decomposing the scene requires that the integral over the hemisphere from the rendering equation is decomposed into its parts. Here, we use a simplified version without self-emittance.
$$L_o(x,\omega_o) = \int_\Omega L_i(x,\omega_i) f_r(x,\omega_i,\omega_o) (\omega_i \cdot n) d\omega_i$$
Here, $L_o$ is the outgoing radiance for a point $x$ in the direction $\omega_o$. This radiance is calculated by integrating all influences over the hemisphere $\Omega$, which are based on the incoming light $L_i$ for each direction $\omega_i$. The surface behavior is expressed as the BRDF $f_r$, which describes how incoming light $\omega_i$ is directed to the outgoing direction $\omega_o$. Lastly, a cosine term is used $(\omega_i \cdot n)$, which reduces the received light based on the angle towards the light source.&lt;/p>
&lt;p>The inverse of this integral is highly ambiguous, and we use several approximations to solve it. We do not use any interreflections or shadowing, which means that we do not compute the incoming radiance recursively. Additionally, our illumination is expressed as SG, which reduces a full continuous integral to - in our case - 24 evaluation of the environment SGs.&lt;/p>
&lt;!-- Our image formation is now expressed as:
$$L_o(x,\omega_o) \approx \sum^{24}_{m=1} \rho_d(\omega_o,\Gamma,x) + \rho_s(\omega_o,\Gamma,x)$$
Where $\Gamma$ defines the parameters for the SGs, $\rho_d$ defines the evaluation for the diffuse part of the BRDF, and $\rho_s$ for the specular component. -->
&lt;figure id="figure-steps-of-the-query-process-in-a-the-volume-is-constructed-by-tracing-rays-from-each-camera-into-the-scene-then-samples-are-placed-along-with-the-rays-in-b-and-based-on-the-density-at-each-sampling-point-additional-samples-are-placed-in-c-the-samples-are-then-evaluated-into-a-brdf-which-is-re-rendered-using-the-jointly-optimized-illumination-in-d">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d)." srcset="
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1bbd1c3ac16b86cda9f44a5e77471f36.webp 400w,
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_49a191e91129f1c86af0cceaab5489f7.webp 760w,
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1bbd1c3ac16b86cda9f44a5e77471f36.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d).
&lt;/figcaption>&lt;/figure>
&lt;p>Inspired by NeRF the method uses two MLPs, which encode the each position in the volume $\textbf{x} = (x,y,z)$ to a volume density $\sigma$ and a color or BRDF parameters. &lt;a href="#figure-steps-of-the-query-process-in-a-the-volume-is-consturcted-by-tracing-rays-from-each-camera-into-the-scene-then-samples-are-placed-along-the-rays-in-b-and-based-on-the-density-at-each-sampling-point-additional-samples-are-placed-in-c-the-samples-are-then-evaluated-into-a-brdf-which-is-re-rendered-using-the-jointly-optimized-illumination-in-d">Figure 1&lt;/a> shows an overview of this optimization process.&lt;/p>
&lt;!-- Similar to NeRF, two networks are trained in conjunction. A training and inference step consists of first creating a rough sampling pattern using our *sampling network*, which learns the object's rough shape. Samp -->
&lt;!--
&lt;figure id="figure-overview-of-the-sampling-network">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Overview of the sampling network." srcset="
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_35d1538a7833f8bc008f37225f81cbca.webp 400w,
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_58e3423051f5c144464c778a5662a18d.webp 760w,
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_35d1538a7833f8bc008f37225f81cbca.webp"
width="712"
height="340"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Overview of the sampling network.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-overview-of-the-decomposition-network">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Overview of the decomposition network." srcset="
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_23296c69e2599015e5425e33d6db0bfb.webp 400w,
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_a1c6d74b033f6ca373eefbf110124f8a.webp 760w,
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_23296c69e2599015e5425e33d6db0bfb.webp"
width="760"
height="347"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Overview of the decomposition network.
&lt;/figcaption>&lt;/figure> -->
&lt;!--
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CyC6PutoJO8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
-->
&lt;h3 id="results">Results&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="real-world">Real-world&lt;/h4>
&lt;div class="gallery">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=gnome" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/nerd-results/assets/models/gnome/input.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=goldcape" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/nerd-results/assets/models/goldcape/input.jpg" alt="">
&lt;/a>
&lt;/div>
&lt;h4 id="real-world-preliminary-in-the-wild">Real-world (Preliminary in the wild)&lt;/h4>
&lt;p>The images from the Statue of Liberty are collected from Flickr, Unsplash, Youtube and Vimeo videos. In total about 120 images are used for training from various phones, cameras and drones. Overall even the COLMAP registration is not perfect due to the simplistic shared camera model.&lt;/p>
&lt;div class="gallery">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=statue" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/nerd-results/assets/models/statue/input.jpg" alt="">
&lt;/a>
&lt;/div>
&lt;h5 id="synthetic-examples">Synthetic Examples&lt;/h5>
&lt;div class="gallery">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=car" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/nerd-results/assets/models/car/input.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=chair" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/nerd-results/assets/models/chair/input.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=globe" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/nerd-results/assets/models/globe/input.jpg" alt="">
&lt;/a>
&lt;/div>
&lt;h3 id="copyright">Copyright&lt;/h3>
&lt;p>This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to &lt;a href="mailto:pubs-permissions@ieee.org">pubs-permissions@ieee.org&lt;/a>.&lt;/p></description></item><item><title>Two-shot Spatially-varying BRDF and Shape Estimation</title><link>https://markboss.me/news/cvpr20-code/</link><pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate><guid>https://markboss.me/news/cvpr20-code/</guid><description/></item><item><title>Two-shot Spatially-varying BRDF and Shape Estimation</title><link>https://markboss.me/publication/cvpr20-two-shot-brdf/</link><pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/cvpr20-two-shot-brdf/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CyC6PutoJO8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="results">Results&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="aksoy-et-al---a-dataset-of-flash-and-ambient-illumination-pairs-from-the-crowd">Aksoy et al. - A Dataset of Flash and Ambient Illumination Pairs from the Crowd&lt;/h4>
&lt;div class="gallery">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf0" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf0/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf1" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf1/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf2" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf2/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf3" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf3/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf4" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf4/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf5" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf5/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf6" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf6/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf7" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf7/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf8" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf8/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf9" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf9/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf10" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf10/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf11" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf11/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf12" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf12/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf13" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf13/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf14" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf14/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf15" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/fnf15/input_flash.jpg" alt="">
&lt;/a>
&lt;/div>
&lt;h5 id="real-world-examples">Real-world Examples&lt;/h5>
&lt;div class="gallery">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw0" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/rw0/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw1" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/rw1/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw2" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/rw2/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw3" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/rw3/input_flash.jpg" alt="">
&lt;/a>
&lt;/div>
&lt;h5 id="synthetic-examples">Synthetic Examples&lt;/h5>
&lt;div class="gallery">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn0" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/syn0/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn1" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/syn1/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn2" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/syn2/input_flash.jpg" alt="">
&lt;/a>
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn3" href="javascript:;">
&lt;img class="fixed-width-img" src="https://markboss.me/files/cvpr20-results/predictions/syn3/input_flash.jpg" alt="">
&lt;/a>
&lt;/div>
&lt;h3 id="copyright">Copyright&lt;/h3>
&lt;p>This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to &lt;a href="mailto:pubs-permissions@ieee.org">pubs-permissions@ieee.org&lt;/a>.&lt;/p></description></item><item><title>Two-shot Spatially-varying BRDF and Shape Estimation</title><link>https://markboss.me/news/cvpr20-accept/</link><pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate><guid>https://markboss.me/news/cvpr20-accept/</guid><description/></item><item><title>Infomark</title><link>https://markboss.me/project/infomark/</link><pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate><guid>https://markboss.me/project/infomark/</guid><description/></item><item><title>Single Image BRDF Parameter Estimation with a Conditional Adversarial Network</title><link>https://markboss.me/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/</guid><description/></item><item><title>Deep Dual Loss BRDF Parameter Estimation</title><link>https://markboss.me/publication/deep-dual-loss-brdf-parameter-estimation/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/deep-dual-loss-brdf-parameter-estimation/</guid><description/></item><item><title>Legal details</title><link>https://markboss.me/terms/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate><guid>https://markboss.me/terms/</guid><description>&lt;h3 id="legal-disclosure">Legal Disclosure&lt;/h3>
&lt;p>Information in accordance with Section 5 TMG&lt;/p>
&lt;p>Mark Boss&lt;br>
Bismarckstr. 114&lt;br>
72072 Tbingen&lt;/p>
&lt;h3 id="contact-information">Contact Information&lt;/h3>
&lt;p>E-Mail: &lt;a href="mailto:markboss@mailbox.org">markboss@mailbox.org&lt;/a>&lt;br>
Internet address: markboss.me&lt;/p>
&lt;h1 id="disclaimer">Disclaimer&lt;/h1>
&lt;h3 id="accountability-for-content">Accountability for content&lt;/h3>
&lt;p>The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents&amp;rsquo; accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this matter, please note that we are not obliged to monitor the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per  8 to 10 of the Telemedia Act (TMG).&lt;/p>
&lt;h3 id="accountability-for-links">Accountability for links&lt;/h3>
&lt;p>Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately.&lt;/p>
&lt;h3 id="copyright">Copyright&lt;/h3>
&lt;p>Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law, every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are only allowed for private use. The materials from these pages are copyrighted and any unauthorized use may violate copyright laws.&lt;/p></description></item><item><title>Privacy Policy</title><link>https://markboss.me/privacy/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate><guid>https://markboss.me/privacy/</guid><description>&lt;p>Your privacy is important to us. It is Mark Boss&amp;rsquo; policy to respect your privacy regarding any information we may collect from you across our website, &lt;a href="https://www.markboss.me" target="_blank" rel="noopener">https://www.markboss.me&lt;/a>, and other sites we own and operate.&lt;/p>
&lt;p>We only ask for personal information when we truly need it to provide a service to you. We collect it by fair and lawful means, with your knowledge and consent. We also let you know why were collecting it and how it will be used.&lt;/p>
&lt;p>We only retain collected information for as long as necessary to provide you with your requested service. What data we store, well protect within commercially acceptable means to prevent loss and theft, as well as unauthorized access, disclosure, copying, use or modification.&lt;/p>
&lt;p>We dont share any personally identifying information publicly or with third-parties, except when required to by law.&lt;/p>
&lt;p>Our website may link to external sites that are not operated by us. Please be aware that we have no control over the content and practices of these sites, and cannot accept responsibility or liability for their respective privacy policies.&lt;/p>
&lt;p>You are free to refuse our request for your personal information, with the understanding that we may be unable to provide you with some of your desired services.&lt;/p>
&lt;p>Your continued use of our website will be regarded as acceptance of our practices around privacy and personal information. If you have any questions about how we handle user data and personal information, feel free to contact us.&lt;/p>
&lt;p>This policy is effective as of 1 April 2020.&lt;/p></description></item></channel></rss>