<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.4.0 for Hugo"><meta name=google-site-verification content="S4KoopgNiTG4YgetELfTyLoFaDS9bHbC3gJBOdfft3o"><meta name=msvalidate.01 content="2625B23426FAE2D4025878BAF046F9A0"><meta name=author content="Mark Boss"><meta name=description content="Inspired by Frank Dellaert and his excellent series on the original NeRF Explosion and the following ICCV/CVPR conference gatherings, I decided to look into creating a NeurIPS 22 rundown myself."><link rel=alternate hreflang=en-us href=https://markboss.me/post/nerf_at_neurips22/><meta name=theme-color content="#009688"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.7c5470f4eb8eccc945078663ffb75c1f.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://markboss.me/post/nerf_at_neurips22/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@markb_boss"><meta property="twitter:creator" content="@markb_boss"><meta property="og:site_name" content="Mark Boss"><meta property="og:url" content="https://markboss.me/post/nerf_at_neurips22/"><meta property="og:title" content="NeRF at NeurIPS 2022 | Mark Boss"><meta property="og:description" content="Inspired by Frank Dellaert and his excellent series on the original NeRF Explosion and the following ICCV/CVPR conference gatherings, I decided to look into creating a NeurIPS 22 rundown myself."><meta property="og:image" content="https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-09-18T22:31:51+02:00"><meta property="article:modified_time" content="2022-09-30T22:31:51+02:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://markboss.me/post/nerf_at_neurips22/"},"headline":"NeRF at NeurIPS 2022","datePublished":"2022-09-18T22:31:51+02:00","dateModified":"2022-09-30T22:31:51+02:00","author":{"@type":"Person","name":"Mark Boss"},"publisher":{"@type":"Organization","name":"Mark Boss","logo":{"@type":"ImageObject","url":"https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_192x192_fill_lanczos_center_3.png"}},"description":"Inspired by Frank Dellaert and his excellent series on the original NeRF Explosion and the following ICCV/CVPR conference gatherings, I decided to look into creating a NeurIPS 22 rundown myself."}</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=all onload='this.media="all"'><title>NeRF at NeurIPS 2022 | Mark Boss</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=eab517625633c7d5cd5b01e7d3c2cc30><script src=/js/wowchemy-init.min.9c53337151330e4f750895b53468cbe3.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Mark Boss</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Mark Boss</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/files/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="article-container py-1" style=background:initial><nav class="d-none d-md-flex" aria-label=breadcrumb><ol class=breadcrumb><li class=breadcrumb-item><a href=/>Home</a></li><li class=breadcrumb-item><a href=/post/>Posts</a></li><li class="breadcrumb-item active" aria-current=page>NeRF at NeurIPS 2022</li></ol></nav></div><article class=article><div class="article-container pt-3"><h1>NeRF at NeurIPS 2022</h1><div class=article-metadata><span class=article-date>Last updated on
Sep 30, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>8 min read</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/nerf/>NeRF</a>, <a href=/category/literature-review/>Literature Review</a></span></div></div><div class=article-container><div class=article-style><p>Inspired by <a href=https://dellaert.github.io target=_blank rel=noopener>Frank Dellaert</a> and his excellent series on the original <a href=https://dellaert.github.io/NeRF/ target=_blank rel=noopener>NeRF Explosion</a> and the following <a href=https://dellaert.github.io/NeRF21/ target=_blank rel=noopener>ICCV</a>/<a href=https://dellaert.github.io/NeRF22/ target=_blank rel=noopener>CVPR</a> conference gatherings, I decided to look into creating a NeurIPS 22 rundown myself.</p><p>The papers below are all the papers I could gather by browsing through the extensive list of <a href="https://nips.cc/Conferences/2022/Schedule?type=Poster" target=_blank rel=noopener>accepted NeurIPS papers</a>. I mainly collected all papers where the titles fit and did a brief scan through the paper or only the abstract if the paper wasn&rsquo;t published at the time of writing. If I have mischaracterized or missed any paper, please send me a DM on Twitter <a href=https://twitter.com/markb_boss target=_blank rel=noopener>@markb_boss</a> or via mail.</p><p><strong>Note</strong>: <em>images below belong to the cited papers, and the copyright belongs to the authors or the organization that published these papers. Below I use key figures or videos from some papers under the fair use clause of copyright law.</em></p><h2 id=nerf>NeRF</h2><p>Mildenhall <em>et al.</em> introduced NeRF at ECCV 2020 in the now seminal <a href=https://www.matthewtancik.com/nerf target=_blank rel=noopener>Neural Radiance Field paper</a>. As mentioned in the introduction, there has been fantastic progress in research in this field. NeurIPS this year is no exception to this trend.</p><p>What NeRF does, in short, is similar to the task of computed tomography scans (CT). In CT scans, a rotating head measures densities from x-rays. However, knowing where that measurement should be placed in 3D is challenging. NeRF also tries to estimate where a surface and view-dependent radiance is located in space. This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images. With many posed images, photorealistic novel view synthesis is possible.</p><div class="alert alert-note"><div>Rotate by clicking and dragging, zoom via mouse wheel, and use the buttons (Teal indicates active) to visualize the NeRF process.</div></div><iframe src=/talks/viz/nerf_explainer/ width=100% height=500px style=border:none></iframe><h3 id=fundamentals>Fundamentals</h3><p><figure id=figure-surface-reconstruction-improvements-with-improved-surface-reconstruction-using-high-frequency-detailshttpsarxivorgabs220607850><div class="d-flex justify-content-center"><div class=w-100><img alt="Surface reconstructions improvements with a high-frequency deformation field" srcset="/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_2d95c11ceca55453108f8ae0f2f49134.webp 400w,
/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_8f44542db3618cb5e9f001558da0d7f1.webp 760w,
/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_2d95c11ceca55453108f8ae0f2f49134.webp width=760 height=148 loading=lazy data-zoomable></div></div><figcaption>Surface reconstruction improvements with: <a href=https://arxiv.org/abs/2206.07850 target=_blank rel=noopener>Improved surface reconstruction using high-frequency details</a>.</figcaption></figure></p><p>These address more fundamental areas of view-synthesis with NeRF methods.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55157" target=_blank rel=noopener>NTRF</a></strong> extends NeRF to improve transmission and reflections, using neural transmittance fields and edge constraints.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55396" target=_blank rel=noopener>PNF</a></strong> introduces a new class of neural fields using basis-encoded polynomials. These can represent the signal as a composition of manipulable and interpretable components.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=52805" target=_blank rel=noopener>LoENerf</a></strong> introduces a Levels-of-Experts (LoE) framework to create a novel coordinate-based representation with an MLP. The weights of the MLP are hierarchical, periodic, and position-dependent. Each layer of the MLP has multiple candidate values of the weight matrix, which are individually tiled across the input space.</p><p><strong><a href=https://arxiv.org/abs/2206.07850 target=_blank rel=noopener>Improved surface reconstruction using high-frequency details</a></strong> can be achieved by splitting the base low-frequency shape into signed distance fields and the high-frequent details in a deformation field on the base shape.</p><p><strong><a href=https://arxiv.org/abs/2205.15674 target=_blank rel=noopener>Generalised Implicit Neural Representations</a></strong> venture far beyond the realm of euclidean coordinate systems and propose to observe the continuous high dimensional signal as a discrete graph and perform a spectral embedding on each node to establish the input for the neural field.</p><p><strong><a href=https://arxiv.org/abs/2205.15848 target=_blank rel=noopener>Geo-Neus</a></strong> introduces explicit multi-view geometry constraints to generate geometry consistent surface reconstructions. These losses include ones for signed distance function (SDF) from sparse structure-from-motion (SFM) point clouds and ones for photometric consistency.</p><p>While SDFs are often used to express geometry in NeRFs, they can only represent closed shapes. Using unsigned distance functions (UDF) can express open and watertight surfaces, but they can pose challenges in meshing. <strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55176" target=_blank rel=noopener>HSDF</a></strong> presents a learnable representation that combines the benefits of each.</p><h3 id=audio>Audio</h3><p><figure id=figure-results-from-nafhttpsarxivorgabs220400628-showing-the-sound-propagation-in-the-rooms-shown-above-from-the-emitter-indicated-in-red><div class="d-flex justify-content-center"><div class=w-100><img alt="Sound propagation modeling results from NAF" srcset="/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_66bf9ccee1aa19dbeb9938565b0413c5.webp 400w,
/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_35598e6df4c252308dda01e296ff9fa3.webp 760w,
/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_66bf9ccee1aa19dbeb9938565b0413c5.webp width=760 height=214 loading=lazy data-zoomable></div></div><figcaption>Results from <a href=https://arxiv.org/abs/2204.00628 target=_blank rel=noopener>NAF</a> showing the sound propagation in the rooms shown above from the emitter, indicated in red.</figcaption></figure></p><p>As sound propagates similar to rays in a volume, NeRF also found usage in this area.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55190" target=_blank rel=noopener>INRAS</a></strong> stores high-fidelity time domain impulse responses at arbitrary positions using neural fields. This allows modeling spatial acoustic for a scene efficiently.</p><p><strong><a href=https://arxiv.org/abs/2204.00628 target=_blank rel=noopener>NAF</a></strong> follow a similar approach to INRAS, learning impulse response functions in spatially varying scene.</p><h3 id=priors-and-generative>Priors and Generative</h3><p><figure id=figure-overview-of-neuformhttpsarxivorgabs220708890-which-combines-generalizable-priors-with-the-overfitting-of-typical-nerf-model-adaptively><div class="d-flex justify-content-center"><div class=w-100><img alt="Visualization of prior-based generalization and NeRF-style overfitting combination in NeuForm." srcset="/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_19972b94f1141cac9c8daa0b8d07fe46.webp 400w,
/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_b393e8ad9ea1ed746d58736be1c44bc0.webp 760w,
/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_19972b94f1141cac9c8daa0b8d07fe46.webp width=681 height=281 loading=lazy data-zoomable></div></div><figcaption>Overview of <a href=https://arxiv.org/abs/2207.08890 target=_blank rel=noopener>NeuForm</a> which combines generalizable priors with the overfitting of typical NeRF model adaptively.</figcaption></figure></p><p>Priors can either aid in the reconstruction or can be used in a generative manner. For example, in the reconstruction, priors either increase the quality of neural view synthesis or enable reconstructions from sparse image collections.</p><p>In <strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55117" target=_blank rel=noopener>CoCo-INR</a></strong>, two attention modules are used. One to extract useful information from the prior codebook and the other to encode these entries into the scene. With this prior, the method is capable of working on sparse images.</p><p>In sparse image collections, several areas are often rarely observed. To circumvent this issue, <strong><a href=https://arxiv.org/abs/2207.08890 target=_blank rel=noopener>NeuForm</a></strong> relies on generalizable category-specific representation in less observed areas. In well-observed areas, the method uses the accurate overfitting of NeRF.</p><p><strong><a href=https://arxiv.org/abs/2206.00665 target=_blank rel=noopener>MonoSDF</a></strong> integrates recent monocular depth and normal prediction networks as priors. Given these additional priors, the authors show performance improvement under either MLP neural fields or voxel-based grid methods.</p><p><strong><a href=https://arxiv.org/abs/2206.07695 target=_blank rel=noopener>VoxGRAF</a></strong> combines recent methods for speeding up NeRFs using voxel-based structures and 3D convolutions to generate novel objects in a single forward pass. These scenes can then be rendered from any viewpoint.</p><h3 id=articulated>Articulated</h3><p><figure id=figure-overview-of-nemfhttpsarxivorgabs220603287-which-generates-novel-motions-from-a-learned-prior-with-a-continuous-temporal-field><div class="d-flex justify-content-center"><div class=w-100><img alt="NeMF motion prior visualized" srcset="/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_83625c3cdb2e154623f52edaab166192.webp 400w,
/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_ac37d238defd72e34ef6c07538026258.webp 760w,
/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_83625c3cdb2e154623f52edaab166192.webp width=760 height=223 loading=lazy data-zoomable></div></div><figcaption>Overview of <a href=https://arxiv.org/abs/2206.03287 target=_blank rel=noopener>NeMF</a> which generates novel motions from a learned prior with a continuous temporal field.</figcaption></figure></p><p>Capturing dynamic objects is a trend that started in previous conference years. However, this year I noticed a trend in adding priors.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54986" target=_blank rel=noopener>Neural Shape Deformation Priors</a></strong> uses a transformer trained on large datasets to learn a prior of non-rigid transformations. A source mesh can be morphed with a continuous neural deformation field to a target mesh with a partially described surface deformation.</p><p><strong><a href=https://arxiv.org/abs/2206.03287 target=_blank rel=noopener>NeMF</a></strong> learns a prior of human and quadruped motion and uses it generatively in a neural motion field. The authors show use cases for motion interpolation or in-betweening.</p><p><strong><a href=https://arxiv.org/abs/2205.15723 target=_blank rel=noopener>DeVRF</a></strong> proposes to use 3D spatial voxel grids alongside a 4D deformation field to model dynamic scenes in a highly efficient manner.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54805" target=_blank rel=noopener>FNeVR</a></strong> enables animating faces with NeRFs by combining 2D motion warping with neural rendering.</p><p><strong><a href=https://arxiv.org/abs/2206.15258 target=_blank rel=noopener>NDR</a></strong> jointly optimizes the surface and deformation from a dynamic scene using monocular RGB-D cameras. They propose an invertible deformation network to provide cycle consistency between frames. As the topology might change in dynamic scenes, they also employ a strategy to generate topology variants.</p><h3 id=editable-and-composable>Editable and Composable</h3><p><figure id=figure-ccnerfhttpsarxivorgabs220514870-enables-compression-and-composition-of--nerfs><div class="d-flex justify-content-center"><div class=w-100><img alt="Examples of compression and composition with CCNeRF" srcset="/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_16364f0c3087528b920dca3fe481a91f.webp 400w,
/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_dc0ab6ad297ad599d0e1beb132739229.webp 760w,
/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_16364f0c3087528b920dca3fe481a91f.webp width=760 height=258 loading=lazy data-zoomable></div></div><figcaption><a href=https://arxiv.org/abs/2205.14870 target=_blank rel=noopener>CCNeRF</a> enables compression and composition of NeRFs.</figcaption></figure></p><p>This section covers NeRFs that provide methods for composing, controlling, or editing.</p><p><strong><a href=https://arxiv.org/abs/2205.15838v2 target=_blank rel=noopener>D<sup>2</sup>NeRF</a></strong> learns a decoupling of static and dynamic objects from monocular video. Here, two networks are trained separately, which handle the respective areas.</p><p><strong><a href=https://ylqiao.net/publication/2022nerf/ target=_blank rel=noopener>NeuPhysics</a></strong> allows editing a dynamic scene by editing physics parameters. This editing is performed on a hexahedra mesh tied to an underlying neural field in a two-way conversion. The physics simulation is done differently on the mesh and can propagate back to the neural field.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54786" target=_blank rel=noopener>CageNeRF</a></strong> uses low-poly cages around the object to create a simple deformation target. Deforming the cage is then simple in manual editing. The deformation can then be used to warp the previously trained NeRF.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55279" target=_blank rel=noopener>INSP-Net</a></strong> introduces signal processing into neural fields by introducing a differential operator framework, which can directly work on the fields without discretization. The authors even propose a CovnNet running on the neural representation.</p><p><strong><a href=https://arxiv.org/abs/2205.15585 target=_blank rel=noopener>Decomposing NeRF for Editing via Feature Field Distillation</a></strong> uses existing 2D feature extractors such as CLIP-LSeg to provide additional supervision to detect semantics in the 3D volume. Users can query based on text, image patches, or direct pixel selection to allow semantic editing.</p><p><strong><a href=https://arxiv.org/abs/2205.14870 target=_blank rel=noopener>CCNeRF</a></strong> uses a tensor rank decomposition to express the neural fields in a highly efficient and compressible manner. As the representation is explicit, the learned models are also composable.</p><h3 id=decomposition>Decomposition</h3><p><figure id=figure-overview-of-samuraihttpsarxivorgabs220515768-shows-the-applications-of-shape-brdf-and-illumination-decomposition-with-a-simultaneous-pose-recovery><div class="d-flex justify-content-center"><div class=w-100><img alt="Shape, BRDF, and illumination decomposition with a simultaneous pose recovery with SAMURAI." srcset="/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_aa542951c0ad27f1a98034a6abc4c123.webp 400w,
/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_276ba311288569682a06b1e6c576e6ab.webp 760w,
/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_aa542951c0ad27f1a98034a6abc4c123.webp width=760 height=280 loading=lazy data-zoomable></div></div><figcaption>Overview of <a href=https://arxiv.org/abs/2205.15768 target=_blank rel=noopener>Samurai</a> shows the applications of shape, BRDF, and illumination decomposition with a simultaneous pose recovery.</figcaption></figure></p><p>In this section, the radiance of NeRF is split into geometry, BRDF, and illumination. This enables consistent relighting under any illumination.</p><p><strong><a href=https://arxiv.org/abs/2206.03858 target=_blank rel=noopener>RENI</a></strong> uses <a href=https://www.vincentsitzmann.com/siren/ target=_blank rel=noopener>SIREN</a> and vector neurons to learn a neural prior on natural HDR illuminations. They extend the vector neurons to handle rotation equivariance for spherical images.</p><p><strong><a href=https://github.com/neuralps3d/neuralps3d target=_blank rel=noopener>Neural Reflectance Field from Shading and Shadow under a Fixed Viewpoint</a></strong> leverages a fixed viewpoint with a moving light source. This way, the geometry, and BRDF have to be recovered based on shading cues. Due to the neural fields, the 3D scene is recovered even from the fixed viewpoint.</p><p>In <strong><a href=https://arxiv.org/abs/2206.03380 target=_blank rel=noopener>Shape, Light & Material Decomposition from Images using Monte Carlo Rendering and Denoising</a></strong> no previously used low-frequency basis or pre-integration is used to express the environment illumination. Instead, regular Monte Carlo integration is used, which is passed through a denoising network to ease the learning process from noisy gradients.</p><p>In <strong><a href=https://arxiv.org/abs/2205.15768 target=_blank rel=noopener>SAMURAI</a></strong> the model can decompose coarsely posed image collections into shape, BRDF, and illumination. The cameras are also optimized during training with a camera multiplex. Especially, datasets of challenging scenes where traditional SFM methods fail can be decomposed with this method. <em>For transparency, I&rsquo;m the author of the paper</em></p><h3 id=other>Other</h3><p><figure id=figure-overview-of-the-perfceptionhttpsarxivorgabs220811537-dataset-with-its-applications><div class="d-flex justify-content-center"><div class=w-100><img alt="Overview of the new PeRFception dataset." srcset="/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_e5c475dca3ff95f87b124468ea564adf.webp 400w,
/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_0a9781da2c6107fcd23e432c3a6608ff.webp 760w,
/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_e5c475dca3ff95f87b124468ea564adf.webp width=760 height=333 loading=lazy data-zoomable></div></div><figcaption>Overview of the <a href=https://arxiv.org/abs/2208.11537 target=_blank rel=noopener>PeRFception</a> dataset with its applications.</figcaption></figure></p><p>Several works with excellent results in various fields.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55004" target=_blank rel=noopener>NeMF</a></strong> proposes to leverage neural fields for semantic correspondence matching. A coarse cost volume is used to guide a high-precision neural matching field.</p><p><strong><a href=https://arxiv.org/abs/2206.01724 target=_blank rel=noopener>SNAKE</a></strong> proposes introducing shape reconstruction with neural fields to aid in 3D keypoint detection for point clouds. No supervision for the detection is required.</p><p>Segmentation and concept learning in NeRFs can benefit from <strong><a href=https://arxiv.org/abs/2207.06403 target=_blank rel=noopener>3D Concept Grounding on Neural Fields</a></strong>, where a high-dimensional feature descriptor at each spatial location is taken from language embeddings.</p><p><strong><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=53336" target=_blank rel=noopener>ULNef</a></strong> specifically targets virtual try-on settings where multiple garments are layered. These layers are modeled as neural fields, and the collision handling is directly performed on the fields.</p><p>In <strong><a href=https://arxiv.org/abs/2206.01634 target=_blank rel=noopener>NeRF-RL</a></strong>, a neural field is used to learn a latent space as the state representation for the reinforcement learning algorithm.</p><p><strong><a href=https://arxiv.org/abs/2208.11537 target=_blank rel=noopener>PeRFception</a></strong> establishes new large-scale datasets for perception-related tasks such as segmentation, classification, etc. They also evaluate a plenoxels variant on these datasets.</p><h2 id=conclusion>Conclusion</h2><p>This was quite the amount of work, and I have enormous respect for Frank Dellaert for having it done thrice before. The field of NeRF moves incredibly fast, and even for a single conference, there exists a massive amount of reading material.</p><p>It is also interesting to see NeRF find new applications in other fields such as <a href=#audio>audio</a> and that sorting papers into a single category becomes challenging. This might be related to fast progress in the field and the high risk of getting scooped.</p></div><div class="media author-card content-widget-hr"><a href=https://markboss.me/><img class="avatar mr-3 avatar-circle" src=/author/mark-boss/avatar_hu8228efeb083742449721d94a9befbadd_1942861_270x270_fill_q75_lanczos_center.jpg alt="Mark Boss"></a><div class=media-body><h5 class=card-title><a href=https://markboss.me/>Mark Boss</a></h5><h6 class=card-subtitle>Research Scientist</h6><p class=card-text>I&rsquo;m a researcher at Unity Technologies with research interests in the intersection of machine learning and computer graphics.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:hello@markboss.me><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/markb_boss target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=y23cQ6wAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/vork target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/markbboss target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/nerf_at_eccv22/>NeRF at ECCV 2022</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/privacy/>Privacy Policy</a>
&#183;
<a href=/terms/>Legal details</a></p><p class="powered-by copyright-license-text">© 2023 Mark Boss. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26cbef546776b4d6032d1ea3dafadab.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/elm.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/kotlin.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/rust.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.fe369d7ba3ba979cfdb3e0f7e5240185.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.b0d291ed6d27eacec233e6cf5204f99a.js type=module></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script></body></html>