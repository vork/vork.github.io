<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Mark Boss</title><link>https://markboss.me/post/</link><atom:link href="https://markboss.me/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 May 2023 18:24:54 +0200</lastBuildDate><image><url>https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png</url><title>Posts</title><link>https://markboss.me/post/</link></image><item><title>NeRF at CVPR 2023</title><link>https://markboss.me/post/nerf_at_cvpr23/</link><pubDate>Mon, 01 May 2023 18:24:54 +0200</pubDate><guid>https://markboss.me/post/nerf_at_cvpr23/</guid><description>&lt;p>It is now my &lt;a href="https://markboss.me/category/literature-review/">third time&lt;/a> writing a summary of NeRFy things at a conference. This time it is the big one: CVPR. The &lt;a href="https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers" target="_blank" rel="noopener">list of accepted papers&lt;/a> is massive again, with 2359 papers.&lt;/p>
&lt;p>What is even more astounding is that the number of NeRF papers has grown significantly. I scanned the provisional program for potential NeRF titles and manually confirmed a relationship to the NeRF field.&lt;/p>
&lt;p>However, writing a summary is an entirely different task. Frank Dellaert contacted me on Twitter about the CVPR post this year, and he gave me an idea to automate the entire summary using an LLM and creating slides for the posts. He even provided me access to his tools, research database, and a ChatGPT API key. Thank you so much - the idea worked wonders :).&lt;/p>
&lt;p>So, I built a small program: ARCHIVE: Ai assisted Research Conference Human-readable Instant surVEy. It automatically creates a summary (thanks, OpenAI ðŸ˜…) and formats the blog posts below. As the blog post is rather long, I also extended the tool to auto-generate a &lt;a href="https://markboss.me/archive_presentations/cvpr23/">slide deck&lt;/a> which also showcases prominent figures from the papers (automatically extracted). The ESC key displays a slide overview and you can use the arrow keys to move through the presentation (Categories left/right, individual slides up/down). The slide title is also a link to the paper itself. The entire deck is automatically generated by the tool (including extracting images and figure descriptions).&lt;/p>
&lt;p>I am extending the tool to provide other use cases, but this will be left for another blog post.&lt;/p>
&lt;p>While I reread all summaries, my little tool might get something wrong, and I did not detect it during the final read. If this happened or I missed any paper, please send me a DM on Twitter &lt;a href="https://twitter.com/markb_boss" target="_blank" rel="noopener">@markb_boss&lt;/a> or via mail.&lt;/p>
&lt;p>Here we go again!&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: &lt;em>images below belong to the cited papers, and the copyright belongs to the authors or the organization that published these papers. Below I use key figures or videos from some papers under the fair use clause of copyright law.&lt;/em>&lt;/p>
&lt;h2 id="nerf">NeRF&lt;/h2>
&lt;p>Mildenhall &lt;em>et al.&lt;/em> introduced NeRF at ECCV 2020 in the now seminal &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">Neural Radiance Field paper&lt;/a>. As mentioned in the introduction, fantastic progress in research in this field has been made, and this is my third time covering it. CVPR 2023 is no exception, and NeRF finds applications in many new areas.&lt;/p>
&lt;p>What NeRF does, in short, is similar to the task of computed tomography scans (CT). In CT scans, a rotating head measures densities from X-rays. However, knowing where that measurement should be placed in 3D is challenging. NeRF also tries to estimate where a surface and view-dependent radiance is located in space. This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images. With many posed images, photorealistic novel view synthesis is possible.&lt;/p>
&lt;h3 id="fundamentals">Fundamentals&lt;/h3>
&lt;figure>
&lt;video src="https://dynibar.github.io/static/videos/new-teaser_3d.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Performance improvements with &lt;a href="https://dynibar.github.io" target="_blank" rel="noopener">DynIBaR&lt;/a> on complex dynamic novel view synthesis.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>These papers address more fundamental problems of view-synthesis with NeRF methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14001v1" target="_blank" rel="noopener">Grid-guided Neural Radiance Fields for Large Urban Scenes&lt;/a>&lt;/strong>: The authors propose a new methodology for high fidelity rendering in large urban scenes using a multiresolution ground feature plane representation in combination with an MLP-based neural radiance field (NeRF). This allows for the benefits of both a lightweight NeRF and joint-optimized ground planes, resulting in photorealistic novel views with fine details.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.04452v2" target="_blank" rel="noopener">Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos&lt;/a>&lt;/strong>: ReRF is introduced as a compact neural representation for long-duration dynamic scenes enabling real-time free-view video rendering, using a global coordinate-based tiny MLP as the feature decoder. A compact grid-based approach is utilized to handle large motions in interframe features. Improved compression and faster, higher-quality video generation were demonstrated using ReRF.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.08476v1" target="_blank" rel="noopener">SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory&lt;/a>&lt;/strong>: Neural Lightweight View Synthesis uses temporal consistency to render novel views faster than traditional techniques. A low-resolution feature map is generated first, and a lightweight 2D neural renderer is applied to generate the output image at target resolution leveraging the features of preceding and current frames.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.16386v1" target="_blank" rel="noopener">Compressing Volumetric Radiance Fields to 1 MB&lt;/a>&lt;/strong>: VQRF introduces a framework for compressing volumetric radiance fields by pruning grid models and applying vector quantization to improve compactness, resulting in a 100x compression ratio with minimal loss in visual quality. The proposed approach is generalizable across multiple volumetric structures and facilitates the use of volumetric radiance fields in real-world applications.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.07871v1" target="_blank" rel="noopener">DINER: Disorder-Invariant Implicit Neural Representation&lt;/a>&lt;/strong>: DINER overcomes the spectral limitations of implicit neural representations using a hash table. This allows the network to handle arbitrary ordering of input signals and generalize better across different tasks. The authors demonstrate the superiority of their approach compared to state-of-the-art algorithms across various tasks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.07418v1" target="_blank" rel="noopener">FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization&lt;/a>&lt;/strong>: FreeNeRF proposes a baseline for few-shot novel view synthesis with sparse inputs using frequency regularization on NeRF&amp;rsquo;s inputs and densities. This leads to superior performance compared to existing complicated methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.10537v1" target="_blank" rel="noopener">Learning Neural Duplex Radiance Fields for Real-Time View Synthesis&lt;/a>&lt;/strong>: Duplex Mesh Neural Radiance Fields (DM-NeRF) bakes neural radiance fields into mesh representations for better rendering performance and screen-space convolution. DM-NeRF distills and compresses the radiance information on a two-layer mesh structure to allow fast and accurate rendering with minimal MLP evaluations for each pixel. They show improved performance on standard datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2306.03092v2" target="_blank" rel="noopener">Neuralangelo: High-Fidelity Neural Surface Reconstruction&lt;/a>&lt;/strong>: Neuralangelo uses a multi-resolution 3D hash grid to learn representations for dense 3D surface structures from multi-view images and videos. It also uses numerical gradients and coarse-to-fine optimization for higher-fidelity surface reconstruction.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12562v2" target="_blank" rel="noopener">PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices&lt;/a>&lt;/strong>: PermutoSDF expands on Neural Radiance-Density field by using permutohedral lattice to encode the SDF and achieve faster optimization and high-frequency detail retrieval. The authors&amp;rsquo; regularization scheme is additionally crucial to high-frequency geometric detail recovery, while novel view rendering (using sphere tracing) is also achieved at a high fps rate.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.04268v1" target="_blank" rel="noopener">Multi-Space Neural Radiance Fields&lt;/a>&lt;/strong>: MS-NeRF represents scenes using parallel feature fields in sub-spaces to handle reflective and refractive objects. It is a modification of existing NeRF methods with small computational overheads, providing better rendering performance for complex light paths through mirrored objects.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rivas-Manzaneque_NeRFLight_Fast_and_Light_Neural_Radiance_Fields_Using_a_Shared_CVPR_2023_paper.html" target="_blank" rel="noopener">NeRFLight: Fast and Light Neural Radiance Fields using a Shared Feature Grid&lt;/a>&lt;/strong>: The authors propose a lightweight method for real-time view synthesis via a decoupled grid-based NeRF approach. The approach uses multiple density decoders that share one common feature grid. This results in a model that achieves real-time performance while maintaining high-quality models.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yoon_Cross-Guided_Optimization_of_Radiance_Fields_With_Multi-View_Image_Super-Resolution_for_CVPR_2023_paper.html" target="_blank" rel="noopener">Cross-Guided Optimization of Radiance Fields with Multi-View Image Super-Resolution for High-Resolution Novel View Synthesis&lt;/a>&lt;/strong>: The paper proposes a differentiable framework for cross-guided optimization of single-image super-resolution and radiance fields for high-resolution novel view synthesis. By performing multi-view image super-resolution during radiance fields optimization, train-view images obtain multi-view consistency and high-frequency details, leading to better performance in novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.14340v2" target="_blank" rel="noopener">HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization&lt;/a>&lt;/strong>: HelixSurf combines traditional multi-view stereo with neural implicit surface learning to improve scene geometry reconstruction. The method uses intermediate predictions from one strategy to guide the learning of the other in an iterative process.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.01735v2" target="_blank" rel="noopener">Neural Fourier Filter Bank&lt;/a>&lt;/strong>: The authors propose a grid-based paradigm for spatial decomposition, which is optimized to store the information both spatially and frequency-wise. The method uses adaptive sine activation features, and the network is composed of sine activation fully connected layers, which learn to decompose signals over different scales and frequencies progressively. The proposed method shows improved performance over the state-of-the-art techniques.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13791v1" target="_blank" rel="noopener">Progressively Optimized Local Radiance Fields for Robust View Synthesis&lt;/a>&lt;/strong>: The authors propose an algorithm to reconstruct a large-scale scene&amp;rsquo;s radiance field from a single video. They do so by jointly estimating camera poses and a radiance field in a progressive manner. Local radiance fields are trained to handle large and unbounded scenes within a temporal window.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2208.00277v5" target="_blank" rel="noopener">MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures&lt;/a>&lt;/strong>: PolyNeRF is a new approach for Neural Radiance Fields that replaces ray marching with polygon rasterization algorithms. This allows for fast and efficient rendering while maintaining high quality.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15060v1" target="_blank" rel="noopener">TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering&lt;/a>&lt;/strong>: The authors introduce a pipeline for creating textured meshes from a single smartphone by first using RGB-D aided structure from motion. This is followed by neural implicit surface reconstruction and differentiable rendering to generate finetuned texture maps that are closer to the original scene.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.05937v1" target="_blank" rel="noopener">Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction&lt;/a>&lt;/strong>: S-MPI improves MPI by approximating the 3D scene of the image with plane structures that produce high-quality results for both RGBA layers and plane poses. Instead of an MPI, S-MPI accounts for non-planar surfaces and multi-view consistency. A transformer-based network architecture is deployed to produce expressive S-MPI layers and corresponding masks, poses, and RGBA contexts.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2204.06552v3" target="_blank" rel="noopener">Neural Vector Fields for Implicit Surface Representation and Inference&lt;/a>&lt;/strong>: Vector Fields (VF) are proposed as a new implicit 3D shape representation, yielding faster convergence rates, higher data fidelity, and superior geometric detail compared to traditional distance-based approaches. VF&amp;rsquo;s significant advantage over existing methods is that the insertion of a singularity in the field wherever geometry changes occur enables learning of discontinuous surfaces and interior boundaries.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.11601v1" target="_blank" rel="noopener">Towards Better Gradient Consistency for Neural Signed Distance Functions via Level Set Alignment&lt;/a>&lt;/strong>: This paper proposes a level set alignment loss to better the accuracy of neural signed distance function (SDF) inference from point clouds or multi-view images. Their approach constrains gradients at queries to ensure better gradient consistency across the field. They demonstrate the effectiveness of the method through various benchmarks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.05187v1" target="_blank" rel="noopener">WIRE: Wavelet Implicit Neural Representations&lt;/a>&lt;/strong>: WIRE is a Wavelet-based Implicit neural REpresentation that uses a continuous complex Gabor wavelet activation function, allowing it to achieve high accuracy and robustness. The authors show that WIRE outperforms other INR models in image denoising, super-resolution, computed tomography reconstruction, and novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.08788v2" target="_blank" rel="noopener">MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs&lt;/a>&lt;/strong>: MixNeRF improves the efficiency of NeRF by using a mixture of distributions to estimate a ray&amp;rsquo;s RGB colors and a new training objective based on ray depth estimation. It outperforms other state-of-the-art methods and is more efficient in terms of both training and inference.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.08706v1" target="_blank" rel="noopener">Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections&lt;/a>&lt;/strong>: NeuS-HSR is a surface reconstruction framework based on implicit neural rendering, which can deal with high specular reflections of objects when captured through glasses. The framework parameterizes the object surface as an implicit signed distance function and decomposes the rendered image into a target object and auxiliary plane appearance to generate auxiliary plane appearances. NeuS-HSR outperforms state-of-the-art approaches in reconstructing target surfaces accurately and robustly against high specular reflections.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.14173v1" target="_blank" rel="noopener">NeuralUDF: Learning Unsigned Distance Fields for Multi-view Reconstruction of Surfaces with Arbitrary Topologies&lt;/a>&lt;/strong>: NeuralUDF introduces a new method for reconstructing arbitrary-topology surfaces from 2D images with volume rendering. By using Unsigned Distance Functions (UDFs) and a new density function, NeuralUDF enables high-quality reconstruction of non-closed shapes, achieving comparable performance to Signed Distance Function (SDF) based methods for closed surfaces.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2209.15511v2" target="_blank" rel="noopener">Sphere-Guided Training of Neural Implicit Surfaces&lt;/a>&lt;/strong>: SphereGuided jointly trains a neural distance function with a coarse sphere-based triangular reconstruction to improve sampling efficiency in high-frequency detail regions.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.17968v1" target="_blank" rel="noopener">VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence Normalization&lt;/a>&lt;/strong>: VDN-NeRF is a new method that normalizes the geometry of neural radiance fields (NeRF) by distilling invariant information encoded in the fields. This technique improves the synthesis of 3D scenes with dynamic lighting or non-Lambertian surfaces and minimizes shape-radiance ambiguity.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.12012v1" target="_blank" rel="noopener">NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from Multi-view Images&lt;/a>&lt;/strong>: NeAT is a new neural rendering framework that represents 3D surfaces as a level set of a signed distance function with a validity branch. This allows for learning implicit surfaces with arbitrary topologies from multi-view images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.08971v1" target="_blank" rel="noopener">SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Reconstruction of Indoor Scenes&lt;/a>&lt;/strong>: SurfelNeRF combines explicit geometric surfel representations with NeRF rendering to enable efficient online reconstruction and high-quality rendering. The method also includes a differentiable rasterization scheme for rendering the neural surfel radiance fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.14086v2" target="_blank" rel="noopener">ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision&lt;/a>&lt;/strong>: ShadowNeuS proposes to model shadow rays, which result from light sources in order to reconstruct an SDF neural model from single RGB views or corresponding shadow information when full scene sampling is not available. The approach allows the effective reconstruction of 3D models beyond the line of sight.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.03361v2" target="_blank" rel="noopener">Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision&lt;/a>&lt;/strong>: Nerflets are introduced as a local scene representation. Each Nerflet represents local information regarding object type, location, and orientation within a scene. By joint optimization of Nerflet parameters, efficient and structure-aware 3D scene representation can be obtained without global modeling.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15484v1" target="_blank" rel="noopener">Regularize implicit neural representation by itself&lt;/a>&lt;/strong>: INRR is introduced as a regularizer for the Implicit Neural Representation (INR). INRR measures the similarity between rows/columns of a matrix and integrates the smoothness of the Laplacian matrix by parameterizing learned Dirichlet Energy with a small INR. The method aims to improve the generalization of INR for signal representation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.00833v1" target="_blank" rel="noopener">RobustNeRF: Ignoring Distractors with Robust Losses&lt;/a>&lt;/strong>: Robust NeRF suggests incorporating an outlier rejection component to NeRF training, removing moving objects and ephemeral elements from the training data. The method is a simple optimization problem and works with existing NeRF frameworks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2209.00082v2" target="_blank" rel="noopener">Multi-View Reconstruction using Signed Ray Distance Functions (SRDF)&lt;/a>&lt;/strong>: This paper introduces a new optimization framework for multi-view 3D shape reconstructions using a novel volumetric shape representation that combines differentiable rendering with local depth predictions to yield pixel-wise geometric accuracy. The approach optimizes the depths of an implicit-parameterized shape representation at each 3D location. The method outperforms existing approaches for geometry estimation over standard 3D benchmarks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_Self-Supervised_Super-Plane_for_Neural_3D_Reconstruction_CVPR_2023_paper.html" target="_blank" rel="noopener">Self-supervised Super-plane for Neural 3D Reconstruction&lt;/a>&lt;/strong>: S3PRecon introduces self-supervised super-plane constraints for neural implicit surface representation methods to handle texture-less planar regions without any annotated datasets. An iterative training scheme of grouping pixels and optimizing the reconstruction network via a super-plane constraint is used to achieve better performance than using ground truth plane segmentation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.05594v1" target="_blank" rel="noopener">PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces&lt;/a>&lt;/strong>: PET-NeuS improves NeuS&amp;rsquo;s MLP sign distance field parametrization by representing the signed distance field using triplanes and MLPs in a mixture. This results in a more expressive data structure with noise. PET-NeuS ameliorates this noise by using a new learnable positional encoding and a self-attention convolution operation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_RefSR-NeRF_Towards_High_Fidelity_and_Super_Resolution_View_Synthesis_CVPR_2023_paper.html" target="_blank" rel="noopener">RefSR-NeRF: Towards High Fidelity and Super Resolution View Synthesis&lt;/a>&lt;/strong>: RefSR-NeRF improves NeRF super-resolution images by first generating a low-resolution image and then using a high-resolution reference image to reconstruct high-frequency details. The authors design a novel lightweight RefSR model for learning the inverse degradation process from NeRF renderings to target HR images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11082v3" target="_blank" rel="noopener">DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>&lt;/strong>: DynIBAR synthesizes realistic views in the presence of large camera and object movements in videos by adapting the image-based rendering method in a scene-motion-aware manner. Efficient and performs well against long videos of complex scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_F2-NeRF_Fast_Neural_Radiance_Field_Training_With_Free_Camera_Trajectories_CVPR_2023_paper.html" target="_blank" rel="noopener">F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories&lt;/a>&lt;/strong>: F^2-NeRF introduces a new warping method, called perspective warping, in the context of grid-based NeRFs, enabling it to handle unbounded scenes. The method shows significant performance improvements compared to other approaches on various free-camera single-object datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.02375v2" target="_blank" rel="noopener">NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction&lt;/a>&lt;/strong>: NeuDA proposes a hierarchical approach to implicit surface reconstruction using anchor grids to capture 3D contexts. By maintaining an adaptive anchor structure, capturing different topological structures is achieved.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_PlenVDB_Memory_Efficient_VDB-Based_Radiance_Fields_for_Fast_Training_and_CVPR_2023_paper.html" target="_blank" rel="noopener">PlenVDB: Memory Efficient VDB-Based Radiance Fields for Fast Training and Rendering&lt;/a>&lt;/strong>: PlenVDB aims to accelerate the training and inference stages in NeRFs by introducing the VDB hierarchical sparsely-filled data structure. This method accomplished results with faster training convergence, compressed data for NeRF models, and faster rendering on commercial hardware.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.07743v1" target="_blank" rel="noopener">SeaThru-NeRF: Neural Radiance Fields in Scattering Media&lt;/a>&lt;/strong>: NeRF in the Fog modifies NeRF to account for the medium&amp;rsquo;s transmission and scattering. The authors use SeaThru&amp;rsquo;s image formation model and propose a suitable architecture. The method can also render clear views, removing the medium between the camera and the scene.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.16630v2" target="_blank" rel="noopener">DINER: Depth-aware Image-based NEural Radiance fields&lt;/a>&lt;/strong>: DINER uses depth information to guide the reconstruction of a volumetric neural radiance field representation for 3D object rendering. DINER achieves higher synthesis quality than the state-of-the-art methods and can capture scenes more completely with greater disparity.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.09069v2" target="_blank" rel="noopener">Masked Wavelet Representation for Compact Neural Radiance Fields&lt;/a>&lt;/strong>: The authors propose a method to compress grid-based neural fields and make them more efficient using wavelet transforms. Their approach includes a trainable masking technique that produces a more compact representation and achieves state-of-the-art performance within a memory budget of 2 MB.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12285v2" target="_blank" rel="noopener">Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for Neural Radiance Fields&lt;/a>&lt;/strong>: Exact-NeRF computes the Integrated Positional Encoding in a pyramid-based, precise analytical approach, rather than an approximated conical one. The paper shows that this new approach outperforms the approximated model when scenes are distant or extended.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.06919v2" target="_blank" rel="noopener">NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer&lt;/a>&lt;/strong>: NeRFLiX trains an inter-viewpoint mixer to remove rendering artifacts like noise and blur in existing NeRF models, using a NeRF degradation modeling approach and inter-viewpoint aggregation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.10080v1" target="_blank" rel="noopener">NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering&lt;/a>&lt;/strong>: NeUDF is an extension to signed distance function (SDF) reconstruction algorithms to recover arbitrary shapes with open surfaces. The method utilizes the unsigned distance function (UDF) as the underlying representation and employs two new formulations of weight function and normal regularization strategy for efficient volume rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.06689v4" target="_blank" rel="noopener">TINC: Tree-structured Implicit Neural Compression&lt;/a>&lt;/strong>: TINC proposes a tree-structured approach to compress implicit neural representations of data through partitioned local MLP fitting. The parameter-sharing approach helps to capture both local and non-local correlations across the scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13817v1" target="_blank" rel="noopener">ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field&lt;/a>&lt;/strong>: ABLE-NeRF improves the view-dependent effects of Neural Radiance Fields (NeRF) in volumetric rendering by using a self-attention-based framework along rays and Learnable Embeddings to capture local lighting. The method reduces the artifacts on several materials, resulting in higher-quality rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.12652v1" target="_blank" rel="noopener">Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur&lt;/a>&lt;/strong>: The Hybrid Neural Rendering model uses a combination of neural and image-based representations to render high-fidelity, view-consistent images, even for large-scale scenes with motion blur. Additionally, the authors propose methods to simulate the blur effects and reduce the impact of blurriness during training.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13805v1" target="_blank" rel="noopener">Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container&lt;/a>&lt;/strong>: ReNeuS is a novel method for recovering the 3D geometry of objects in transparent enclosures. It models the scene as two sub-spaces, using an existing method NeuS to represent the inner sub-space. A combination of volume rendering and ray tracing is used to render the model, and then the geometry and appearance are recovered by minimizing differences between real and hybrid-rendered images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.08057v1" target="_blank" rel="noopener">Real-Time Neural Light Field on Mobile Devices&lt;/a>&lt;/strong>: MobileNeRF introduces a mobile-friendly network architecture that runs in real-time on mobile devices for neural rendering of 3D scenes, with high-resolution generation and similar image quality as NeRF. It requires low latency and small size, saving 15-24 times the storage compared with MobileNeRF.&lt;/p>
&lt;h3 id="priors-and-generative">Priors and Generative&lt;/h3>
&lt;figure>
&lt;video src="https://raw.githubusercontent.com/paintingnature/paintingnature.github.io/master/static/videos/4.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Create realistic 3D landscape synthesis using a single semantic mask using &lt;a href="https://arxiv.org/abs/2302.07224" target="_blank" rel="noopener">Painting 3D Nature in 2D&lt;/a>.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>Priors can either aid in the reconstruction or can be used in a generative manner. For example, in the reconstruction, priors either increase the quality of neural view synthesis or enable reconstructions from sparse image collections.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.07702v2" target="_blank" rel="noopener">Learning 3D-aware Image Synthesis with Unknown Pose Distribution&lt;/a>&lt;/strong>: The PoF3D method frees generative radiance fields from the requirements of 3D pose priors. It equips the generator with an efficient pose learner to infer a pose from a latent code and assigns the discriminator a task to learn pose distribution. The pose-free generator and the pose-aware discriminator are jointly trained in an adversarial manner.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.07224v1" target="_blank" rel="noopener">Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask&lt;/a>&lt;/strong>: The paper introduces a novel approach for 3D-aware image synthesis that can produce photorealistic multi-view consistent color images of natural scenes. The key idea is to use a semantic field as an intermediate representation and convert it to a radiance field using semantic image synthesis models. The method outperforms baseline methods and requires only a single semantic mask input.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11674v2" target="_blank" rel="noopener">Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion&lt;/a>&lt;/strong>: The end-to-end monetizable NeRF (MeNRF) framework generates high-quality 3D reconstructions with accurate pose and appearance from a single image of arbitrary topologies. MeNRF leverages an unconditional 3D-aware generator while using a hybrid inversion scheme to refine the solution via optimization without exploiting multiple views. The network can de-render an image in as few as 10 steps, which is useful for practical applications.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.12746v1" target="_blank" rel="noopener">Local Implicit Ray Function for Generalizable Radiance Field Representation&lt;/a>&lt;/strong>: LIRF is a novel approach to neural rendering that aggregates information from conical frustums to construct each ray resulting in high-quality novel view rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.08247v1" target="_blank" rel="noopener">Multiview Compressive Coding for 3D Reconstruction&lt;/a>&lt;/strong>: MCC is a method for single-view 3D reconstruction that operates on 3D points of single objects or whole scenes. Its efficient size compression allows large-scale training from diverse RGB-D videos for learning a generalizable representation. MCC shows strong generalization to novel objects and objects captured in the wild.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.13223v2" target="_blank" rel="noopener">Generalizable Implicit Neural Representations via Instance Pattern Composers&lt;/a>&lt;/strong>: The authors present a new method for implicit neural representations (INRs) which improves generalization by learning a small set of weights that modulate an early layer of the MLP network while keeping the remaining MLP weights constant. The resulting pattern composition rules enable the network to represent common features across instances.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12046v4" target="_blank" rel="noopener">DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors&lt;/a>&lt;/strong>: DP-NeRF proposes a clean novel framework to handle blurry images for 3D reconstruction. The approach utilizes two physical priors for color consistency and 3D geometric consistency, which are derived from the actual blurring process during image acquisition by the camera. The authors show that the proposed method improves the perceptual quality of the constructed scene in synthetic and real scenes with both camera motion blur and defocus blur.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Song_DIFu_Depth-Guided_Implicit_Function_for_Clothed_Human_Reconstruction_CVPR_2023_paper.html" target="_blank" rel="noopener">DIFu: Depth-Guided Implicit Function for Clothed Human Reconstruction&lt;/a>&lt;/strong>: DIFu is a new method for single image clothed human reconstruction. It uses projected depth information to create a voxel-aligned human reconstruction that can contain pixels with detailed 3D information, such as hair and clothing and estimates occupancies with pixel and voxel-aligned features. The method also includes a texture inference branch for color estimation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.05321v1" target="_blank" rel="noopener">HumanGen: Generating Human Radiance Fields with Explicit Priors&lt;/a>&lt;/strong>: HumanGen is a 3D human generation method that combines various priors from 2D and 3D models of humans using an anchor image. It features a hybrid feature representation, pronged design for geometry and appearance generation, and incorporates off-the-shelf 2D latent editing methods into 3D. The method generates view-consistent radiance fields with detailed geometry and realistic free-view rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.09869v2" target="_blank" rel="noopener">RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation&lt;/a>&lt;/strong>: RenderDiffusion is a diffusion-based model for 3D inference, training and generation. The authors present a novel image denoising method that provides consistency with a 3D intermediate representation. The method enables 2D inpainting for editing 3D scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.00774v1" target="_blank" rel="noopener">Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&lt;/a>&lt;/strong>: A properly trained diffusion model can be used to backpropagate score through the Jacobian of a differentiable renderer. A scene representation example can be a voxel radiance field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15012v1" target="_blank" rel="noopener">3D-Aware Multi-Class Image-to-Image Translation with NeRFs&lt;/a>&lt;/strong>: The paper proposes a new method for 3D-aware multi-class image-to-image (I2I) translation using a combination of a 3D-aware GAN step and a 3D-aware I2I translation step. The authors introduce a new conditional architecture and training strategy for the multi-class GAN and several new techniques for the I2I translation step to improve view-consistency.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.07600v1" target="_blank" rel="noopener">Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures&lt;/a>&lt;/strong>: This paper proposes Latent-NeRF, which uses score distillation adapted to latent diffusion models for text-guided image generation. The authors integrate sketch-shape constraints to control the 3D shape generation process and apply latent score distillation on 3D meshes. Implementation is available at the provided GitHub link.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.11052v3" target="_blank" rel="noopener">ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real Novel View Synthesis via Contrastive Learning&lt;/a>&lt;/strong>: The paper reports that models trained on synthetic data tend to produce sharper but less accurate volume densities. To address this, a geometry-aware contrastive learning approach is introduced, and cross-view attention is adopted. The method helps to render higher quality and better detailed images when working with synthetic-to-real novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13582v1" target="_blank" rel="noopener">SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates&lt;/a>&lt;/strong>: SCADE improves NeRF reconstruction quality by leveraging depth estimates produced with monocular depth estimation models, which can generalize across scenes. It uses a space carving loss to fusing multiple hypothesized depth maps from each view and distilling from them a consistent geometry.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.08067v2" target="_blank" rel="noopener">VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction&lt;/a>&lt;/strong>: VolRecon is introduced as a more generalizable neural implicit scene reconstruction method with Signed Ray Distance Function. It projects multi-view features and combines them with volume features.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14662v1" target="_blank" rel="noopener">OTAvatar: One-shot Talking Face Avatar with Controllable Tri-plane Rendering&lt;/a>&lt;/strong>: OTAvator is a one-shot learning system for 3D facial avatars. It employs tri-plane volumetric rendering with an efficient CNN and disentangles facial identity and motion representing using a decoupling-by-inverting strategy, which allows to create new avatars quickly from as few as one reference portrait.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.16509v2" target="_blank" rel="noopener">HoloDiffusion: Training a 3D Diffusion Model using 2D Images&lt;/a>&lt;/strong>: The authors address the challenge of 3D training data scarcity and the memory complexity of 3D extension by introducing a new diffusion model that can be trained using only 2D images and an image formation model that decouples model memory from spatial memory. The approach is shown to be competitive with existing techniques on the CO3D dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.03267v1" target="_blank" rel="noopener">NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a>&lt;/strong>: NeRDi is a NeRF-based single-view 3D reconstruction method with general image priors from 2D diffusion models, using pre-trained vision-language models for conditional input and depth maps for geometric regularization.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13515v1" target="_blank" rel="noopener">Persistent Nature: A Generative Model of Unbounded 3D Worlds&lt;/a>&lt;/strong>: The authors present a method for the unconditioned synthesis of unbounded nature scenes based on an extendible planar scene layout grid and a panoramic skydome. Renderer can freely move through the space without auto-regression.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.16677v1" target="_blank" rel="noopener">3D Neural Field Generation using Triplane Diffusion&lt;/a>&lt;/strong>: TriDiff is a diffusion-based model for 3D-aware generation of neural fields. TriDiff preprocesses training data by converting meshes to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Training the diffusion model with these representations yields high-quality 3D neural fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shim_Diffusion-Based_Signed_Distance_Fields_for_3D_Shape_Generation_CVPR_2023_paper.html" target="_blank" rel="noopener">Diffusion-Based Signed Distance Fields for 3D Shape Generation&lt;/a>&lt;/strong>: SDF-Diffusion proposes a two-stage generative process by using diffusion models. The first stage generates a low-resolution SDF, which is further processed in the second stage to obtain high-resolution results. The network can generate complex high-resolution 3D shapes using 3D SDF previously used for shape completion tasks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Towards_Unbiased_Volume_Rendering_of_Neural_Implicit_Surfaces_With_Geometry_CVPR_2023_paper.html" target="_blank" rel="noopener">Towards Unbiased Volume Rendering of Neural Implicit Surfaces with Geometry Priors&lt;/a>&lt;/strong>: The paper proposes a new way to render Signed Distance Functions, where the scale factor is dependent on angle to the normal vector of the surface, which leads to a reduction in bias in volume rendering. The authors pre-train a Multi-View Stereo network for supervision at zero crossing intersection points between the implicit surface and the viewing frustum.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.html" target="_blank" rel="noopener">Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild&lt;/a>&lt;/strong>: NPF is a novel Neural Proto-face Field that disentangles the common/specific facial cues to allow precise face modeling. It is able to learn 3D-consistent identity via uncertainty modeling and multi-image priors from photo collections. The disentangled learning methodology predicts superior 3D face shapes and textures compared to the state-of-the-art methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.10440v2" target="_blank" rel="noopener">Magic3D: High-Resolution Text-to-3D Content Creation&lt;/a>&lt;/strong>: Magic3D is a two-stage optimization framework that uses a diffusion prior to obtain a coarse model and accelerates it with a sparse 3D hash grid structure. It further optimizes a textured 3D mesh model to create high-quality 3D mesh models in 40 minutes, which is 2x faster than DreamFusion.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.01206v2" target="_blank" rel="noopener">DiffRF: Rendering-guided 3D Radiance Field Diffusion&lt;/a>&lt;/strong>: DiffRF proposes a novel volumetric radiance field synthesis based on denoising diffusion probabilistic models. The framework is designed to generate radiance fields by rendering a set of posed images with a deviated prior, which contains multi-view consistent priors with good quality for image synthesis. Unlike 3D GANs, the method allows free-view synthesis through learning multi-view consistent priors at inference time.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.12231v2" target="_blank" rel="noopener">DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models&lt;/a>&lt;/strong>: NeRF-Prior improves NeRF training introducing regularizing RGBD patch priors. These priors are learned with a denoising diffusion model and can improve the generalization of reconstructed geometry and color fields to novel scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.04965v1" target="_blank" rel="noopener">Seeing a Rose in Five Thousand Ways&lt;/a>&lt;/strong>: The proposed method in this work is capable of learning object intrinsics (geometry, texture, and material) from a single image of a certain object category, such as roses, and then use this knowledge to generate different images of the same object under changing poses and lighting conditions. The resulting model shows a superior performance across various related tasks compared to existing methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.09787v1" target="_blank" rel="noopener">NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models&lt;/a>&lt;/strong>: NeuralField-LDM is a generative model that can synthesize complex 3D environments using Latent Diffusion Models for efficiency and high-quality 3D content. With a scene autoencoder, voxel grids, and latent-autoencoder, the authors improve upon existing scene generation models and demonstrate potential uses in 3D content creation applications.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.17260v2" target="_blank" rel="noopener">SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene&lt;/a>&lt;/strong>: SinGRAF is a 3D-aware generative model that can generate photorealistic 3D objects with few input images. The 3D GAN architecture enables SinGRAF to produce different realizations of a scene while preserving appearance and varying the layout.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.11424v1" target="_blank" rel="noopener">Polynomial Implicit Neural Representations For Large Diverse Datasets&lt;/a>&lt;/strong>: Poly-INR is a new implicit neural representation technique that replaces sinusoidal positional encoding with polynomial functions. The proposed model eliminates the need for positional encodings and performs comparably to state-of-the-art generative models with far fewer trainable parameters.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.06287v2" target="_blank" rel="noopener">NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds&lt;/a>&lt;/strong>: NeRFVS utilizes holistic priors such as pseudo-depth maps and view coverage information to guide the learning of implicit neural representations of 3D indoor scenes. Robust depth loss and variance loss are proposed to further improve the performance, and these losses are modulated during NeRF optimization according to the view coverage information to reduce the influence of view coverage imbalance. The method achieves high-fidelity free navigation results on indoor scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13777v1" target="_blank" rel="noopener">GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images&lt;/a>&lt;/strong>: GM-NeRF synthesizes novel view images for human performers using a geometry-guided attention mechanism and neural rendering. This allows efficient improvement of perceptual quality of synthesis and outperforms state-of-the-art methods in terms of novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.02163v1" target="_blank" rel="noopener">GINA-3D: Learning to Generate Implicit Neural Assets in the Wild&lt;/a>&lt;/strong>: GINA-3D uses camera and LiDAR data to learn 3D assets of vehicles and pedestrians using a generative approach. The method decouples representation learning and generative modeling into two stages with a tri-plane latent structure, which is shown to perform better than existing approaches when evaluated on a large-scale object-centric dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11208v2" target="_blank" rel="noopener">Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars&lt;/a>&lt;/strong>: The authors propose a novel 3D GAN framework for unsupervised learning of high-quality facial avatars from unstructured 2D images. The method introduces a new 3D representation called generative texture-rasterized tri-planes. The proposed representation accurately models deformation and flexibility, enabling fine-grained expression control and animation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.16431v2" target="_blank" rel="noopener">NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360Â° Views&lt;/a>&lt;/strong>: NeuralLift-360 is a technique that generates a 3D object with 360-degree views that correspond well with a reference image, easing workflows for 3D artists and XR designers. It uses a depth-aware NeRF and denoising diffusion models guided by CLIP to provide coherent guidance and can be guided with rough depth estimation in the wild through a ranking loss. The method outperforms existing state-of-the-art baselines.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.17235v1" target="_blank" rel="noopener">NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation&lt;/a>&lt;/strong>: This paper proposes a method to finetune NeRF-GAN models to generate high-fidelity animation of real subjects based on a single image. The method includes 2D loss functions to reduce the identity gap, as well as explicit and implicit 3D regularizations to remove artifacts.&lt;/p>
&lt;h3 id="dynamic">Dynamic&lt;/h3>
&lt;figure>
&lt;video src="https://blendfields.github.io/videos/concatenated/supplementary_expression.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Neural fields to create blends of various facial expressions using &lt;a href="https://arxiv.org/abs/2305.07514" target="_blank" rel="noopener">BlendFields&lt;/a>.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>Capturing dynamic objects is a trend that started in previous conference years. This can either be solved with parametrization or via neural priors.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.07514v1" target="_blank" rel="noopener">BlendFields: Few-Shot Example-Driven Facial Modeling&lt;/a>&lt;/strong>: The authors propose a solution for fine-grained face rendering that blends sparse expressions to infer the appearance of unseen expressions. The fine-grained details are captured as appearance differences between each of the extreme poses in their method. The approach is robust and generalizes well beyond faces.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12499v2" target="_blank" rel="noopener">INSTA - Instant Volumetric Head Avatars&lt;/a>&lt;/strong>: INSTA uses a neural radiance field-based pipeline to reconstruct digital avatars from a single monocular RGB video. The output model, based on a parametric face model, offers high-quality rendering and interactivity. It also performs well in unseen pose conditions.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14124v1" target="_blank" rel="noopener">Towards Scalable Neural Representation for Diverse Videos&lt;/a>&lt;/strong>: D-NeRV is an INR-based framework designed for encoding long and diverse videos. It decouples visual content from motion information while introducing temporal reasoning into the implicit neural network to improve compression results. The proposed model surpasses NeRV and traditional video compression techniques while also achieving higher accuracy on action recognition tasks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.12237v2" target="_blank" rel="noopener">Learning Neural Volumetric Representations of Dynamic Humans in Minutes&lt;/a>&lt;/strong>: The paper presents a novel technique to accelerate the learning process of neural radiance fields for free-viewpoint video reconstruction from sparse multi-view videos. The proposed method uses a part-based voxelized representation and a 2D motion parameterization scheme to increase convergence rates. The method is shown to achieve competitive visual quality with a much faster training process.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12497v3" target="_blank" rel="noopener">MagicPony: Learning Articulated 3D Animals in the Wild&lt;/a>&lt;/strong>: MagicPony is a method for predicting detailed 3D articulated shapes and appearances of animals using only single-view images of the same category. It utilizes a novel implicit-explicit representation that combines the strengths of neural fields and meshes. MagicPony includes a self-supervised visual transformer and a viewpoint sampling technique to improve performance and generalization.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.02633v1" target="_blank" rel="noopener">HNeRV: A Hybrid Neural Representation for Videos&lt;/a>&lt;/strong>: HNeRV uses content-adaptive embeddings and re-designed architecture to outperform existing methods in video regression besides allowing for higher resolution and fewer parameters. The method also shows advantages in video decoding for speed, flexibility, and deployment. HNeRV can be used in downstream tasks like video compression and inpainting.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2306.07579v1" target="_blank" rel="noopener">Parametric Implicit Face Representation for Audio-Driven Facial Reenactment&lt;/a>&lt;/strong>: This work proposes a novel audio-driven facial reenactment framework that uses a parametric, interpretable implicit face representation. It improves audio-to-expression parameters encoding, uses conditional image synthesis, and data augmentation techniques to achieve high-quality results with more fidelity to the identities and speaking styles of speakers.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.14593v1" target="_blank" rel="noopener">NIRVANA: Neural Implicit Representations of Videos with Adaptive Networks and Autoregressive Patch-wise Modeling&lt;/a>&lt;/strong>: NIRVANA adopts a patch-wise approach to video compression where groups of frames are each fitted to separate networks to exploit temporal redundancy. The method uses autoregressive modeling, quantized network parameters, and scaling based on GPU use to achieve joint improvements in quality, speed, and scalability with efficient variable bitrate compression.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.02238v2" target="_blank" rel="noopener">HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling&lt;/a>&lt;/strong>: HyperReel is a 6-DoF video representation with a hyper network for ray-conditioned sample prediction. It has a compact and memory-efficient dynamic volume representation and outperforms existing approaches in visual quality, memory requirements, and frame rate.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.06544v1" target="_blank" rel="noopener">DNeRV: Modeling Inherent Dynamics via Difference Neural Representation for Videos&lt;/a>&lt;/strong>: DNeRV introduces the difference frame as an essential channel for implicit video representation, resulting in state-of-the-art performance in intraprediction and video compression on x264.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.02001v1" target="_blank" rel="noopener">MonoHuman: Animatable Human Neural Field from Monocular Video&lt;/a>&lt;/strong>: MonoHuman proposes a three-part pipeline to generate view-consistent and high-fidelity avatars under arbitrary novel poses. A shared bidirectional deformation module creates a pose-independent, generalizable deformation field, followed by a forward correspondence search module. Finally, a rendering network leverages multi-view consistent features to produce the final avatar. The authors show improved performance over current state-of-the-art methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13825v1" target="_blank" rel="noopener">HandNeRF: Neural Radiance Fields for Animatable Interacting Hands&lt;/a>&lt;/strong>: HandNeRF uses pose estimation to generate a detailed explicit triangle mesh of interacting hands from multi-view images. They design a shared axis space for multiple poses, allowing each pose to add to the view space. A neural feature distillation method is used to enhance image quality while avoiding artifacts. Expensive ground-truth data is used to remove occlusions in the learning process.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14368v1" target="_blank" rel="noopener">FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views&lt;/a>&lt;/strong>: FlexNeRF provides photorealistic free-viewpoint rendering of people in motion from monocular videos. The method handles fast and complex motions under sparse views through a joint optimization approach where canonical time and pose are optimized with pose-dependent motion fields and pose-independent temporal deformations. The approach proposes novel consistency constraints and provides improved performance over existing benchmarks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Flow_Supervision_for_Deformable_NeRF_CVPR_2023_paper.html" target="_blank" rel="noopener">Flow Supervision for Deformable NeRF&lt;/a>&lt;/strong>: The authors present a deformable NeRF method that uses optical flow as supervision, with improvements over baselines that don&amp;rsquo;t use flow supervision. They show that inverting the backward deformation function is not needed for computing scene flows between frames, simplifying the problem.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.13660v1" target="_blank" rel="noopener">NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action&lt;/a>&lt;/strong>: NeMo is a neural motion field optimized to reconstruct 3D human motion from multiple video instances of the same action. The method outperforms existing monocular HMR methods in terms of 2D keypoint detection and achieves better 3D reconstruction compared to baselines on a small MoCap dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14435v1" target="_blank" rel="noopener">NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects&lt;/a>&lt;/strong>: The paper presents a modified version of NeRF called NeRF-DS for rendering novel views from RGB video input with dynamic scenes, which is capable of modeling the reflected color of specular surfaces during motion. NeRF-DS conditions the radiance field function on surface position and orientation in the observation space and uses a mask of moving objects to guide the deformation field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.09311v2" target="_blank" rel="noopener">Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields&lt;/a>&lt;/strong>: The proposed method learns spatiotemporal neural representations for scenes using neural network modules or 4D hash grids for extracting and interpolating features from space-time inputs, achieving state-of-the-art performance and/or 100 times faster training speed.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.02239v2" target="_blank" rel="noopener">Robust Dynamic Radiance Fields&lt;/a>&lt;/strong>: DRF-Net improves the robustness of dynamic radiance field reconstruction by jointly estimating the static and dynamic radiance fields alongside camera parameters. The method demonstrates improved performance on challenging videos compared to state-of-the-art methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Distilling_Neural_Fields_for_Real-Time_Articulated_Shape_Reconstruction_CVPR_2023_paper.html" target="_blank" rel="noopener">Distilling Neural Fields for Real-Time Articulated Shape Reconstruction&lt;/a>&lt;/strong>: The authors present a method for real-time reconstruction of articulated 3D models from video without test-time optimization or manual 3D supervision. The method trains a fast feed-forward network using off-the-shelf video-based dynamic NeRFs as 3D supervision to reconstruct arbitrary deformations represented by articulated bones and blend skinning.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.11113v1" target="_blank" rel="noopener">Implicit Neural Head Synthesis via Controllable Local Deformation Fields&lt;/a>&lt;/strong>: The paper presents a method of generating personalized 3D head avatars from 2D videos, with sharper deformations and greater facial detail compared to existing methods. This is achieved through a novel formulation of multiple implicit deformation fields with local semantic rig-like control and a local control loss, as well as an attention mask mechanism.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.09632v2" target="_blank" rel="noopener">HexPlane: A Fast Representation for Dynamic Scenes&lt;/a>&lt;/strong>: HexPlane proposes a new method to represent dynamic 3D scenes explicitly using six planes of learned features. HexPlane computes color features efficiently for voxels by fusing six vectors. Its combination with a small MLP produces impressive results in novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.03184v1" target="_blank" rel="noopener">Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream&lt;/a>&lt;/strong>: Instant-NVR proposes a monocular tracking and rendering system for complex human-object interactions in real-time. The authors use a hybrid deformation module and an online reconstruction strategy for efficient rendering. The system can capture the dynamic and static radiance fields for image synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14243v1" target="_blank" rel="noopener">DyLiN: Making Light Field Networks Dynamic&lt;/a>&lt;/strong>: DyLiN is proposed to handle non-rigid deformations in dynamic light fields in a computationally efficient manner. The method is based on learning a deformation field and lifting them into a higher dimensional space for handling discontinuities. CoDyLiN is proposed to handle controllable attributes in addition to non-rigid deformations.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11610v2" target="_blank" rel="noopener">Tensor4D: Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering&lt;/a>&lt;/strong>: Tensor4D uses a 4D tensor decomposition model for capturing dynamic 3D scenes from sparse-view camera rigs or even a monocular camera. The tensor is decomposed hierarchically into three time-aware volumes and nine compact feature planes. The proposed tensor factorization scheme allows for structural motion and detailed changes to be learned from coarse to fine.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.10241v2" target="_blank" rel="noopener">K-Planes: Explicit Radiance Fields in Space, Time, and Appearance&lt;/a>&lt;/strong>: K-Planes is a white-box model for radiance fields in arbitrary dimensions that represents a d-dimensional scene using choose-2 planes, making it easy to add dimension-specific priors. Despite using a linear feature decoder, it yields similar performance to a non-linear black-box MLP decoder with low memory usage and fast optimization. The method achieves state-of-the-art reconstruction fidelity whereby one can easily add temporal smoothness and other structure priors.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.06717v1" target="_blank" rel="noopener">Representing Volumetric Videos as Dynamic MLP Maps&lt;/a>&lt;/strong>: The paper proposes a method for real-time view synthesis of dynamic 3D scenes by representing the radiance field of each frame as a set of shallow MLPs stored as &amp;ldquo;MLP maps,&amp;rdquo; and dynamically predicted by a shared 2D CNN decoder. This achieves high rendering quality with state-of-the-art efficiency and speed.&lt;/p>
&lt;h3 id="editable-and-composable">Editable and Composable&lt;/h3>
&lt;p>
&lt;figure id="figure-the-left-image-shows-the-observation-the-central-one-shows-the-optimization-process-of-onsfhttpsarxivorgabs230608748-and-the-right-the-optimized-light-position-highlighted-as-a-green-dot">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s-tian.github.io/assets/actionosf/light_pose_opt.gif" alt="The left image shows the observation, the central one shows the optimization process of &amp;lt;a href=&amp;#34;https://arxiv.org/abs/2306.08748&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;ONSF&amp;lt;/a&amp;gt;, and on the right, the optimized light position is highlighted as a green dot." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The left image shows the observation, the central one shows the optimization process of &lt;a href="https://arxiv.org/abs/2306.08748" target="_blank" rel="noopener">ONSF&lt;/a>, and the right the optimized light position highlighted as a green dot.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>This section covers NeRFs that propose composing, controlling, or editing methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2306.08748v1" target="_blank" rel="noopener">Multi-Object Manipulation via Object-Centric Neural Scattering Functions&lt;/a>&lt;/strong>: Object-centric neural scattering functions (OSFs) are used as object representations to enable compositional scene re-rendering under object rearrangement and varying lighting conditions. This approach leads to improved model-predictive control performance and generalization in compositional multi-object environments.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.03049v1" target="_blank" rel="noopener">NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds&lt;/a>&lt;/strong>: NeuralEditor is a shape-editing algorithm that works on the explicit point cloud representation of NeRF. It employs K-D tree-guided density-adaptive voxels to perform deterministic integration and optimize the neural network. The resulting point cloud is then used to perform shape editing to achieve state-of-the-art results on shape deformation tasks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.11966v1" target="_blank" rel="noopener">Removing Objects From Neural Radiance Fields&lt;/a>&lt;/strong>: The paper presents a method for inpainting objects in an already generated NeRF using confidence-based view selection. A user-provided mask is used to overwrite data likely to contain the object, and the NeRF is then re-trained with individual 2D images selected by the view selection procedure.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2203.14402v5" target="_blank" rel="noopener">UV Volumes for Real-time Rendering of Editable Free-view Human Performance&lt;/a>&lt;/strong>: The UV Volumes proposes an approach for rendering human performers in real-time for VR/AR applications. It separates the high-frequency appearance from the 3D volume and encodes them into 2D texture stacks, which allows faster computation with shallower neural networks while maintaining editability and generalization. Other applications, like retexturing, are also made possible using this approach.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.06344v3" target="_blank" rel="noopener">DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization&lt;/a>&lt;/strong>: DPLayer is a differentiable parameterization layer to pick a meaningful region of a mesh for low-distortion parameterization. The method uses a neural segmentation network to learn to select the 3D region, which is then parameterized in 2D.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.html" target="_blank" rel="noopener">SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields&lt;/a>&lt;/strong>: The paper proposes a novel 3D inpainting method for removing unwanted objects from a 3D scene. The method leverages learned 2D image inpainters and a 3D segmentation mask to address the challenges of view consistency and geometric validity. Additionally, the authors introduce a dataset comprised of real-world scenes to evaluate the effectiveness of the proposed method.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.10699v2" target="_blank" rel="noopener">PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields&lt;/a>&lt;/strong>: PaletteNeRF provides an efficient and realistic approach to edit the appearance of neural radiance fields. The appearance can be decomposed into palettes shared across the scene and optimized alongside the basis functions. The method allows efficient editing of the appearance through direct modification of the color palettes. Additionally, compressed semantic features can be introduced for semantic-aware appearance editing.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13232v1" target="_blank" rel="noopener">Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization&lt;/a>&lt;/strong>: LipRF uses a Lipschitz mapping to stylize 3D scenes photorealistically. LipRF couples pre-trained NeRF with 2D photorealistic style transfer and learns 3D styles from views.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Occlusion-Free_Scene_Recovery_via_Neural_Radiance_Fields_CVPR_2023_paper.html" target="_blank" rel="noopener">OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields&lt;/a>&lt;/strong>: The proposed method directly maps occlusion-free scene details from position and viewing angles with Neural Radiance Field. The scheme optimizes both camera parameters and scene reconstruction in the presence of occlusions without the need for labeled external data for training.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.10448v1" target="_blank" rel="noopener">ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects&lt;/a>&lt;/strong>: ReNe is a dataset that contains real-world object scenes captured with one-light-at-a-time (OLAT) conditions. The dataset contains complex geometries and challenging materials. The authors perform an ablation study to identify a lightweight architecture capable of rendering objects under novel light conditions and establish a non-trivial baseline for the new dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.04247v2" target="_blank" rel="noopener">EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points&lt;/a>&lt;/strong>: EditableNeRF models dynamic scenes by detecting key points and weights. Then the key points can be dragged and dropped, allowing for editing from single-camera image sequences.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.10598v3" target="_blank" rel="noopener">StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields&lt;/a>&lt;/strong>: StyleRF is a 3D style transfer method that performs style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes for reliable geometry restoration via volume rendering, with deferred style transformation of 2D feature maps that ensure high-quality zero-shot style transfer across a variety of new styles. The proposed method performs sampling-invariant content transformation to ensure multi-view consistency.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.02766v2" target="_blank" rel="noopener">Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization&lt;/a>&lt;/strong>: Ref-NPR uses a reference image as a style source to stylize a 3D scene using radiance fields, with view interpolation and semantic disambiguation fills occlusion areas with visuals consistent with the stylized reference.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13277v2" target="_blank" rel="noopener">SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field&lt;/a>&lt;/strong>: SinE lets users easily edit NeRFs using semantic strokes or text prompts. Techniques like cyclic constraints with a proxy mesh, color compositing, and feature cluster-based regularization are used to stabilize the process. The authors show examples of both real-world and synthetic data that achieve high-quality multi-view consistency.&lt;/p>
&lt;h3 id="pose-estimation">Pose Estimation&lt;/h3>
&lt;figure>
&lt;video src="https://rover-xingyu.github.io/L2G-NeRF/images/girl.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Pose alignment with &lt;a href="https://arxiv.org/abs/2211.11505" target="_blank" rel="noopener">L2G-NeRF&lt;/a> compared to prior work.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>Estimating the pose of objects or the camera is a fundamental problem in computer vision. This can also be done to improve the quality of scenes with noisy camera poses.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11505v3" target="_blank" rel="noopener">Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields&lt;/a>&lt;/strong>: L2G-NeRF is a bundle-adjustment method for finding accurate camera poses for Neural Radiance Fields in novel view synthesis. The method applies a pixel-wise local alignment, followed by a frame-wise global alignment using differentiable parameter estimation solvers. L2G-NeRF improves reconstruction and outperforms the state-of-the-art.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14478v1" target="_blank" rel="noopener">DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields&lt;/a>&lt;/strong>: DBARF is an extension of BARF for GeNeRFs. The geometric cost feature map used in DBARF has been shown to support self-supervised learning, allowing it to be trained across domains, in contrast to state of the art.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.09682v1" target="_blank" rel="noopener">AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training&lt;/a>&lt;/strong>: AlignNerf combines convolutional layers with MLPs in high-resolution NeRF reconstructions and also includes a novel training strategy and a high-frequency aware loss to improve reconstruction quality, performing better than other state-of-the-art NeRF models in high-frequency detail recovery.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.07388v3" target="_blank" rel="noopener">NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior&lt;/a>&lt;/strong>: The authors propose a novel method for training and rendering NeRF from mobile camera videos. The method works better with the addition of monocular depth prior to the relative motion estimation between frames. The authors show that their method has promising results for mobile camera use cases.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11738v3" target="_blank" rel="noopener">SPARF: Neural Radiance Fields from Sparse and Noisy Poses&lt;/a>&lt;/strong>: SPARF is introduced to allow novel view synthesis from a few input views given noisy camera poses. It exploits multi-view geometry constraints to jointly refine camera poses and estimate the NeRF. SPARF sets a new state-of-the-art in the sparse-view regime on multiple datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12853v1" target="_blank" rel="noopener">BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields&lt;/a>&lt;/strong>: The authors present BAD-NeRF, a bundle-adjusted deblur neural radiance field, which models the physical image formation process to jointly learn camera poses and NeRF parameters and is robust to motion blur. They show superior performance over prior works on synthetic and real datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2211.12018" target="_blank" rel="noopener">Level-S2fM: Structure from Motion on Neural Level Set of Implicit Surfaces&lt;/a>&lt;/strong>: This paper introduces Level-S2fM, a neural incremental Structure-from-Motion (SfM) approach that uses coordinate MLPs to estimate camera poses and scene geometry from uncalibrated images. It addresses challenges in optimizing volumetric neural rendering with unknown camera poses and demonstrates promising results in camera pose estimation, scene geometry reconstruction, and neural implicit rendering.&lt;/p>
&lt;h3 id="decomposition">Decomposition&lt;/h3>
&lt;figure>
&lt;video src="https://nv-tlabs.github.io/fegr/assets/FEGR_asset_clip.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>&lt;a href="http://arxiv.org/abs/2304.03266v1" target="_blank" rel="noopener">Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes&lt;/a> allows exporting captured urban objects into any graphics engine.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>In this section, the radiance of NeRF is split into geometry, BRDF, and illumination. This enables consistent relighting under any illumination.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.16617v1" target="_blank" rel="noopener">NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination&lt;/a>&lt;/strong>: The authors of this paper present an inverse rendering pipeline that considers near-field indirect illumination and uses path tracing Monte Carlo sampling. They demonstrate state-of-the-art performance in inter-reflection decomposition by introducing radiance consistency constraints between implicit neural radiance and path tracing results.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_I2-SDF_Intrinsic_Indoor_Scene_Reconstruction_and_Editing_via_Raytracing_in_CVPR_2023_paper.html" target="_blank" rel="noopener">I2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs&lt;/a>&lt;/strong>: I^2-SDF is a neural radiance field-based framework that jointly estimates shapes, incident radiance, and materials for indoor scene reconstruction and editing. The neural radiance field is decomposed into a spatially-varying material through differentiable Monte Carlo raytracing that enables photorealistic scene relighting and editing applications.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15101v2" target="_blank" rel="noopener">DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering&lt;/a>&lt;/strong>: DANI-Net is an inverse rendering framework for uncalibrated photometric stereo problems. It incorporates differentiable shadow handling and anisotropic reflectance modeling in its design.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Complementary_Intrinsics_From_Neural_Radiance_Fields_and_CNNs_for_Outdoor_CVPR_2023_paper.html" target="_blank" rel="noopener">Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting&lt;/a>&lt;/strong>: The proposed framework combines NeRF and CNNs for outdoor scene relighting through intrinsic image decomposition. NeRF provides richer and more reliable pseudo-labels for CNNs training to predict interpretable lighting parameters, which enable realistic relighting.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14092v2" target="_blank" rel="noopener">NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images&lt;/a>&lt;/strong>: NeuFace introduces an approximated BRDF integration and a low-rank prior to create accurate and physically-meaningful 3D facial representations using neural rendering techniques. The method incorporates neural BRDFs into physically based rendering, allowing for the capture of complex facial geometry and appearance clues.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.03266v1" target="_blank" rel="noopener">Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes&lt;/a>&lt;/strong>: The presented method jointly reconstructs geometry, materials, and HDR lighting from posed RGB images. An explicit mesh is used to model high-order lighting effects such as shadows. The method disentangles complex geometry and materials from lighting effects for photorealistic relighting and virtual object insertion.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.10206v4" target="_blank" rel="noopener">Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes&lt;/a>&lt;/strong>: TexIR proposes a Texture-based Lighting representation of indoor scenes that models direct and infinite-bounce indirect lighting. A hybrid lighting representation with precomputed irradiance helps with efficiency and material optimization noise. The method enables mixed-reality applications such as material editing, novel view synthesis, and relighting.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.11900v1" target="_blank" rel="noopener">Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting&lt;/a>&lt;/strong>: The paper presents a 3D human reconstruction framework that includes a visibility field in addition to the occupancy field and the albedo field. A discretized visibility is supplied with coupled 3D depth and 2D image features, and a TransferLoss is proposed to improve the alignment between visibility and occupancy fields. The proposed method improves reconstruction accuracy and achieves accurate relighting comparable to ray-traced ground truth.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14190v1" target="_blank" rel="noopener">WildLight: In-the-wild Inverse Rendering with a Flashlight&lt;/a>&lt;/strong>: The authors propose a photometric approach to inverse rendering for unknown ambient lighting. The approach exploits a smartphone&amp;rsquo;s flashlight to produce a minimal light source and decomposes image intensities into a static appearance that corresponds to ambient flux and a dynamic reflection induced by the flashlight.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.12461v1" target="_blank" rel="noopener">TensoIR: Tensorial Inverse Rendering&lt;/a>&lt;/strong>: TensoIR proposes an inverse rendering approach based on tensor factorization and neural fields, allowing for efficient and physically-based model estimation for multi-view images in unknown lighting conditions. This provides photorealistic novel view synthesis and relighting results.&lt;/p>
&lt;h3 id="other">Other&lt;/h3>
&lt;figure>
&lt;video src="https://neural-lens.github.io/assets/opt_timelapse.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Neural Fields can be used to learn a differentiable and bijective lens with &lt;a href="https://arxiv.org/abs/2304.04848" target="_blank" rel="noopener">Neural Lens Modeling&lt;/a>.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>Several works with excellent results in various fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.12995v2" target="_blank" rel="noopener">Raw Image Reconstruction with Learned Compact Metadata&lt;/a>&lt;/strong>: The paper proposes a framework to learn a compressed latent representation of raw images containing fewer metadata than commonly compressed raw image files. The method features an sRGB-guided context model with improved entropy estimation strategies. The compressed representation enables the allocation of more bits for important regions.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.17603v1" target="_blank" rel="noopener">NeRF-Supervised Deep Stereo&lt;/a>&lt;/strong>: This paper proposes a novel framework for training deep stereo networks without ground-truth by using a combination of Neural Rendering and NeRF-supervised training. The rendered stereo triplets are used to compensate for occlusions and depth maps as proxy labels. The proposed model shows a significant improvement over existing self-supervised methods on the Middlebury dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.16493v1" target="_blank" rel="noopener">AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation&lt;/a>&lt;/strong>: AnyFlow generates optical flow accurately by representing it as a coordinate-based representation. It can accurately estimate flow from images of various resolutions and performs better in detail preservation of tiny objects than previous models when given low-resolution inputs.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.04848v1" target="_blank" rel="noopener">Neural Lens Modeling&lt;/a>&lt;/strong>: NeuroLens is an end-to-end optimization method for image distortion and vignetting, which can be used for point projection and ray casting. It allows performing pre-capture and post-reconstruction calibration, outperforming standard methods and being easy to use.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.00613v2" target="_blank" rel="noopener">NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation&lt;/a>&lt;/strong>: In NeuWigs, two stages are used to model human hair for virtual reality: the first learns a latent space of 3D hair states and the second performs temporal hair transfer. The learned model outperforms the state-of-the-art and can create new hair animations without additional observations.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.04328v1" target="_blank" rel="noopener">Neural Voting Field for Camera-Space 3D Hand Pose Estimation&lt;/a>&lt;/strong>: NVF unifies the two-stage process used traditionally in hand pose estimation, directly predicting 3D dense local evidence and global hand geometry via point-wise voting of 3D points in the camera frustum to alleviate 2D-to-3D ambiguities.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.00341v1" target="_blank" rel="noopener">JacobiNeRF: NeRF Shaping with Mutual Information Gradients&lt;/a>&lt;/strong>: JacobiNeRF learns to encode mutual correlation patterns between entities via maximizing their mutual information and is used to improve label propagation for sparse label regimes, semantic, and instance segmentation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Peters_pCON_Polarimetric_Coordinate_Networks_for_Neural_Scene_Representations_CVPR_2023_paper.html" target="_blank" rel="noopener">pCON: Polarimetric Coordinate Networks for Neural Scene Representations&lt;/a>&lt;/strong>: Polarimetric Coordinate Networks (pCON) is an architecture designed to preserve polarimetric information and address artifacts created by coordinate network architecture when reconstructing three polarimetric quantities of interest. The current state-of-the-art models do not consider preserving physical quantities.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2306.07970v1" target="_blank" rel="noopener">Neural Scene Chronology&lt;/a>&lt;/strong>: The proposed scene representation Space-Time Radiance Field (STRF) can model discrete scene-level changes as piece-wise constant temporal step functions. By using this representation, the proposed method can reconstruct a time-varying 3D scene model from internet imagery while separating scene-level and illumination changes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.04531v2" target="_blank" rel="noopener">ORCa: Glossy Objects as Radiance Field Cameras&lt;/a>&lt;/strong>: The proposed method uses glossy objects to recover the 5D environment radiance field visible to them by conversion into radiance-field cameras, which can be used to create noise-free images in real-time and to synthesize novel viewpoints. This method can also image around occluders in a scene while estimating object geometry, radiance, and the radiance field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12886v3" target="_blank" rel="noopener">OReX: Object Reconstruction from Planar Cross-sections Using Neural Fields&lt;/a>&lt;/strong>: OReX uses a Neural Field as an interpolation prior to reconstruct 3D shapes from planar cross-sections. The approach involves iterative estimation architecture and a hierarchical input sampling scheme. A regularization scheme is employed to alleviate gradient ripples. OReX outperforms previous methods and scales well with input size.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.18139v2" target="_blank" rel="noopener">Efficient View Synthesis and 3D-based Multi-Frame Denoising with Multiplane Feature Representations&lt;/a>&lt;/strong>: The authors propose a 3D-based multi-frame denoising method that outperforms 2D-based methods with lower computational requirements. The approach extends the multiplane image framework with a learnable encoder-renderer pair manipulating multiplane representations in feature space for better performance.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11646v3" target="_blank" rel="noopener">NeRF-RPN: A general framework for object detection in NeRFs&lt;/a>&lt;/strong>: NeRF-RPN is an object detection framework that directly operates on NeRF to detect objects in 3D. It exploits a novel voxel representation and multi-scale 3D neural volumetric features to regress the 3D bounding boxes of objects without rendering the NeRF at any viewpoint. A benchmark dataset with both synthetic and real-world data is also provided.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.01838v2" target="_blank" rel="noopener">vMAP: Vectorised Object Mapping for Neural Field SLAM&lt;/a>&lt;/strong>: vMAP is a dense SLAM system using MLPs to represent objects, enabling efficient, incrementally-built models without 3D priors. The authors show this approach to be more efficient and effective than previous neural field SLAM systems.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.08504v1" target="_blank" rel="noopener">PersonNeRF: Personalized Reconstruction from Photo Collections&lt;/a>&lt;/strong>: PersonNeRF is a method that builds a customized neural volumetric 3D model from a collection of photos of a subject captured across multiple years, enables the rendering of the subject with arbitrary combinations of viewpoint, body pose, and appearance. It addresses the issue of sparse observations by recovering a canonical T-pose neural volumetric representation of the subject that allows for changing appearance across different observations but uses a shared pose-dependent motion field across all observations.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Worchel_Differentiable_Shadow_Mapping_for_Efficient_Inverse_Graphics_CVPR_2023_paper.html" target="_blank" rel="noopener">Differentiable Shadow Mapping for Efficient Inverse Graphics&lt;/a>&lt;/strong>: The authors show that pre-filtered shadow mapping can be combined with existing differentiable rasterizers to allow efficient shadow computation for inverse graphics problems. Such a technique has faster convergence than differentiable light transport simulation and allows for better results in implicit 3D reconstruction.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chang_Depth_Estimation_From_Indoor_Panoramas_With_Neural_Scene_Representation_CVPR_2023_paper.html" target="_blank" rel="noopener">Depth Estimation from Indoor Panoramas with Neural Scene Representation&lt;/a>&lt;/strong>: The proposed method for depth estimation from multi-view indoor panoramic images with the Neural Radiance Field technology outperforms previous works by a large margin in quantitative and qualitative evaluations. Two networks were developed to learn the Signed Distance Function for depth measurement and the Radiance Field from panoramas, respectively, as well as a novel spherical position embedding scheme. A geometric consistency loss leveraging surface normal further refines depth estimation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.19590v2" target="_blank" rel="noopener">Neural Kernel Surface Reconstruction&lt;/a>&lt;/strong>: This paper introduces enhancement over the Neural Kernel Field (NKF) method. The new method, SparseVox, uses compactly supported kernel functions, making it robust to noise, trainable with any dataset of dense oriented points, and capable of reconstructing millions of points in a few seconds. SparseVox also outperforms NKF for reconstructing scenes, objects, and outdoor environments.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.00304v4" target="_blank" rel="noopener">Renderable Neural Radiance Map for Visual Navigation&lt;/a>&lt;/strong>: The Renderable Neural Radiance Map (RNR-Map) is a grid structure that stores latent codes and extracts the radiance field of images using the grid positions. This structure serves as a visual guideline for efficient navigation and localization. It provides data for camera tracking, visual localization, and curved scenarios navigation that are fast and robust to environmental changes and actuation noises.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.08556v1" target="_blank" rel="noopener">NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis&lt;/a>&lt;/strong>: SPARTN uses NeRFs to synthetically inject corrective noise into visual robotic manipulation policies, eliminating the need for expert supervision or additional interaction. It improves success rates by 2.8x over imitation learning without augmentation and even outperforms some methods that use online supervision.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.04823v2" target="_blank" rel="noopener">GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields&lt;/a>&lt;/strong>: GazeNeRF is a 3D-aware method for gaze redirection that models the face and eye volumes separately and projects them onto a 2D image by finetuning using a 3D rotation matrix.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14158v1" target="_blank" rel="noopener">BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects&lt;/a>&lt;/strong>: Neural Object Field (NOF) combines 6-DoF tracking and 3D reconstruction for arbitrary objects, with learning and optimization done at the same time. It produces a high-fidelity reconstruction of objects even without visual textures and performs well in sequences with large pose changes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.08730v2" target="_blank" rel="noopener">Novel-view Acoustic Synthesis&lt;/a>&lt;/strong>: ViGAS learns to synthesize the sound of an arbitrary point in 3D space from audio-visual input. The authors introduce a novel-view acoustic synthesis task, which is addressed by learning to reason about the spatial acoustics cues. To enable this work, two large-scale multi-view datasets have been collected: one synthetic and one real.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.07653v2" target="_blank" rel="noopener">NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images&lt;/a>&lt;/strong>: NEF is a learned implicit curve representation based on neural networks. It is optimized with view-based rendering loss and is able to output 3D feature curves without relying on 3D geometric operators or cross-view correspondence. On a synthetic benchmark, NEF outperforms existing methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15126v1" target="_blank" rel="noopener">NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation&lt;/a>&lt;/strong>: NeuralPCI introduces Neural Field Interpolation, an end-to-end 4D spatio-temporal neural field for 3D point cloud interpolation, extrapolation, morphing, and auto-labeling to handle both indoor and outdoor scenarios&amp;rsquo; large non-linear motions. It achieves state-of-the-art performance on the DHB (Dynamic Human Bodies) and NL-Drive datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.02493v3" target="_blank" rel="noopener">Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields&lt;/a>&lt;/strong>: CaFi-Net can produce a canonical coordinate-based implicit representation of an object category without the need for pre-aligned datasets, using a Siamese network architecture for category-level canonicalization. It extracts features from radiance fields and estimates canonical fields with consistent 3D pose. The method is tested on a dataset of 1300 NeRF models across 13 object categories and compared to point cloud-based methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13014v1" target="_blank" rel="noopener">Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention&lt;/a>&lt;/strong>: The proposed S-Ray model combines semantic understanding of radiance and cross-view attention mechanisms to learn from multiple scenes efficiently while providing generalizable results.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2206.11896v3" target="_blank" rel="noopener">EventNeRF: Neural Radiance Fields from a Single Colour Event Camera&lt;/a>&lt;/strong>: The authors present an approach to dense and photorealistic view synthesis from a single color event stream. This is done with a neural radiance field trained in a self-supervised way using a tailored ray sampling strategy. The resulting method produces significantly denser and more visually appealing renderings than existing methods while being robust in challenging scenarios.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.09802v1" target="_blank" rel="noopener">Panoptic Lifting for 3D Scene Understanding with Neural Fields&lt;/a>&lt;/strong>: Panoptic Lifting presents a new method to learn 3D panoptic representations of in-the-wild scenes using a neural field representation trained on machine-generated 2D panoptic segmentation masks. The method accounts for inconsistencies in 2D instance identifiers and incorporates improvements to make it more robust to noisy labels. The approach shows improvement in scene-level PQ over state-of-the-art methods on several datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.html" target="_blank" rel="noopener">Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving&lt;/a>&lt;/strong>: ImplicitO reports an implicit representation of the occupancy of the street with a flow factor. This approach reduces computational cost as the motion planner can directly query it and avoids preoccupying the infrastructure with unnecessary computational work. The model also uses a global attention mechanism.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>That was many papers, and compared to the previous &lt;a href="https://dellaert.github.io/NeRF22/" target="_blank" rel="noopener">CVPR&lt;/a> (57 papers), the paper count has now grown to 175. CVPR 23 alone had more papers than NeurIPS, ECCV, and CVPR 2022 combined (140).&lt;/p>
&lt;p>I have the feeling my little ARCHIVE tool will come in handy this year.&lt;/p>
&lt;p>Apart from the sheer number of papers, there are prominent trends in the papers. To no one&amp;rsquo;s surprise, leveraging NeRFs for anything generative is a substantial new area, and it shows that NeRF and its extension are a mighty tool for this task. I also noticed many papers in the decomposition area - very close to my heart. This is great, and here also, NeRFs mainly play the role of a tool in the reconstruction process. There are also several papers in the SLAM field. So while Frank Dellaert originally termed it as the &lt;a href="https://dellaert.github.io/NeRF/" target="_blank" rel="noopener">NeRF Explosion&lt;/a>, we may now enter the time of the NeRFusion, where NeRF becomes a building block in many different areas.&lt;/p>
&lt;p>Especially with the current speed of research and new trendy fields constantly emerging, it is exciting to see where NeRF will head next :).&lt;/p></description></item><item><title>NeRF at ECCV 2022</title><link>https://markboss.me/post/nerf_at_eccv22/</link><pubDate>Sat, 01 Oct 2022 18:24:54 +0200</pubDate><guid>https://markboss.me/post/nerf_at_eccv22/</guid><description>&lt;p>I recently went through the &lt;a href="https://eccv2022.ecva.net/program/provisional-program/" target="_blank" rel="noopener">the provisional programm&lt;/a> of ECCV 2022. After my last &lt;a href="https://markboss.me/post/nerf_at_neurips22/">post on &amp;ldquo;NeRF at NeurIPS&amp;rdquo;&lt;/a> got such great feedback, and I anyway compiled a list of all NeRFy things, I decided to do it all again.&lt;/p>
&lt;p>I again tried to find all papers by parsing the titles of the provisional program. A brief scan through the paper or abstract was then done to confirm if it is NeRFy and get a rough idea about the paper. If I have mischaracterized or missed any paper, please send me a DM on Twitter &lt;a href="https://twitter.com/markb_boss" target="_blank" rel="noopener">@markb_boss&lt;/a> or via mail.&lt;/p>
&lt;p>Here we go again!&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: &lt;em>images below belong to the cited papers, and the copyright belongs to the authors or the organization that published these papers. Below I use key figures or videos from some papers under the fair use clause of copyright law.&lt;/em>&lt;/p>
&lt;h2 id="nerf">NeRF&lt;/h2>
&lt;p>Mildenhall &lt;em>et al.&lt;/em> introduced NeRF at ECCV 2020 in the now seminal &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">Neural Radiance Field paper&lt;/a>. As mentioned in the introduction, there has been fantastic progress in research in this field. NeurIPS this year is no exception to this trend.&lt;/p>
&lt;p>What NeRF does, in short, is similar to the task of computed tomography scans (CT). In CT scans, a rotating head measures densities from x-rays. However, knowing where that measurement should be placed in 3D is challenging. NeRF also tries to estimate where a surface and view-dependent radiance is located in space. This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images. With many posed images, photorealistic novel view synthesis is possible.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
Rotate by clicking and dragging, zoom via mouse wheel, and use the buttons (Teal indicates active) to visualize the NeRF process.
&lt;/div>
&lt;/div>
&lt;iframe
src="https://markboss.me/talks/viz/nerf_explainer/"
width="100%"
height="500px"
style="border:none;">
&lt;/iframe>
&lt;h3 id="fundamentals">Fundamentals&lt;/h3>
&lt;figure>
&lt;video autoplay loop >
&lt;source src="https://snap-research.github.io/R2L/static/videos/DONeRF/forest.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
&lt;p>Light field distillation with &lt;a href="https://arxiv.org/abs/2203.17261" target="_blank" rel="noopener">R2L&lt;/a>. Here, on the left, the regular NeRF rendering is compared with the distilled light field. The quality improves with the light field (NeRF PSNR: 28.11 vs. R2L PSNR: 34.18).&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>These papers address more fundamental problems of view-synthesis with NeRF methods.&lt;/p>
&lt;p>Typically in streaming, the quality of the signal is adjusted depending on the connection quality. NeRFs, on the other hand, encode the entire signal in the weights. &lt;strong>&lt;a href="https://arxiv.org/abs/2207.09663" target="_blank" rel="noopener">Streamable Neural Fields&lt;/a>&lt;/strong> aims to fix that by encoding everything into smaller sub-networks that can be streamed over time.&lt;/p>
&lt;p>While NeRFs provide photorealistic novel view synthesis results, they do not offer a way to estimate the certainty of their reconstruction. Especially in medical or autonomous driving, this is a highly desirable property. &lt;strong>&lt;a href="https://arxiv.org/abs/2203.10192" target="_blank" rel="noopener">Conditional-Flow NeRF&lt;/a>&lt;/strong> aims to alleviate this by leveraging a flexible data-driven approach with conditional normalizing flows coupled with latent variable modeling.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.17261" target="_blank" rel="noopener">R2L&lt;/a>&lt;/strong> aims to directly learn a surface light field from a pre-trained NeRF. This way, only a single point per ray needs to be evaluated, drastically improving the inference performance.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.10312" target="_blank" rel="noopener">AdaNeRF&lt;/a>&lt;/strong> aims to speed up NeRF rendering by learning to reduce the sample count drastically using a split network architecture. One network predicts the sampling, and the other the shading. By enforcing sparsity during training, the number of samples can be lowered.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.07967" target="_blank" rel="noopener">Intrinsic Neural Fields&lt;/a>&lt;/strong> proposes a new representation for neural fields on manifolds using the spectral properties of the Laplace-Beltrami operator.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2111.15135" target="_blank" rel="noopener">Beyond Periodicity&lt;/a>&lt;/strong> examines the activation functions of NeRFs and tests several novel non-periodic activation functions which enable high-frequency signal encoding.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.14455" target="_blank" rel="noopener">NeDDF&lt;/a>&lt;/strong> proposes a novel 3D representation that reciprocally constrains the distance and density fields. This enables the definition of a distance field for objects with indefinite boundaries (smoke, hairballs, glass, etc.) without losing density information.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2112.05504" target="_blank" rel="noopener">BungeeNeRF&lt;/a>&lt;/strong> enables training NeRFs on large-scale scenes by growing the network during training with a residual block structure. Each block progressively encodes a finer scale.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.06340" target="_blank" rel="noopener">SNeS&lt;/a>&lt;/strong> proposes to leverage symmetry during NeRF training for unseen areas of objects. Geometry and material are often symmetrical, but illumination and shadowing are not symmetric. Therefore, the method introduces a soft symmetry constraint for geometry and materials and includes a global illumination model which can break the symmetry.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2208.06787" target="_blank" rel="noopener">HDR-Plenoxels&lt;/a>&lt;/strong> learns an HDR radiance field with multiple LDR images under different camera settings. The method achieves this by modeling the tone mapping of camera imaging pipelines.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2202.03532" target="_blank" rel="noopener">MINER&lt;/a>&lt;/strong> enables gigapixel image or large-scale point cloud learning with Laplacian pyramids to learn multi-scale signal decompositions. Small MLPs learn disjointed patches in each pyramid scale, and the scales allow the network to grow in capacity during training.&lt;/p>
&lt;h3 id="priors-and-generative">Priors and Generative&lt;/h3>
&lt;p>
&lt;figure id="figure-generalizable-patch-based-neural-renderinghttpsarxivorgabs220710662-uses-transformers-to-generalize-the-3d-reconstruction-to-novel-views-using-transformers-on-epipolar-geometry-patches">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://mohammedsuhail.net/gen_patch_neural_rendering/img/model_animation.gif" alt="Genearlizable patch-based neural rendering with transformers." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://arxiv.org/abs/2207.10662" target="_blank" rel="noopener">Generalizable Patch-Based Neural Rendering&lt;/a> uses transformers to generalize the 3D reconstruction to novel views using transformers on epipolar geometry patches.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Priors can either aid in the reconstruction or can be used in a generative manner. For example, in the reconstruction, priors either increase the quality of neural view synthesis or enable reconstructions from sparse image collections.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.13691" target="_blank" rel="noopener">ShAPO&lt;/a>&lt;/strong> aims to solve the challenge of 3D understanding from a single RGB-D image by posing the problem as a latent space regression of shape, appearance, and pose latent codes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.04992" target="_blank" rel="noopener">KeypointNeRF&lt;/a>&lt;/strong> aims to represent general human avatars with NeRFs by encoding relative spatial 3D information from sparse 3D keypoints. These keypoints are easily applicable with human priors to novel avatars or capture setups.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.11770" target="_blank" rel="noopener">DFRF&lt;/a>&lt;/strong> uses NeRFs to tackle talking head synthesis, which is the task of animating a head from audio. Here, the NeRF is conditioned on 2D appearance and audio features. This also acts as a prior and enables fast adjustments to novel identities.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.05737" target="_blank" rel="noopener">SparseNeuS&lt;/a>&lt;/strong> enables reconstructions from sparse images by learning generalizable priors from image features by introducing geometry encoding volumes for generic surface prediction.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2112.04481" target="_blank" rel="noopener">Scene-DRDF&lt;/a>&lt;/strong> predicts a full 3D scene from a single unseen image. For this, the method is trained on a dataset of realistic non-watertight scans of scenes and predicts the Directed Ray Distance Function (DRDF).&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://arxiv.org/abs/2207.10662" target="_blank" rel="noopener">Generalizable Patch-Based Neural Rendering&lt;/a>&lt;/strong>, the color of a ray in a novel scene is predicted directly from a collection of patches sampled from the scene. The method leverages epipolar geometry to extract patches along the epipolar line, linearly projects them into a 1D feature vector, and transformers process the collection.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.10157" target="_blank" rel="noopener">ViewFormer&lt;/a>&lt;/strong> proposes tackling the novel view synthesis task from 2D images in a single pass network. Here, a two-stage architecture consisting of a codebook and a transformer model is used. The former is used to embed individual images into a smaller latent space, and the transformer solves the view synthesis task in this more compact space.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2208.02801" target="_blank" rel="noopener">Transformers as Meta-Learners for Implicit Neural Representations&lt;/a>&lt;/strong> proposes to leverage transformers as hyper networks for neural fields, which generate the weights of the NeRF. This circumvents the information bottleneck of only conditioning the field based on a single latent vector.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.10821" target="_blank" rel="noopener">Sem2NeRF&lt;/a>&lt;/strong> reconstructs a 3D scene from a single-view semantic mask. This is achieved by encoding the mask into a latent code that controls the scene representation of a pre-trained decoder.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.11467" target="_blank" rel="noopener">CompNVS&lt;/a>&lt;/strong> proposes to perform novel view synthesis from RGB-D images with largely incomplete scene coverage. The method works on a grid-based representation and completes unobserved parts with pre-trained networks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2204.00928" target="_blank" rel="noopener">SiNeRF&lt;/a>&lt;/strong> trains a NeRF from a single view by applying geometric and semantic regularizations. Here, image warping is used to obtain geometry pseudo labels, and adversarial training, as well as a pre-trained ViT, are utilized for semantic pseudo labels&lt;/p>
&lt;p>There exists a large body of GANs which are not 3D aware. &lt;strong>&lt;a href="https://arxiv.org/abs/2207.10642" target="_blank" rel="noopener">Generative Multiplane Images&lt;/a>&lt;/strong> asks how to modify the existing body of work to be 3D aware with as few modifications as possible.&lt;/p>
&lt;h3 id="articulated">Articulated&lt;/h3>
&lt;figure>
&lt;video autoplay loop >
&lt;source src="https://lsongx.github.io/projects/images/pref-overview.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
&lt;p>Motion estimation with &lt;a href="https://arxiv.org/abs/2209.10691" target="_blank" rel="noopener">PREF&lt;/a>. PREF is capable of motion estimation even under topological changes.&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Capturing dynamic objects is a trend that started in previous conference years. However, this year I noticed a trend in adding neural priors.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2209.10691" target="_blank" rel="noopener">PREF&lt;/a>&lt;/strong> learns a neural motion field by introducing regularization on the predictability of motion based on previous frames using a predictor network.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://neuralbodies.github.io/arah" target="_blank" rel="noopener">ARAH&lt;/a>&lt;/strong> enables learning animated clothed human avatars from multi-view videos. These avatars have pose-dependent geometry and appearance and generalize to out-of-distribution poses.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.12575" target="_blank" rel="noopener">NeuMan&lt;/a>&lt;/strong> proposes a framework to reconstruct humans and scenes from a single video with a moving camera. Separate training for the scene and the human is performed, and a warping field to the canonical dynamic human is learned.&lt;/p>
&lt;p>Finding a correspondence between two non-rigidly deforming shapes is a challenging task, which &lt;strong>&lt;a href="https://arxiv.org/abs/2203.07694" target="_blank" rel="noopener">Implicit field supervision for robust non-rigid shape matching&lt;/a>&lt;/strong> proposes to solve with an auto-decoder framework, which learns a continuous shape-wise deformation field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.13817" target="_blank" rel="noopener">AutoAvatar&lt;/a>&lt;/strong> extends recent avatar modeling with autoregressive modeling to express dynamic effects such as soft-tissue deformation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.13807" target="_blank" rel="noopener">Pose-NDF&lt;/a>&lt;/strong> learns a prior of human motion as a field of high-dimensional SO(3)$^K$ pose definition with $K$ quaternions. The resulting manifold can then be easily interpolated and create novel poses.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.01666" target="_blank" rel="noopener">DANBO&lt;/a>&lt;/strong> introduces two inductive biases to enable learning of robust body geometry: Exploiting body part dependencies defined by the skeleton structure using Graph Neural Networks, and each bone predicts a part-specific volume that encodes the local geometry feature. A final aggregation network blends the associated voxel features and creates the neural field.&lt;/p>
&lt;h3 id="editable-and-composable">Editable and Composable&lt;/h3>
&lt;p>
&lt;figure id="figure-object-compositions-with-object-compositional-neural-implicit-surfaceshttpsarxivorgabs220709686">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://wuqianyi.top/objectsdf/img/teaser.gif" alt="Object compositions with Object-Compositional Neural Implicit Surfaces." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Object compositions with &lt;a href="https://arxiv.org/abs/2207.09686" target="_blank" rel="noopener">Object-Compositional Neural Implicit Surfaces&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>This section covers NeRFs that propose composing, controlling, or editing methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.12298" target="_blank" rel="noopener">Deforming Radiance Fields with Cages&lt;/a>&lt;/strong> uses low-poly cages around the object to create a simple deformation target. Deforming the cage is then simple with manual editing. The deformation can warp the previously trained NeRF with a dense warping field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.09686" target="_blank" rel="noopener">Object-Compositional Neural Implicit Surfaces&lt;/a>&lt;/strong> models a scene as a combination of Signed Distance Functions (SDF) of individual objects. For this, the strong association between an object&amp;rsquo;s SDF and semantic label is used, and the semantics are tied to the SDF from each object.&lt;/p>
&lt;h3 id="pose-estimation">Pose Estimation&lt;/h3>
&lt;p>
&lt;figure id="figure-neural-correspondence-field-for-object-pose-estimationhttpsarxivorgabs220800113-find-3d-to-3d-correspondences-in-complex-scenes">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Neural Correspondence Field for Object Pose Estimation." srcset="
/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_2d12e5364440d0e4ce39369d3ac45f54.webp 400w,
/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_339b7686e502915bccfbed0f3b372600.webp 760w,
/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_2d12e5364440d0e4ce39369d3ac45f54.webp"
width="760"
height="324"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://arxiv.org/abs/2208.00113" target="_blank" rel="noopener">Neural Correspondence Field for Object Pose Estimation&lt;/a> find 3D to 3D correspondences in complex scenes.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Estimating the pose of objects or the camera is a fundamental problem in computer vision.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2208.00113" target="_blank" rel="noopener">Neural Correspondence Field for Object Pose Estimation&lt;/a>&lt;/strong> estimates the pose of an object with a known 3D model by constructing neural correspondence fields, which create a 3D mapping between query points and the object space.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.13296" target="_blank" rel="noopener">RayTran&lt;/a>&lt;/strong> estimates the pose of objects and the shape from RGB videos. Here, a global 3D grid of features and an array of view-specific 2D grids is used. A progressive exchange of information is performed with a bidirectional attention mechanism.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2204.05735" target="_blank" rel="noopener">GARF&lt;/a>&lt;/strong> proposes to leverage Gaussian activation functions for pose estimation. Similar to &lt;a href="https://arxiv.org/abs/2104.06405" target="_blank" rel="noopener">BARF&lt;/a>, the method also increases the bandwidth of the Gaussian function over time.&lt;/p>
&lt;h3 id="decomposition">Decomposition&lt;/h3>
&lt;figure>
&lt;video autoplay loop >
&lt;source src="https://akshatdave.github.io/pandora/video/teaser_animation.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
&lt;p>&lt;a href="https://arxiv.org/abs/2203.13458" target="_blank" rel="noopener">PANDORA&lt;/a> enables decompositions into shape, diffuse, and specular using polarization cues.&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>In this section, the radiance of NeRF is split into geometry, BRDF, and illumination. This enables consistent relighting under any illumination.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2112.05140" target="_blank" rel="noopener">NeRF-OSR&lt;/a>&lt;/strong> enables relighting of outdoor scenes by decomposing the radiance to an albedo, Spherical Harmonics (SH) illumination model and explicitly modeling the shadowing from the SH illumination.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.13458" target="_blank" rel="noopener">PANDORA&lt;/a>&lt;/strong> leverages polarization cues to accurately predict shapes and decompositions into diffuse and specular components.&lt;/p>
&lt;p>Relightable dynamic humans are enabled with &lt;strong>&lt;a href="https://arxiv.org/abs/2207.07104" target="_blank" rel="noopener">Relighting4D&lt;/a>&lt;/strong>. Here, the human body is decomposed into surface normals, occlusion, diffuse, and specular with neural fields and rendered with a physically-based renderer.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.11406" target="_blank" rel="noopener">PS-NeRF&lt;/a>&lt;/strong> learns a decomposition from images under sparse views, where each view is illuminated by multiple unknown directional lights. The method can produce detailed surfaces from sparse viewpoints with a shadow-aware renderer and supervision from the multiple illumination images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.07182" target="_blank" rel="noopener">NeILF&lt;/a>&lt;/strong> represents scene lighting as a neural incident light field, which handles occlusions and indirect illumination. With this illumination representation, a decomposition into BRDFs is performed, which a regularized with a Lambertian assumption and bilateral smoothness.&lt;/p>
&lt;h3 id="other">Other&lt;/h3>
&lt;figure>
&lt;video autoplay loop >
&lt;source src="https://www.cs.cornell.edu/projects/arf/videos/Playground_14.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>
&lt;p>&lt;a href="https://arxiv.org/abs/2206.06360" target="_blank" rel="noopener">ARF&lt;/a> enables simple stylization of NeRF reconstructions.&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Several works with excellent results in various fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.03949" target="_blank" rel="noopener">RC-MVSNet&lt;/a>&lt;/strong> introduces neural rendering to Multi-View Stereo (MVS) to reduce ambiguity in correspondences on non-Lambertian surfaces.&lt;/p>
&lt;p>What if we do not reconstruct photorealistic 3D scenes but style them according to paintings? &lt;strong>&lt;a href="https://arxiv.org/abs/2206.06360" target="_blank" rel="noopener">ARF&lt;/a>&lt;/strong> creates these highly stylized novel views from regular radiance fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.01831" target="_blank" rel="noopener">LTEW&lt;/a>&lt;/strong> introduces continuous neural fields to image warping. The method enables learning high-frequency content with a Local Texture Estimator instead of the classical NeRF Fourier embedding.&lt;/p>
&lt;p>Periodic patterns appear in many man-made scenes. NeRF-style methods do not capture these periodic patterns. &lt;strong>&lt;a href="https://arxiv.org/abs/2208.12278" target="_blank" rel="noopener">NPP&lt;/a>&lt;/strong> introduces a periodicity-aware warping module in front of the neural field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2112.04267" target="_blank" rel="noopener">Implicit Neural Representations for Image Compression&lt;/a>&lt;/strong> investigates using neural fields for image compression tasks by introducing quantization, quantization-aware retraining, and entropy coding to neural fields. Meta-learned initializations from MAML also enable shorter training times.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.14067" target="_blank" rel="noopener">Neural Strands&lt;/a>&lt;/strong> focuses on hair representation based on a neural scalp texture that encodes the geometry and appearance of individual strands at each texel location. The rendering is performed based on rasterization of the learned hair strands.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.15065" target="_blank" rel="noopener">DeepShadow&lt;/a>&lt;/strong> learns a 3D reconstruction (depth and normals) based on shadowing. For this, the shadow map generated from a neural field that learns the depth map and a light position is compared to the actual captured one.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.01583" target="_blank" rel="noopener">LaTeRF&lt;/a>&lt;/strong> is a method for extracting an object from a neural field given a natural language description of the object and a set of point-labels of object and non-object points in the input images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2203.15946" target="_blank" rel="noopener">Towards Learning Neural Representations from Shadows&lt;/a>&lt;/strong> estimates the shape from shadows using a neural shadow field. Here, a shadow mapping approach is used to render the shadows, which can be compared to the ground truth shadows.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2207.14782" target="_blank" rel="noopener">Minimal Neural Atlas&lt;/a>&lt;/strong> learns an explicit neural surface representation from a minimal atlas of 3 charts. With a distortion-minimal parameterization for surfaces, arbitrary topology can be represented.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2204.01943" target="_blank" rel="noopener">Unified Implicit Neural Stylization&lt;/a>&lt;/strong> enables stylized neural fields by learning the style of a painting and the content of the scene separately and combining them in an Amalgamation module.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>This is my second time doing a conference roundup, and the number of papers is quite stunning. This time I gathered 50 papers from just a single conference. &lt;a href="https://markboss.me/post/nerf_at_neurips22/">NeurIPS&lt;/a> had 36. If we also take &lt;a href="https://dellaert.github.io/NeRF22/" target="_blank" rel="noopener">CVPR&lt;/a> into account (57 papers), the 3 conferences alone this year produced over 140 NeRF-related papers.&lt;/p>
&lt;p>There are several trends in this year&amp;rsquo;s ECCV. One major one is combining transformers with NeRFs and, in general, adding neural priors. This trend was also visible in my &lt;a href="https://markboss.me/post/nerf_at_neurips22/">NeurIPS roundup&lt;/a>. Another large one is examining NeRF and finding ways to express high-frequency data more easily or capture vast scenes. In my opinion, it is also amazing to see several works which decompose into BRDF, shape, and illumination or to see techniques such as shadow mapping being used in NeRFs.&lt;/p></description></item><item><title>NeRF at NeurIPS 2022</title><link>https://markboss.me/post/nerf_at_neurips22/</link><pubDate>Sun, 18 Sep 2022 22:31:51 +0200</pubDate><guid>https://markboss.me/post/nerf_at_neurips22/</guid><description>&lt;p>Inspired by &lt;a href="https://dellaert.github.io" target="_blank" rel="noopener">Frank Dellaert&lt;/a> and his excellent series on the original &lt;a href="https://dellaert.github.io/NeRF/" target="_blank" rel="noopener">NeRF Explosion&lt;/a> and the following &lt;a href="https://dellaert.github.io/NeRF21/" target="_blank" rel="noopener">ICCV&lt;/a>/&lt;a href="https://dellaert.github.io/NeRF22/" target="_blank" rel="noopener">CVPR&lt;/a> conference gatherings, I decided to look into creating a NeurIPS 22 rundown myself.&lt;/p>
&lt;p>The papers below are all the papers I could gather by browsing through the extensive list of &lt;a href="https://nips.cc/Conferences/2022/Schedule?type=Poster" target="_blank" rel="noopener">accepted NeurIPS papers&lt;/a>. I mainly collected all papers where the titles fit and did a brief scan through the paper or only the abstract if the paper wasn&amp;rsquo;t published at the time of writing. If I have mischaracterized or missed any paper, please send me a DM on Twitter &lt;a href="https://twitter.com/markb_boss" target="_blank" rel="noopener">@markb_boss&lt;/a> or via mail.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: &lt;em>images below belong to the cited papers, and the copyright belongs to the authors or the organization that published these papers. Below I use key figures or videos from some papers under the fair use clause of copyright law.&lt;/em>&lt;/p>
&lt;h2 id="nerf">NeRF&lt;/h2>
&lt;p>Mildenhall &lt;em>et al.&lt;/em> introduced NeRF at ECCV 2020 in the now seminal &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">Neural Radiance Field paper&lt;/a>. As mentioned in the introduction, there has been fantastic progress in research in this field. NeurIPS this year is no exception to this trend.&lt;/p>
&lt;p>What NeRF does, in short, is similar to the task of computed tomography scans (CT). In CT scans, a rotating head measures densities from x-rays. However, knowing where that measurement should be placed in 3D is challenging. NeRF also tries to estimate where a surface and view-dependent radiance is located in space. This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images. With many posed images, photorealistic novel view synthesis is possible.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
Rotate by clicking and dragging, zoom via mouse wheel, and use the buttons (Teal indicates active) to visualize the NeRF process.
&lt;/div>
&lt;/div>
&lt;iframe
src="https://markboss.me/talks/viz/nerf_explainer/"
width="100%"
height="500px"
style="border:none;">
&lt;/iframe>
&lt;h3 id="fundamentals">Fundamentals&lt;/h3>
&lt;p>
&lt;figure id="figure-surface-reconstruction-improvements-with-improved-surface-reconstruction-using-high-frequency-detailshttpsarxivorgabs220607850">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Surface reconstructions improvements with a high-frequency deformation field" srcset="
/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_2d95c11ceca55453108f8ae0f2f49134.webp 400w,
/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_8f44542db3618cb5e9f001558da0d7f1.webp 760w,
/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/HFDeformationNeRF_hu8f8e6698c4ab9f81bdf481f18d2088f0_67111_2d95c11ceca55453108f8ae0f2f49134.webp"
width="760"
height="148"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Surface reconstruction improvements with: &lt;a href="https://arxiv.org/abs/2206.07850" target="_blank" rel="noopener">Improved surface reconstruction using high-frequency details&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>These address more fundamental areas of view-synthesis with NeRF methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55157" target="_blank" rel="noopener">NTRF&lt;/a>&lt;/strong> extends NeRF to improve transmission and reflections, using neural transmittance fields and edge constraints.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55396" target="_blank" rel="noopener">PNF&lt;/a>&lt;/strong> introduces a new class of neural fields using basis-encoded polynomials. These can represent the signal as a composition of manipulable and interpretable components.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=52805" target="_blank" rel="noopener">LoENerf&lt;/a>&lt;/strong> introduces a Levels-of-Experts (LoE) framework to create a novel coordinate-based representation with an MLP. The weights of the MLP are hierarchical, periodic, and position-dependent. Each layer of the MLP has multiple candidate values of the weight matrix, which are individually tiled across the input space.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.07850" target="_blank" rel="noopener">Improved surface reconstruction using high-frequency details&lt;/a>&lt;/strong> can be achieved by splitting the base low-frequency shape into signed distance fields and the high-frequent details in a deformation field on the base shape.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15674" target="_blank" rel="noopener">Generalised Implicit Neural Representations&lt;/a>&lt;/strong> venture far beyond the realm of euclidean coordinate systems and propose to observe the continuous high dimensional signal as a discrete graph and perform a spectral embedding on each node to establish the input for the neural field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15848" target="_blank" rel="noopener">Geo-Neus&lt;/a>&lt;/strong> introduces explicit multi-view geometry constraints to generate geometry consistent surface reconstructions. These losses include ones for signed distance function (SDF) from sparse structure-from-motion (SFM) point clouds and ones for photometric consistency.&lt;/p>
&lt;p>While SDFs are often used to express geometry in NeRFs, they can only represent closed shapes. Using unsigned distance functions (UDF) can express open and watertight surfaces, but they can pose challenges in meshing. &lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55176" target="_blank" rel="noopener">HSDF&lt;/a>&lt;/strong> presents a learnable representation that combines the benefits of each.&lt;/p>
&lt;h3 id="audio">Audio&lt;/h3>
&lt;p>
&lt;figure id="figure-results-from-nafhttpsarxivorgabs220400628-showing-the-sound-propagation-in-the-rooms-shown-above-from-the-emitter-indicated-in-red">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Sound propagation modeling results from NAF" srcset="
/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_66bf9ccee1aa19dbeb9938565b0413c5.webp 400w,
/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_35598e6df4c252308dda01e296ff9fa3.webp 760w,
/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/NAF_hu79f2d4a877c040ad9e80052a9e1d8413_78797_66bf9ccee1aa19dbeb9938565b0413c5.webp"
width="760"
height="214"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Results from &lt;a href="https://arxiv.org/abs/2204.00628" target="_blank" rel="noopener">NAF&lt;/a> showing the sound propagation in the rooms shown above from the emitter, indicated in red.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>As sound propagates similar to rays in a volume, NeRF also found usage in this area.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55190" target="_blank" rel="noopener">INRAS&lt;/a>&lt;/strong> stores high-fidelity time domain impulse responses at arbitrary positions using neural fields. This allows modeling spatial acoustic for a scene efficiently.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2204.00628" target="_blank" rel="noopener">NAF&lt;/a>&lt;/strong> follow a similar approach to INRAS, learning impulse response functions in spatially varying scene.&lt;/p>
&lt;h3 id="priors-and-generative">Priors and Generative&lt;/h3>
&lt;p>
&lt;figure id="figure-overview-of-neuformhttpsarxivorgabs220708890-which-combines-generalizable-priors-with-the-overfitting-of-typical-nerf-model-adaptively">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Visualization of prior-based generalization and NeRF-style overfitting combination in NeuForm." srcset="
/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_19972b94f1141cac9c8daa0b8d07fe46.webp 400w,
/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_b393e8ad9ea1ed746d58736be1c44bc0.webp 760w,
/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/NeuForm_hu3d845c687188c1622a9be0607869eb80_45551_19972b94f1141cac9c8daa0b8d07fe46.webp"
width="681"
height="281"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of &lt;a href="https://arxiv.org/abs/2207.08890" target="_blank" rel="noopener">NeuForm&lt;/a> which combines generalizable priors with the overfitting of typical NeRF model adaptively.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Priors can either aid in the reconstruction or can be used in a generative manner. For example, in the reconstruction, priors either increase the quality of neural view synthesis or enable reconstructions from sparse image collections.&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55117" target="_blank" rel="noopener">CoCo-INR&lt;/a>&lt;/strong>, two attention modules are used. One to extract useful information from the prior codebook and the other to encode these entries into the scene. With this prior, the method is capable of working on sparse images.&lt;/p>
&lt;p>In sparse image collections, several areas are often rarely observed. To circumvent this issue, &lt;strong>&lt;a href="https://arxiv.org/abs/2207.08890" target="_blank" rel="noopener">NeuForm&lt;/a>&lt;/strong> relies on generalizable category-specific representation in less observed areas. In well-observed areas, the method uses the accurate overfitting of NeRF.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.00665" target="_blank" rel="noopener">MonoSDF&lt;/a>&lt;/strong> integrates recent monocular depth and normal prediction networks as priors. Given these additional priors, the authors show performance improvement under either MLP neural fields or voxel-based grid methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.07695" target="_blank" rel="noopener">VoxGRAF&lt;/a>&lt;/strong> combines recent methods for speeding up NeRFs using voxel-based structures and 3D convolutions to generate novel objects in a single forward pass. These scenes can then be rendered from any viewpoint.&lt;/p>
&lt;h3 id="articulated">Articulated&lt;/h3>
&lt;p>
&lt;figure id="figure-overview-of-nemfhttpsarxivorgabs220603287-which-generates-novel-motions-from-a-learned-prior-with-a-continuous-temporal-field">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="NeMF motion prior visualized" srcset="
/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_83625c3cdb2e154623f52edaab166192.webp 400w,
/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_ac37d238defd72e34ef6c07538026258.webp 760w,
/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/NemfMotion_hu2b436a87e280ee61167dd08773aa77e4_57083_83625c3cdb2e154623f52edaab166192.webp"
width="760"
height="223"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of &lt;a href="https://arxiv.org/abs/2206.03287" target="_blank" rel="noopener">NeMF&lt;/a> which generates novel motions from a learned prior with a continuous temporal field.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Capturing dynamic objects is a trend that started in previous conference years. However, this year I noticed a trend in adding priors.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54986" target="_blank" rel="noopener">Neural Shape Deformation Priors&lt;/a>&lt;/strong> uses a transformer trained on large datasets to learn a prior of non-rigid transformations. A source mesh can be morphed with a continuous neural deformation field to a target mesh with a partially described surface deformation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.03287" target="_blank" rel="noopener">NeMF&lt;/a>&lt;/strong> learns a prior of human and quadruped motion and uses it generatively in a neural motion field. The authors show use cases for motion interpolation or in-betweening.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15723" target="_blank" rel="noopener">DeVRF&lt;/a>&lt;/strong> proposes to use 3D spatial voxel grids alongside a 4D deformation field to model dynamic scenes in a highly efficient manner.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54805" target="_blank" rel="noopener">FNeVR&lt;/a>&lt;/strong> enables animating faces with NeRFs by combining 2D motion warping with neural rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.15258" target="_blank" rel="noopener">NDR&lt;/a>&lt;/strong> jointly optimizes the surface and deformation from a dynamic scene using monocular RGB-D cameras. They propose an invertible deformation network to provide cycle consistency between frames. As the topology might change in dynamic scenes, they also employ a strategy to generate topology variants.&lt;/p>
&lt;h3 id="editable-and-composable">Editable and Composable&lt;/h3>
&lt;p>
&lt;figure id="figure-ccnerfhttpsarxivorgabs220514870-enables-compression-and-composition-of--nerfs">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Examples of compression and composition with CCNeRF" srcset="
/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_16364f0c3087528b920dca3fe481a91f.webp 400w,
/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_dc0ab6ad297ad599d0e1beb132739229.webp 760w,
/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/ccnerf_hu00867aa3791de4463d3b32c0dcdfcd52_87982_16364f0c3087528b920dca3fe481a91f.webp"
width="760"
height="258"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://arxiv.org/abs/2205.14870" target="_blank" rel="noopener">CCNeRF&lt;/a> enables compression and composition of NeRFs.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>This section covers NeRFs that provide methods for composing, controlling, or editing.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15838v2" target="_blank" rel="noopener">D&lt;sup>2&lt;/sup>NeRF&lt;/a>&lt;/strong> learns a decoupling of static and dynamic objects from monocular video. Here, two networks are trained separately, which handle the respective areas.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://ylqiao.net/publication/2022nerf/" target="_blank" rel="noopener">NeuPhysics&lt;/a>&lt;/strong> allows editing a dynamic scene by editing physics parameters. This editing is performed on a hexahedra mesh tied to an underlying neural field in a two-way conversion. The physics simulation is done differently on the mesh and can propagate back to the neural field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=54786" target="_blank" rel="noopener">CageNeRF&lt;/a>&lt;/strong> uses low-poly cages around the object to create a simple deformation target. Deforming the cage is then simple in manual editing. The deformation can then be used to warp the previously trained NeRF.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55279" target="_blank" rel="noopener">INSP-Net&lt;/a>&lt;/strong> introduces signal processing into neural fields by introducing a differential operator framework, which can directly work on the fields without discretization. The authors even propose a CovnNet running on the neural representation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.15585" target="_blank" rel="noopener">Decomposing NeRF for Editing via Feature Field Distillation&lt;/a>&lt;/strong> uses existing 2D feature extractors such as CLIP-LSeg to provide additional supervision to detect semantics in the 3D volume. Users can query based on text, image patches, or direct pixel selection to allow semantic editing.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2205.14870" target="_blank" rel="noopener">CCNeRF&lt;/a>&lt;/strong> uses a tensor rank decomposition to express the neural fields in a highly efficient and compressible manner. As the representation is explicit, the learned models are also composable.&lt;/p>
&lt;h3 id="decomposition">Decomposition&lt;/h3>
&lt;p>
&lt;figure id="figure-overview-of-samuraihttpsarxivorgabs220515768-shows-the-applications-of-shape-brdf-and-illumination-decomposition-with-a-simultaneous-pose-recovery">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Shape, BRDF, and illumination decomposition with a simultaneous pose recovery with SAMURAI." srcset="
/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_aa542951c0ad27f1a98034a6abc4c123.webp 400w,
/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_276ba311288569682a06b1e6c576e6ab.webp 760w,
/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/samurai_hueebc0576a6a37e9404d18d9e27e81796_130114_aa542951c0ad27f1a98034a6abc4c123.webp"
width="760"
height="280"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of &lt;a href="https://arxiv.org/abs/2205.15768" target="_blank" rel="noopener">Samurai&lt;/a> shows the applications of shape, BRDF, and illumination decomposition with a simultaneous pose recovery.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>In this section, the radiance of NeRF is split into geometry, BRDF, and illumination. This enables consistent relighting under any illumination.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.03858" target="_blank" rel="noopener">RENI&lt;/a>&lt;/strong> uses &lt;a href="https://www.vincentsitzmann.com/siren/" target="_blank" rel="noopener">SIREN&lt;/a> and vector neurons to learn a neural prior on natural HDR illuminations. They extend the vector neurons to handle rotation equivariance for spherical images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://github.com/neuralps3d/neuralps3d" target="_blank" rel="noopener">Neural Reflectance Field from Shading and Shadow under a Fixed Viewpoint&lt;/a>&lt;/strong> leverages a fixed viewpoint with a moving light source. This way, the geometry, and BRDF have to be recovered based on shading cues. Due to the neural fields, the 3D scene is recovered even from the fixed viewpoint.&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://arxiv.org/abs/2206.03380" target="_blank" rel="noopener">Shape, Light &amp;amp; Material Decomposition from Images using Monte Carlo Rendering and Denoising&lt;/a>&lt;/strong> no previously used low-frequency basis or pre-integration is used to express the environment illumination. Instead, regular Monte Carlo integration is used, which is passed through a denoising network to ease the learning process from noisy gradients.&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://arxiv.org/abs/2205.15768" target="_blank" rel="noopener">SAMURAI&lt;/a>&lt;/strong> the model can decompose coarsely posed image collections into shape, BRDF, and illumination. The cameras are also optimized during training with a camera multiplex. Especially, datasets of challenging scenes where traditional SFM methods fail can be decomposed with this method. &lt;em>For transparency, I&amp;rsquo;m the author of the paper&lt;/em>&lt;/p>
&lt;h3 id="other">Other&lt;/h3>
&lt;p>
&lt;figure id="figure-overview-of-the-perfceptionhttpsarxivorgabs220811537-dataset-with-its-applications">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Overview of the new PeRFception dataset." srcset="
/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_e5c475dca3ff95f87b124468ea564adf.webp 400w,
/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_0a9781da2c6107fcd23e432c3a6608ff.webp 760w,
/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/post/nerf_at_neurips22/perfception_hu4594d4d7a9fcb13cd867c7bbe2f8a082_134477_e5c475dca3ff95f87b124468ea564adf.webp"
width="760"
height="333"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of the &lt;a href="https://arxiv.org/abs/2208.11537" target="_blank" rel="noopener">PeRFception&lt;/a> dataset with its applications.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Several works with excellent results in various fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55004" target="_blank" rel="noopener">NeMF&lt;/a>&lt;/strong> proposes to leverage neural fields for semantic correspondence matching. A coarse cost volume is used to guide a high-precision neural matching field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2206.01724" target="_blank" rel="noopener">SNAKE&lt;/a>&lt;/strong> proposes introducing shape reconstruction with neural fields to aid in 3D keypoint detection for point clouds. No supervision for the detection is required.&lt;/p>
&lt;p>Segmentation and concept learning in NeRFs can benefit from &lt;strong>&lt;a href="https://arxiv.org/abs/2207.06403" target="_blank" rel="noopener">3D Concept Grounding on Neural Fields&lt;/a>&lt;/strong>, where a high-dimensional feature descriptor at each spatial location is taken from language embeddings.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://nips.cc/Conferences/2022/Schedule?showEvent=53336" target="_blank" rel="noopener">ULNef&lt;/a>&lt;/strong> specifically targets virtual try-on settings where multiple garments are layered. These layers are modeled as neural fields, and the collision handling is directly performed on the fields.&lt;/p>
&lt;p>In &lt;strong>&lt;a href="https://arxiv.org/abs/2206.01634" target="_blank" rel="noopener">NeRF-RL&lt;/a>&lt;/strong>, a neural field is used to learn a latent space as the state representation for the reinforcement learning algorithm.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2208.11537" target="_blank" rel="noopener">PeRFception&lt;/a>&lt;/strong> establishes new large-scale datasets for perception-related tasks such as segmentation, classification, etc. They also evaluate a plenoxels variant on these datasets.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>This was quite the amount of work, and I have enormous respect for Frank Dellaert for having it done thrice before. The field of NeRF moves incredibly fast, and even for a single conference, there exists a massive amount of reading material.&lt;/p>
&lt;p>It is also interesting to see NeRF find new applications in other fields such as &lt;a href="#audio">audio&lt;/a> and that sorting papers into a single category becomes challenging. This might be related to fast progress in the field and the high risk of getting scooped.&lt;/p></description></item></channel></rss>