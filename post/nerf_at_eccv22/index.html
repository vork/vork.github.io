<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.4.0 for Hugo"><meta name=google-site-verification content="S4KoopgNiTG4YgetELfTyLoFaDS9bHbC3gJBOdfft3o"><meta name=msvalidate.01 content="2625B23426FAE2D4025878BAF046F9A0"><meta name=author content="Mark Boss"><meta name=description content="I recently went through the the provisional programm of ECCV 2022. After my last post on &ldquo;NeRF at NeurIPS&rdquo; got such great feedback, and I anyway compiled a list of all NeRFy things, I decided to do it all again."><link rel=alternate hreflang=en-us href=https://markboss.me/post/nerf_at_eccv22/><meta name=theme-color content="#009688"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.7c5470f4eb8eccc945078663ffb75c1f.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://markboss.me/post/nerf_at_eccv22/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@markb_boss"><meta property="twitter:creator" content="@markb_boss"><meta property="og:site_name" content="Mark Boss"><meta property="og:url" content="https://markboss.me/post/nerf_at_eccv22/"><meta property="og:title" content="NeRF at ECCV 2022 | Mark Boss"><meta property="og:description" content="I recently went through the the provisional programm of ECCV 2022. After my last post on &ldquo;NeRF at NeurIPS&rdquo; got such great feedback, and I anyway compiled a list of all NeRFy things, I decided to do it all again."><meta property="og:image" content="https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-10-01T18:24:54+02:00"><meta property="article:modified_time" content="2022-10-01T18:24:54+02:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://markboss.me/post/nerf_at_eccv22/"},"headline":"NeRF at ECCV 2022","datePublished":"2022-10-01T18:24:54+02:00","dateModified":"2022-10-01T18:24:54+02:00","author":{"@type":"Person","name":"Mark Boss"},"publisher":{"@type":"Organization","name":"Mark Boss","logo":{"@type":"ImageObject","url":"https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_192x192_fill_lanczos_center_3.png"}},"description":"I recently went through the the provisional programm of ECCV 2022. After my last post on \u0026ldquo;NeRF at NeurIPS\u0026rdquo; got such great feedback, and I anyway compiled a list of all NeRFy things, I decided to do it all again."}</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=all onload='this.media="all"'><title>NeRF at ECCV 2022 | Mark Boss</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=4e3fe78e56deac8b6b3674758189f49b><script src=/js/wowchemy-init.min.9c53337151330e4f750895b53468cbe3.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Mark Boss</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Mark Boss</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/files/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="article-container py-1" style=background:initial><nav class="d-none d-md-flex" aria-label=breadcrumb><ol class=breadcrumb><li class=breadcrumb-item><a href=/>Home</a></li><li class=breadcrumb-item><a href=/post/>Posts</a></li><li class="breadcrumb-item active" aria-current=page>NeRF at ECCV 2022</li></ol></nav></div><article class=article><div class="article-container pt-3"><h1>NeRF at ECCV 2022</h1><div class=article-metadata><span class=article-date>Oct 1, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>11 min read</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/nerf/>NeRF</a>, <a href=/category/literature-review/>Literature Review</a></span></div></div><div class=article-container><div class=article-style><p>I recently went through the <a href=https://eccv2022.ecva.net/program/provisional-program/ target=_blank rel=noopener>the provisional programm</a> of ECCV 2022. After my last <a href=/post/nerf_at_neurips22/>post on &ldquo;NeRF at NeurIPS&rdquo;</a> got such great feedback, and I anyway compiled a list of all NeRFy things, I decided to do it all again.</p><p>I again tried to find all papers by parsing the titles of the provisional program. A brief scan through the paper or abstract was then done to confirm if it is NeRFy and get a rough idea about the paper. If I have mischaracterized or missed any paper, please send me a DM on Twitter <a href=https://twitter.com/markb_boss target=_blank rel=noopener>@markb_boss</a> or via mail.</p><p>Here we go again!</p><p><strong>Note</strong>: <em>images below belong to the cited papers, and the copyright belongs to the authors or the organization that published these papers. Below I use key figures or videos from some papers under the fair use clause of copyright law.</em></p><h2 id=nerf>NeRF</h2><p>Mildenhall <em>et al.</em> introduced NeRF at ECCV 2020 in the now seminal <a href=https://www.matthewtancik.com/nerf target=_blank rel=noopener>Neural Radiance Field paper</a>. As mentioned in the introduction, there has been fantastic progress in research in this field. NeurIPS this year is no exception to this trend.</p><p>What NeRF does, in short, is similar to the task of computed tomography scans (CT). In CT scans, a rotating head measures densities from x-rays. However, knowing where that measurement should be placed in 3D is challenging. NeRF also tries to estimate where a surface and view-dependent radiance is located in space. This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images. With many posed images, photorealistic novel view synthesis is possible.</p><div class="alert alert-note"><div>Rotate by clicking and dragging, zoom via mouse wheel, and use the buttons (Teal indicates active) to visualize the NeRF process.</div></div><iframe src=/talks/viz/nerf_explainer/ width=100% height=500px style=border:none></iframe><h3 id=fundamentals>Fundamentals</h3><figure><video autoplay loop>
<source src=https://snap-research.github.io/R2L/static/videos/DONeRF/forest.mp4 type=video/mp4></video><figcaption><p>Light field distillation with <a href=https://arxiv.org/abs/2203.17261 target=_blank rel=noopener>R2L</a>. Here, on the left, the regular NeRF rendering is compared with the distilled light field. The quality improves with the light field (NeRF PSNR: 28.11 vs. R2L PSNR: 34.18).</p></figcaption></figure><p>These papers address more fundamental problems of view-synthesis with NeRF methods.</p><p>Typically in streaming, the quality of the signal is adjusted depending on the connection quality. NeRFs, on the other hand, encode the entire signal in the weights. <strong><a href=https://arxiv.org/abs/2207.09663 target=_blank rel=noopener>Streamable Neural Fields</a></strong> aims to fix that by encoding everything into smaller sub-networks that can be streamed over time.</p><p>While NeRFs provide photorealistic novel view synthesis results, they do not offer a way to estimate the certainty of their reconstruction. Especially in medical or autonomous driving, this is a highly desirable property. <strong><a href=https://arxiv.org/abs/2203.10192 target=_blank rel=noopener>Conditional-Flow NeRF</a></strong> aims to alleviate this by leveraging a flexible data-driven approach with conditional normalizing flows coupled with latent variable modeling.</p><p><strong><a href=https://arxiv.org/abs/2203.17261 target=_blank rel=noopener>R2L</a></strong> aims to directly learn a surface light field from a pre-trained NeRF. This way, only a single point per ray needs to be evaluated, drastically improving the inference performance.</p><p><strong><a href=https://arxiv.org/abs/2207.10312 target=_blank rel=noopener>AdaNeRF</a></strong> aims to speed up NeRF rendering by learning to reduce the sample count drastically using a split network architecture. One network predicts the sampling, and the other the shading. By enforcing sparsity during training, the number of samples can be lowered.</p><p><strong><a href=https://arxiv.org/abs/2203.07967 target=_blank rel=noopener>Intrinsic Neural Fields</a></strong> proposes a new representation for neural fields on manifolds using the spectral properties of the Laplace-Beltrami operator.</p><p><strong><a href=https://arxiv.org/abs/2111.15135 target=_blank rel=noopener>Beyond Periodicity</a></strong> examines the activation functions of NeRFs and tests several novel non-periodic activation functions which enable high-frequency signal encoding.</p><p><strong><a href=https://arxiv.org/abs/2207.14455 target=_blank rel=noopener>NeDDF</a></strong> proposes a novel 3D representation that reciprocally constrains the distance and density fields. This enables the definition of a distance field for objects with indefinite boundaries (smoke, hairballs, glass, etc.) without losing density information.</p><p><strong><a href=https://arxiv.org/abs/2112.05504 target=_blank rel=noopener>BungeeNeRF</a></strong> enables training NeRFs on large-scale scenes by growing the network during training with a residual block structure. Each block progressively encodes a finer scale.</p><p><strong><a href=https://arxiv.org/abs/2206.06340 target=_blank rel=noopener>SNeS</a></strong> proposes to leverage symmetry during NeRF training for unseen areas of objects. Geometry and material are often symmetrical, but illumination and shadowing are not symmetric. Therefore, the method introduces a soft symmetry constraint for geometry and materials and includes a global illumination model which can break the symmetry.</p><p><strong><a href=https://arxiv.org/abs/2208.06787 target=_blank rel=noopener>HDR-Plenoxels</a></strong> learns an HDR radiance field with multiple LDR images under different camera settings. The method achieves this by modeling the tone mapping of camera imaging pipelines.</p><p><strong><a href=https://arxiv.org/abs/2202.03532 target=_blank rel=noopener>MINER</a></strong> enables gigapixel image or large-scale point cloud learning with Laplacian pyramids to learn multi-scale signal decompositions. Small MLPs learn disjointed patches in each pyramid scale, and the scales allow the network to grow in capacity during training.</p><h3 id=priors-and-generative>Priors and Generative</h3><p><figure id=figure-generalizable-patch-based-neural-renderinghttpsarxivorgabs220710662-uses-transformers-to-generalize-the-3d-reconstruction-to-novel-views-using-transformers-on-epipolar-geometry-patches><div class="d-flex justify-content-center"><div class=w-100><img src=https://mohammedsuhail.net/gen_patch_neural_rendering/img/model_animation.gif alt="Genearlizable patch-based neural rendering with transformers." loading=lazy data-zoomable></div></div><figcaption><a href=https://arxiv.org/abs/2207.10662 target=_blank rel=noopener>Generalizable Patch-Based Neural Rendering</a> uses transformers to generalize the 3D reconstruction to novel views using transformers on epipolar geometry patches.</figcaption></figure></p><p>Priors can either aid in the reconstruction or can be used in a generative manner. For example, in the reconstruction, priors either increase the quality of neural view synthesis or enable reconstructions from sparse image collections.</p><p><strong><a href=https://arxiv.org/abs/2207.13691 target=_blank rel=noopener>ShAPO</a></strong> aims to solve the challenge of 3D understanding from a single RGB-D image by posing the problem as a latent space regression of shape, appearance, and pose latent codes.</p><p><strong><a href=https://arxiv.org/abs/2205.04992 target=_blank rel=noopener>KeypointNeRF</a></strong> aims to represent general human avatars with NeRFs by encoding relative spatial 3D information from sparse 3D keypoints. These keypoints are easily applicable with human priors to novel avatars or capture setups.</p><p><strong><a href=https://arxiv.org/abs/2207.11770 target=_blank rel=noopener>DFRF</a></strong> uses NeRFs to tackle talking head synthesis, which is the task of animating a head from audio. Here, the NeRF is conditioned on 2D appearance and audio features. This also acts as a prior and enables fast adjustments to novel identities.</p><p><strong><a href=https://arxiv.org/abs/2206.05737 target=_blank rel=noopener>SparseNeuS</a></strong> enables reconstructions from sparse images by learning generalizable priors from image features by introducing geometry encoding volumes for generic surface prediction.</p><p>In <strong><a href=https://arxiv.org/abs/2207.10662 target=_blank rel=noopener>Generalizable Patch-Based Neural Rendering</a></strong>, the color of a ray in a novel scene is predicted directly from a collection of patches sampled from the scene. The method leverages epipolar geometry to extract patches along the epipolar line, linearly projects them into a 1D feature vector, and transformers process the collection.</p><p><strong><a href=https://arxiv.org/abs/2203.10157 target=_blank rel=noopener>ViewFormer</a></strong> proposes tackling the novel view synthesis task from 2D images in a single pass network. Here, a two-stage architecture consisting of a codebook and a transformer model is used. The former is used to embed individual images into a smaller latent space, and the transformer solves the view synthesis task in this more compact space.</p><p><strong><a href=https://arxiv.org/abs/2208.02801 target=_blank rel=noopener>Transformers as Meta-Learners for Implicit Neural Representations</a></strong> proposes to leverage transformers as hyper networks for neural fields, which generate the weights of the NeRF. This circumvents the information bottleneck of only conditioning the field based on a single latent vector.</p><p><strong><a href=https://arxiv.org/abs/2203.10821 target=_blank rel=noopener>Sem2NeRF</a></strong> reconstructs a 3D scene from a single-view semantic mask. This is achieved by encoding the mask into a latent code that controls the scene representation of a pre-trained decoder.</p><p><strong><a href=https://arxiv.org/abs/2207.11467 target=_blank rel=noopener>CompNVS</a></strong> proposes to perform novel view synthesis from RGB-D images with largely incomplete scene coverage. The method works on a grid-based representation and completes unobserved parts with pre-trained networks.</p><h3 id=articulated>Articulated</h3><figure><video autoplay loop>
<source src=https://lsongx.github.io/projects/images/pref-overview.mp4 type=video/mp4></video><figcaption><p>Motion estimation with <a href=https://arxiv.org/abs/2209.10691 target=_blank rel=noopener>PREF</a>. PREF is capable of motion estimation even under topological changes.</p></figcaption></figure><p>Capturing dynamic objects is a trend that started in previous conference years. However, this year I noticed a trend in adding neural priors.</p><p><strong><a href=https://arxiv.org/abs/2209.10691 target=_blank rel=noopener>PREF</a></strong> learns a neural motion field by introducing regularization on the predictability of motion based on previous frames using a predictor network.</p><p><strong><a href=https://neuralbodies.github.io/arah target=_blank rel=noopener>ARAH</a></strong> enables learning animated clothed human avatars from multi-view videos. These avatars have pose-dependent geometry and appearance and generalize to out-of-distribution poses.</p><p><strong><a href=https://arxiv.org/abs/2203.12575 target=_blank rel=noopener>NeuMan</a></strong> proposes a framework to reconstruct humans and scenes from a single video with a moving camera. Separate training for the scene and the human is performed, and a warping field to the canonical dynamic human is learned.</p><p>Finding a correspondence between two non-rigidly deforming shapes is a challenging task, which <strong><a href=https://arxiv.org/abs/2203.07694 target=_blank rel=noopener>Implicit field supervision for robust non-rigid shape matching</a></strong> proposes to solve with an auto-decoder framework, which learns a continuous shape-wise deformation field.</p><p><strong><a href=https://arxiv.org/abs/2203.13817 target=_blank rel=noopener>AutoAvatar</a></strong> extends recent avatar modeling with autoregressive modeling to express dynamic effects such as soft-tissue deformation.</p><p><strong><a href=https://arxiv.org/abs/2207.13807 target=_blank rel=noopener>Pose-NDF</a></strong> learns a prior of human motion as a field of high-dimensional SO(3)$^K$ pose definition with $K$ quaternions. The resulting manifold can then be easily interpolated and create novel poses.</p><h3 id=editable-and-composable>Editable and Composable</h3><p><figure id=figure-object-compositions-with-object-compositional-neural-implicit-surfaceshttpsarxivorgabs220709686><div class="d-flex justify-content-center"><div class=w-100><img src=https://wuqianyi.top/objectsdf/img/teaser.gif alt="Object compositions with Object-Compositional Neural Implicit Surfaces." loading=lazy data-zoomable></div></div><figcaption>Object compositions with <a href=https://arxiv.org/abs/2207.09686 target=_blank rel=noopener>Object-Compositional Neural Implicit Surfaces</a>.</figcaption></figure></p><p>This section covers NeRFs that propose composing, controlling, or editing methods.</p><p><strong><a href=https://arxiv.org/abs/2207.12298 target=_blank rel=noopener>Deforming Radiance Fields with Cages</a></strong> uses low-poly cages around the object to create a simple deformation target. Deforming the cage is then simple with manual editing. The deformation can warp the previously trained NeRF with a dense warping field.</p><p><strong><a href=https://arxiv.org/abs/2207.09686 target=_blank rel=noopener>Object-Compositional Neural Implicit Surfaces</a></strong> models a scene as a combination of Signed Distance Functions (SDF) of individual objects. For this, the strong association between an object&rsquo;s SDF and semantic label is used, and the semantics are tied to the SDF from each object.</p><h3 id=pose-estimation>Pose Estimation</h3><p><figure id=figure-neural-correspondence-field-for-object-pose-estimationhttpsarxivorgabs220800113-find-3d-to-3d-correspondences-in-complex-scenes><div class="d-flex justify-content-center"><div class=w-100><img alt="Neural Correspondence Field for Object Pose Estimation." srcset="/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_2d12e5364440d0e4ce39369d3ac45f54.webp 400w,
/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_339b7686e502915bccfbed0f3b372600.webp 760w,
/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/nerf_at_eccv22/neural_correspondencefield_hu91b0b8d75d3457c514c5bc8d87ea3758_79305_2d12e5364440d0e4ce39369d3ac45f54.webp width=760 height=324 loading=lazy data-zoomable></div></div><figcaption><a href=https://arxiv.org/abs/2208.00113 target=_blank rel=noopener>Neural Correspondence Field for Object Pose Estimation</a> find 3D to 3D correspondences in complex scenes.</figcaption></figure></p><p>Estimating the pose of objects or the camera is a fundamental problem in computer vision.</p><p><strong><a href=https://arxiv.org/abs/2208.00113 target=_blank rel=noopener>Neural Correspondence Field for Object Pose Estimation</a></strong> estimates the pose of an object with a known 3D model by constructing neural correspondence fields, which create a 3D mapping between query points and the object space.</p><p><strong><a href=https://arxiv.org/abs/2203.13296 target=_blank rel=noopener>RayTran</a></strong> estimates the pose of objects and the shape from RGB videos. Here, a global 3D grid of features and an array of view-specific 2D grids is used. A progressive exchange of information is performed with a bidirectional attention mechanism.</p><p><strong><a href=https://arxiv.org/abs/2204.05735 target=_blank rel=noopener>GARF</a></strong> proposes to leverage Gaussian activation functions for pose estimation. Similar to <a href=https://arxiv.org/abs/2104.06405 target=_blank rel=noopener>BARF</a>, the method also increases the bandwidth of the Gaussian function over time.</p><h3 id=decomposition>Decomposition</h3><figure><video autoplay loop>
<source src=https://akshatdave.github.io/pandora/video/teaser_animation.mp4 type=video/mp4></video><figcaption><p><a href=https://arxiv.org/abs/2203.13458 target=_blank rel=noopener>PANDORA</a> enables decompositions into shape, diffuse, and specular using polarization cues.</p></figcaption></figure><p>In this section, the radiance of NeRF is split into geometry, BRDF, and illumination. This enables consistent relighting under any illumination.</p><p><strong><a href=https://arxiv.org/abs/2112.05140 target=_blank rel=noopener>NeRF-OSR</a></strong> enables relighting of outdoor scenes by decomposing the radiance to an albedo, Spherical Harmonics (SH) illumination model and explicitly modeling the shadowing from the SH illumination.</p><p><strong><a href=https://arxiv.org/abs/2203.13458 target=_blank rel=noopener>PANDORA</a></strong> leverages polarization cues to accurately predict shapes and decompositions into diffuse and specular components.</p><p>Relightable dynamic humans are enabled with <strong><a href=https://arxiv.org/abs/2207.07104 target=_blank rel=noopener>Relighting4D</a></strong>. Here, the human body is decomposed into surface normals, occlusion, diffuse, and specular with neural fields and rendered with a physically-based renderer.</p><p><strong><a href=https://arxiv.org/abs/2207.11406 target=_blank rel=noopener>PS-NeRF</a></strong> learns a decomposition from images under sparse views, where each view is illuminated by multiple unknown directional lights. The method can produce detailed surfaces from sparse viewpoints with a shadow-aware renderer and supervision from the multiple illumination images.</p><p><strong><a href=https://arxiv.org/abs/2203.07182 target=_blank rel=noopener>NeILF</a></strong> represents scene lighting as a neural incident light field, which handles occlusions and indirect illumination. With this illumination representation, a decomposition into BRDFs is performed, which a regularized with a Lambertian assumption and bilateral smoothness.</p><h3 id=other>Other</h3><figure><video autoplay loop>
<source src=https://www.cs.cornell.edu/projects/arf/videos/Playground_14.mp4 type=video/mp4></video><figcaption><p><a href=https://arxiv.org/abs/2206.06360 target=_blank rel=noopener>ARF</a> enables simple stylization of NeRF reconstructions.</p></figcaption></figure><p>Several works with excellent results in various fields.</p><p><strong><a href=https://arxiv.org/abs/2203.03949 target=_blank rel=noopener>RC-MVSNet</a></strong> introduces neural rendering to Multi-View Stereo (MVS) to reduce ambiguity in correspondences on non-Lambertian surfaces.</p><p>What if we do not reconstruct photorealistic 3D scenes but style them according to paintings? <strong><a href=https://arxiv.org/abs/2206.06360 target=_blank rel=noopener>ARF</a></strong> creates these highly stylized novel views from regular radiance fields.</p><p><strong><a href=https://arxiv.org/abs/2207.01831 target=_blank rel=noopener>LTEW</a></strong> introduces continuous neural fields to image warping. The method enables learning high-frequency content with a Local Texture Estimator instead of the classical NeRF Fourier embedding.</p><p>Periodic patterns appear in many man-made scenes. NeRF-style methods do not capture these periodic patterns. <strong><a href=https://arxiv.org/abs/2208.12278 target=_blank rel=noopener>NPP</a></strong> introduces a periodicity-aware warping module in front of the neural field.</p><p><strong><a href=https://arxiv.org/abs/2112.04267 target=_blank rel=noopener>Implicit Neural Representations for Image Compression</a></strong> investigates using neural fields for image compression tasks by introducing quantization, quantization-aware retraining, and entropy coding to neural fields. Meta-learned initializations from MAML also enable shorter training times.</p><p><strong><a href=https://arxiv.org/abs/2207.14067 target=_blank rel=noopener>Neural Strands</a></strong> focuses on hair representation based on a neural scalp texture that encodes the geometry and appearance of individual strands at each texel location. The rendering is performed based on rasterization of the learned hair strands.</p><p><strong><a href=https://arxiv.org/abs/2203.15065 target=_blank rel=noopener>DeepShadow</a></strong> learns a 3D reconstruction (depth and normals) based on shadowing. For this, the shadow map generated from a neural field that learns the depth map and a light position is compared to the actual captured one.</p><p><strong><a href=https://arxiv.org/abs/2207.01583 target=_blank rel=noopener>LaTeRF</a></strong> is a method for extracting an object from a neural field given a natural language description of the object and a set of point-labels of object and non-object points in the input images.</p><p><strong><a href=https://arxiv.org/abs/2203.15946 target=_blank rel=noopener>Towards Learning Neural Representations from Shadows</a></strong> estimates the shape from shadows using a neural shadow field. Here, a shadow mapping approach is used to render the shadows which can be compared to the ground truth shadows.</p><p><strong><a href=https://arxiv.org/abs/2207.14782 target=_blank rel=noopener>Minimal Neural Atlas</a></strong> learns an explicit neural surface representation from a minimal atlas of 3 charts. With a distortion-minimal parameterization for surfaces, arbitrary topology can be represented.</p><h2 id=conclusion>Conclusion</h2><p>This is my second time doing a conference roundup, and the number of papers is quite stunning. This time I gathered 46 papers from just a single conference. <a href=/post/nerf_at_neurips22/>NeurIPS</a> had 36. If we also take <a href=https://dellaert.github.io/NeRF22/ target=_blank rel=noopener>CVPR</a> into account (57 papers), the 3 conferences alone this year produced nearly 140 NeRF-related papers.</p><p>There are several trends in this year&rsquo;s ECCV. One major one is combining transformers with NeRFs and, in general, adding neural priors. This trend was also visible in my <a href=/post/nerf_at_neurips22/>NeurIPS roundup</a>. Another large one is examining NeRF and finding ways to express high-frequency data more easily or capture vast scenes. In my opinion, it is also amazing to see several works which decompose into BRDF, shape, and illumination or to see techniques such as shadow mapping being used in NeRFs.</p></div><div class="media author-card content-widget-hr"><a href=https://markboss.me/><img class="avatar mr-3 avatar-circle" src=/author/mark-boss/avatar_hu8228efeb083742449721d94a9befbadd_1942861_270x270_fill_q75_lanczos_center.jpg alt="Mark Boss"></a><div class=media-body><h5 class=card-title><a href=https://markboss.me/>Mark Boss</a></h5><h6 class=card-subtitle>Research Scientist</h6><p class=card-text>I&rsquo;m a researcher at Unity Technologies with research interests in the intersection of machine learning and computer graphics.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:hello@markboss.me><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/markb_boss target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=y23cQ6wAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/vork target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/markbboss target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/nerf_at_neurips22/>NeRF at NeurIPS 2022</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/privacy/>Privacy Policy</a>
&#183;
<a href=/terms/>Legal details</a></p><p class="powered-by copyright-license-text">© 2022 Mark Boss. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26cbef546776b4d6032d1ea3dafadab.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/elm.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/kotlin.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/rust.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.fe369d7ba3ba979cfdb3e0f7e5240185.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.b0d291ed6d27eacec233e6cf5204f99a.js type=module></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script></body></html>