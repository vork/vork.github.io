<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.6"><link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.4228702cea630023ae1883491e9ef88b.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=google-site-verification content="S4KoopgNiTG4YgetELfTyLoFaDS9bHbC3gJBOdfft3o"><meta name=msvalidate.01 content="2625B23426FAE2D4025878BAF046F9A0"><meta name=author content="Mark Boss"><meta name=description content="Personal Research Website of Mark Boss"><link rel=alternate hreflang=en-us href=https://markboss.me/><link rel=canonical href=https://markboss.me/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hube1743d0a4940c76c300ec8349475861_6659_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#009688"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@markb_boss"><meta property="twitter:creator" content="@markb_boss"><meta property="twitter:image" content="https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png"><meta property="og:type" content="profile"><meta property="og:site_name" content="Mark Boss"><meta property="og:url" content="https://markboss.me/"><meta property="og:title" content="Mark Boss"><meta property="og:description" content="Personal Research Website of Mark Boss"><meta property="og:image" content="https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-01-08T18:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://markboss.me/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://markboss.me/"}</script><link rel=alternate href=/index.xml type=application/rss+xml title="Mark Boss"><title>Mark Boss</title>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css crossorigin=anonymous media=print onload='this.media="all"' integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw=="></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.e1d05065ea0d70e40dcd8bf319bec2e9.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Mark Boss</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Mark Boss</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts data-target=#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#talks data-target=#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experience data-target=#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/files/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/author/mark-boss/avatar_hu8228efeb083742449721d94a9befbadd_1942861_270x270_fill_q75_lanczos_center.jpg alt="Mark Boss"><div class=portrait-title><h2>Mark Boss</h2><h3>Research Scientist</h3><h3><a href=https://stability.ai/ target=_blank rel=noopener><span>Stability AI</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=mailto:hello@markboss.me aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://twitter.com/markb_boss target=_blank rel=noopener aria-label=twitter><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?user=y23cQ6wAAAAJ&amp;hl=en" target=_blank rel=noopener aria-label=graduation-cap><i class="fas fa-graduation-cap big-icon"></i></a></li><li><a href=https://github.com/vork target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=https://www.linkedin.com/in/markbboss target=_blank rel=noopener aria-label=linkedin><i class="fab fa-linkedin big-icon"></i></a></li><li><a href=https://unsplash.com/@vork target=_blank rel=noopener aria-label=unsplash><i class="fab fa-unsplash big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Biography</h1><div class=article-style><p>Mark Boss is a researcher at Stability AI. He worked at Unity Technologies before and completed his PhD at the University of Tübingen in the computer graphics group of Prof. Hendrik Lensch. His research interests lie at the intersection of machine learning and computer graphics, focusing mainly on inferring physical properties (shape, material, illumination) from images.</p><p>If you are interested in a research collaboration, please drop me a <a href=mailto:hello@markboss.me> 
<i class="fas fa-envelope pr-1 fa-fw"></i> mail</a> with your CV.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests mb-0"><li>Machine Learning</li><li>Computer Graphics</li><li>Inverse Rendering</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>PhD in Computer Science, 2023</p><p class=institution>University of Tübingen</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>MSc in Computer Science, 2018</p><p class=institution>University of Tübingen</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BSc in Computer Science, 2016</p><p class=institution>Osnabrück University of Applied Sciences</p></div></li></ul></div></div></div></div></div></section><section id=news class="home-section wg-github-vork-news"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>News</h1></div><div class="col-12 col-lg-8"><div class="view-list view-list-item"><span>SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images</span><div class=article-metadata><span>08 Jan 2025 - <strong><a href=https://spar3d.github.io/assets/paper.pdf target=_blank rel=noopener>ArXiv</a></strong> submission and <strong><a href=https://spar3d.github.io/ target=_blank rel=noopener>Project page</a></strong> online. It also became a full product at <strong><a href=https://stability.ai/news/stable-point-aware-3d target=_blank rel=noopener>Stability AI</a></strong>.</span></div></div><div class="view-list view-list-item"><span>SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement</span><div class=article-metadata><span>01 Aug 2024 - <strong><a href=https://arxiv.org/abs/2408.00653 target=_blank rel=noopener>ArXiv</a></strong> submission and <strong><a href=https://stable-fast-3d.github.io target=_blank rel=noopener>Project page</a></strong> online. It also became a full product at <strong><a href=https://stability.ai/news/introducing-stable-fast-3d target=_blank rel=noopener>Stability AI</a></strong>.</span></div></div><div class="view-list view-list-item"><span>Two Papers Accepted at ECCV 2024</span><div class=article-metadata><span>01 Jul 2024 - <strong><a href=publication/2024-pbr-collab-control/>Collaborative Control for Geometry-Conditioned PBR Image Generation</a></strong> and <strong><a href=publication/2024-sv3d/>SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion</a></strong> got accepted for ECCV 2024.</span></div></div><div class="view-list view-list-item"><span>SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion</span><div class=article-metadata><span>15 Mar 2024 - <strong><a href=http://arxiv.org/abs/2403.12008 target=_blank rel=noopener>ArXiv</a></strong> submission and <strong><a href=https://sv3d.github.io target=_blank rel=noopener>Project page</a></strong> online</span></div></div><div class="view-list view-list-item"><span>Collaborative Control for Geometry-Conditioned PBR Image Generation</span><div class=article-metadata><span>09 Feb 2024 - <strong><a href=https://arxiv.org/abs/2402.05919 target=_blank rel=noopener>ArXiv</a></strong> submission and <strong><a href=https://unity-research.github.io/holo-gen/ target=_blank rel=noopener>Project page</a></strong> online</span></div></div><div class=see-all><a href=/news/>See all
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=posts class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent Posts</h1></div><div class="col-12 col-lg-8"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/post/nerf_at_cvpr23/>NeRF at CVPR 2023</a></div><a href=/post/nerf_at_cvpr23/ class=summary-link><div class=article-style>It is now my third time writing a summary of NeRFy things at a conference. This time it is the big one: CVPR. The list of accepted papers is massive again, with 2359 papers.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/frank-dellaert/>Frank Dellaert</a></span></div><span class=article-date>May 1, 2023
</span><span class=middot-divider></span>
<span class=article-reading-time>49 min read
</span><span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/nerf/>NeRF</a>, <a href=/category/literature-review/>Literature Review</a></span></div></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/post/nerf_at_eccv22/>NeRF at ECCV 2022</a></div><a href=/post/nerf_at_eccv22/ class=summary-link><div class=article-style>I recently went through the the provisional programm of ECCV 2022. After my last post on &ldquo;NeRF at NeurIPS&rdquo; got such great feedback, and I anyway compiled a list of all NeRFy things, I decided to do it all again.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Oct 1, 2022
</span><span class=middot-divider></span>
<span class=article-reading-time>12 min read
</span><span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/nerf/>NeRF</a>, <a href=/category/literature-review/>Literature Review</a></span></div></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/post/nerf_at_neurips22/>NeRF at NeurIPS 2022</a></div><a href=/post/nerf_at_neurips22/ class=summary-link><div class=article-style>Inspired by Frank Dellaert and his excellent series on the original NeRF Explosion and the following ICCV/CVPR conference gatherings, I decided to look into creating a NeurIPS 22 rundown myself.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Last updated on
Sep 30, 2022
</span><span class=middot-divider></span>
<span class=article-reading-time>8 min read
</span><span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/nerf/>NeRF</a>, <a href=/category/literature-review/>Literature Review</a></span></div></div></div><div class=ml-3></div></div></div></div></div></section><section id=talks class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent & Upcoming Talks</h1></div><div class="col-12 col-lg-8"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/inverse-rendering-for-games/>Inverse Rendering for Games</a></div><a href=/talk/inverse-rendering-for-games/ class=summary-link><div class=article-style>Asset production in the game industry is time-consuming, and since &ldquo;The Vanishing of Ethan Carter&rdquo; photogrammetry has …</div></a><div class="stream-meta article-metadata"><div><span>Nov 28, 2023 4:40 PM &mdash; 5:15 PM
</span><span class=middot-divider></span>
<span>Hochschule der Medien, Stuttgart</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/talks/games_day_23 target=_blank rel=noopener>Slides
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://events.mi.hdm-stuttgart.de/2023-11-28-games-day/Inverse%20Rendering%20for%20Games target=_blank rel=noopener>Video</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/neural-reflectance-decomposition/>Neural Reflectance Decomposition</a></div><a href=/talk/neural-reflectance-decomposition/ class=summary-link><div class=article-style>In this talk, I will present our recent work on decomposing an object into its shape, reflectance, and illumination. This highly …</div></a><div class="stream-meta article-metadata"><div><span>Jul 26, 2022 10:00 AM &mdash; 11:00 AM
</span><span class=middot-divider></span>
<span>Adobe Research</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/talks/adobe_talk target=_blank rel=noopener>Slides</a></div></div><div class=ml-3></div></div></div></div></div></section><section id=publications class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Publications</h1></div><div class="col-12 col-lg-8"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2025-spar3d/>SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images</a></div><a href=/publication/2025-spar3d/ class=summary-link><div class=article-style>We study the problem of single-image 3D object reconstruction. Recent works have diverged into two directions: regression-based …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/zixuan-huang/>Zixuan Huang</a></span>, <span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/aaryaman-vasishta/>Aaryaman Vasishta</a></span>, <span><a href=/author/james-m.-rehg/>James M. Rehg</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2025-spar3d/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://spar3d.github.io/assets/paper.pdf target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://spar3d.github.io/ target=_blank rel=noopener><i class="fas fa-globe mr-1"></i>Website</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=mlO3Nc3Nsng" target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Stability-AI/stable-point-aware-3d target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://huggingface.co/stabilityai/stable-point-aware-3d target=_blank rel=noopener><i class="fas fa-database mr-1"></i>Model</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://huggingface.co/spaces/stabilityai/stable-point-aware-3d target=_blank rel=noopener><i class="fas fa-laptop mr-1"></i>Demo</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2024-sf3d/>SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement</a></div><a href=/publication/2024-sf3d/ class=summary-link><div class=article-style>We present SF3D, a novel method for rapid and high-quality textured mesh reconstruction from a single image. Utilizing the Large …</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/zixuan-huang/>Zixuan Huang</a></span>, <span><a href=/author/aaryaman-vasishta/>Aaryaman Vasishta</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2024-sf3d/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2408.00653 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://stable-fast-3d.github.io target=_blank rel=noopener><i class="fas fa-globe mr-1"></i>Website</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/uT96UCBSBko target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Stability-AI/stable-fast-3d target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://huggingface.co/stabilityai/stable-fast-3d target=_blank rel=noopener><i class="fas fa-database mr-1"></i>Model</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://huggingface.co/spaces/stabilityai/stable-fast-3d target=_blank rel=noopener><i class="fas fa-laptop mr-1"></i>Demo</a></div></div><div class=ml-3><a href=/publication/2024-sf3d/><img src=/publication/2024-sf3d/featured_hu30910665284931ace4f57faa1e01d828_290816_a17770a0bfe57b462cc2cbd8632d0d45.webp height=84 width=150 alt="SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2024-sv3d/>SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion</a></div><a href=/publication/2024-sv3d/ class=summary-link><div class=article-style>We present Stable Video 3D (SV3D) - a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/vikram-voleti/>Vikram Voleti</a></span>, <span><a href=/author/chun-han-yao/>Chun-Han Yao</a></span>, <span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/adam-letts/>Adam Letts</a></span>, <span><a href=/author/david-pankratz/>David Pankratz</a></span>, <span><a href=/author/dmitrii-tochilkin/>Dmitrii Tochilkin</a></span>, <span><a href=/author/christian-laforte/>Christian Laforte</a></span>, <span><a href=/author/robin-rombach/>Robin Rombach</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2024-sv3d/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2403.12008 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://sv3d.github.io/ target=_blank rel=noopener><i class="fas fa-globe mr-1"></i>Website</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/Zqw4-1LcfWg target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a></div></div><div class=ml-3><a href=/publication/2024-sv3d/><img src=/publication/2024-sv3d/featured_hu9e461223b7ede27ad582e3af118936ad_541487_8b218467354db93580e5a361c4b08337.webp height=46 width=150 alt="SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2024-pbr-collab-control/>Collaborative Control for Geometry-Conditioned PBR Image Generation</a></div><a href=/publication/2024-pbr-collab-control/ class=summary-link><div class=article-style>Current 3D content generation builds on generative models that output RGB images. Modern graphics pipelines, however, require …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/shimon-vainer/>Shimon Vainer</a></span>, <span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/mathias-parger/>Mathias Parger</a></span>, <span><a href=/author/konstantin-kutsy/>Konstantin Kutsy</a></span>, <span><a href=/author/dante-de-nigris/>Dante De Nigris</a></span>, <span><a href=/author/ciara-rowles/>Ciara Rowles</a></span>, <span><a href=/author/nicolas-perony/>Nicolas Perony</a></span>, <span><a href=/author/simon-donne/>Simon Donné</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2024-pbr-collab-control/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2402.05919 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://unity-research.github.io/holo-gen/ target=_blank rel=noopener><i class="fas fa-globe mr-1"></i>Website</a></div></div><div class=ml-3><a href=/publication/2024-pbr-collab-control/><img src=/publication/2024-pbr-collab-control/featured_hua2d832bd04dce210763d3ddbd4ba53f7_342061_66e216412fde3f5a663b16399703a7a8.webp height=75 width=150 alt="Collaborative Control for Geometry-Conditioned PBR Image Generation" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2024-shinobi/>SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild</a></div><a href=/publication/2024-shinobi/ class=summary-link><div class=article-style>We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/andreas-engelhardt/>Andreas Engelhardt</a></span>, <span><a href=/author/amit-raj/>Amit Raj</a></span>, <span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/yunzhi-zhang/>Yunzhi Zhang</a></span>, <span><a href=/author/abhishek-kar/>Abhishek Kar</a></span>, <span><a href=/author/yuanzhen-li/>Yuanzhen Li</a></span>, <span><a href=/author/deqing-sun/>Deqing Sun</a></span>, <span><a href=/author/ricardo-martin-brualla/>Ricardo Martin Brualla</a></span>, <span><a href=/author/jonathan-t.-barron/>Jonathan T. Barron</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2024-shinobi/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2401.10171 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=iFENQ6AcYd8" target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://shinobi.aengelhardt.com/ target=_blank rel=noopener><i class="fas fa-globe mr-1"></i>Website</a></div></div><div class=ml-3><a href=/publication/2024-shinobi/><img src=/publication/2024-shinobi/featured_hu8f4ab94836ed4264f7845ce06607b728_162833_619d2f4321dce6d0b87f825faac4cc1f.webp height=52 width=150 alt="SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/phdthesis/>Neural Reflectance Decomposition</a></div><a href=/publication/phdthesis/ class=summary-link><div class=article-style>Creating relightable objects from images or collections is a fundamental challenge in computer vision and graphics. This problem is …</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/phdthesis/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=/talks/phd_defense/ target=_blank rel=noopener>Slides
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://dx.doi.org/10.15496/publikation-79535 target=_blank rel=noopener><i class="fas fa-pdf mr-1"></i>Publication</a></div></div><div class=ml-3><a href=/publication/phdthesis/><img src=/publication/phdthesis/featured_hu24f1a4c7f360f896f1c8f317261b8f5f_71752_c73aff9b98f3c33565ed946842fb7575.webp height=123 width=150 alt="Neural Reflectance Decomposition" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2022-samurai/>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</a></div><a href=/publication/2022-samurai/ class=summary-link><div class=article-style>Inverse rendering of an object under entirely unknown capture conditions is a fundamental challenge in computer vision and graphics. …</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/andreas-engelhardt/>Andreas Engelhardt</a></span>, <span><a href=/author/abhishek-kar/>Abhishek Kar</a></span>, <span><a href=/author/yuanzhen-li/>Yuanzhen Li</a></span>, <span><a href=/author/deqing-sun/>Deqing Sun</a></span>, <span><a href=/author/jonathan-t.-barron/>Jonathan T. Barron</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2022-samurai/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=/talks/samurai_neurips/ target=_blank rel=noopener>Slides
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2205.15768 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/LlYuGDjXp-8 target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/google/samurai target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.dropbox.com/sh/x3u2szvaqjtaykl/AACCZn05NciMa5bHhn60p9vja?dl=0" target=_blank rel=noopener><i class="fas fa-database mr-1"></i>Scenes</a></div></div><div class=ml-3><a href=/publication/2022-samurai/><img src=/publication/2022-samurai/featured_hueebc0576a6a37e9404d18d9e27e81796_130114_4a8b21b7b1eb6145c6b3ac6f41c47476.webp height=55 width=150 alt="SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2022-tlcyzer/>An open-source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening</a></div><a href=/publication/2022-tlcyzer/ class=summary-link><div class=article-style>Substandard and falsified medicines present a serious threat to public health. Simple, low-cost screening tools are important in the …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/cathrin-hauk/>Cathrin Hauk</a></span>, <span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/julia-gabel/>Julia Gabel</a></span>, <span><a href=/author/simon-schafermann/>Simon Schäfermann</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span>, <span><a href=/author/lutz-heide/>Lutz Heide</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2022-tlcyzer/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=/project/tlcyzer/>Project
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1038/s41598-022-17527-y target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/TLCyzer/tlcyzer target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://tlcyzer.github.io target=_blank rel=noopener><i class="fas fa-globe mr-1"></i>Website</a></div></div><div class=ml-3><a href=/publication/2022-tlcyzer/><img src=/publication/2022-tlcyzer/featured_hu3ded59b12af0e577d0f6c9ba658bb523_562259_378ef30f17ca6b5e47e7e7513ebfc489.webp height=128 width=150 alt="An open-source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2021-neural-pil/>Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</a></div><a href=/publication/2021-neural-pil/ class=summary-link><div class=article-style>Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural …</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span>, <span><a href=/author/raphael-braun/>Raphael Braun</a></span>, <span><a href=/author/ce-liu/>Ce Liu</a></span>, <span><a href=/author/jonathan-t.-barron/>Jonathan T. Barron</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2021-neural-pil/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.14373 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/p5cKaNwVp4M target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/cgtuebingen/Neural-PIL target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a></div></div><div class=ml-3><a href=/publication/2021-neural-pil/><img src=/publication/2021-neural-pil/featured_hu557982f4a73d07f363dd1cf0452a897a_50708_f1821e1f7d6cc23918cc45308d6376e7.webp height=40 width=150 alt="Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2021-nerd/>NeRD: Neural Reflectance Decomposition from Image Collections</a></div><a href=/publication/2021-nerd/ class=summary-link><div class=article-style>Decomposing a scene into its shape, reflectance, and illumination is a challenging but important problem in computer vision and …</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/raphael-braun/>Raphael Braun</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span>, <span><a href=/author/jonathan-t.-barron/>Jonathan T. Barron</a></span>, <span><a href=/author/ce-liu/>Ce Liu</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2021-nerd/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2012.03918 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/JL-qMTXw9VU target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition#datasets target=_blank rel=noopener><i class="fas fa-database mr-1"></i>Dataset</a></div></div><div class=ml-3><a href=/publication/2021-nerd/><img src=/publication/2021-nerd/featured_hu9ce3dee256724e8b0641511e91e45d8f_249226_f4e749fb1c920e262a027e81e580234b.webp height=50 width=150 alt="NeRD: Neural Reflectance Decomposition from Image Collections" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/cvpr20-two-shot-brdf/>Two-shot Spatially-varying BRDF and Shape Estimation</a></div><a href=/publication/cvpr20-two-shot-brdf/ class=summary-link><div class=article-style>Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in …</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/varun-jampani/>Varun Jampani</a></span>, <span><a href=/author/kihwan-kim/>Kihwan Kim</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span>, <span><a href=/author/jan-kautz/>Jan Kautz</a></span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/cvpr20-two-shot-brdf/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=/files/cvpr20-two-shot-brdf.pdf><i class="fas fa-file-pdf mr-1"></i>Paper + Supplementary</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2004.00403 target=_blank rel=noopener><i class="fas fa-file-pdf mr-1"></i>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/CyC6PutoJO8 target=_blank rel=noopener><i class="fab fa-youtube mr-1"></i>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/NVlabs/two-shot-brdf-shape target=_blank rel=noopener><i class="fas fa-code mr-1"></i>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/14mou3Va65deimPYE5GtFdK8OS3I0BSzq/view?usp=sharing" target=_blank rel=noopener><i class="fas fa-database mr-1"></i>Dataset</a></div></div><div class=ml-3><a href=/publication/cvpr20-two-shot-brdf/><img src=/publication/cvpr20-two-shot-brdf/featured_hu30910665284931ace4f57faa1e01d828_139209_3b520930509e00a09be3273f9acb7b31.webp height=65 width=150 alt="Two-shot Spatially-varying BRDF and Shape Estimation" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/>Single Image BRDF Parameter Estimation with a Conditional Adversarial Network</a></div><a href=/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/ class=summary-link><div class=article-style>Creating plausible surfaces is an essential component in achieving a high degree of realism in rendering. To relieve artists, who …</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1910.05148 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/><img src=/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/featured_hu30910665284931ace4f57faa1e01d828_105779_8b6c4eb257ff41379a53062474493fe6.webp height=84 width=150 alt="Single Image BRDF Parameter Estimation with a Conditional Adversarial Network" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/deep-dual-loss-brdf-parameter-estimation/>Deep Dual Loss BRDF Parameter Estimation</a></div><a href=/publication/deep-dual-loss-brdf-parameter-estimation/ class=summary-link><div class=article-style>Surface parameter estimation is an essential field in computer games and movies. An exact representation of a real-world surface allows …</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted><a href=/author/mark-boss/>Mark Boss</a></span>, <span><a href=/author/fabian-groh/>Fabian Groh</a></span>, <span><a href=/author/sebastian-herholz/>Sebastian Herholz</a></span>, <span><a href=/author/hendrik-p.-a.-lensch/>Hendrik P. A. Lensch</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/files/deep_dual_loss_brdf_egsr_mam_2018.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/deep-dual-loss-brdf-parameter-estimation/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.2312/mam.20181199 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/deep-dual-loss-brdf-parameter-estimation/><img src=/publication/deep-dual-loss-brdf-parameter-estimation/featured_hu30910665284931ace4f57faa1e01d828_98512_d024b268d3cc764f797023b67dc74990.webp height=73 width=150 alt="Deep Dual Loss BRDF Parameter Estimation" loading=lazy></a></div></div></div></div></div></section><section id=projects class="home-section wg-portfolio"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Projects</h1></div><div class="col-12 col-lg-8"><div class="row js-layout-row"><div class="col-12 isotope-item js-id-nerf js-id-literature-review"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://nerfherder.xyz target=_blank rel=noopener>Nerfherder.xyz</a></div><a href=https://nerfherder.xyz target=_blank rel=noopener class=summary-link><div class=article-style>The amount of NeRF paper each day can become overwhelming. After several <a href=/category/literature-review/>summaries</a>, I&rsquo;ve created a small tool that can create blog posts automatically but also can create daily slide decks from arXiv. The tool runs entirely independently and categorizes cs.CV, writes a summary, extracts a teaser image, and builds a slide deck.</div></a><div class="stream-meta article-metadata"></div></div><div class=ml-3><a href=https://nerfherder.xyz target=_blank rel=noopener><img src=/project/nerfherder/featured_hue3e5300f6cc0eac7fa761afdfc30a4dc_579102_5790d42e66c49619c607d74ac86c1424.webp height=74 width=150 alt=Nerfherder.xyz loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-android js-id-rust js-id-kotlin js-id-open-source"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://tlcyzer.github.io target=_blank rel=noopener>TLCyzer</a></div><a href=https://tlcyzer.github.io target=_blank rel=noopener class=summary-link><div class=article-style>TLCyzer is a free and open source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening, described and validated in a scientific study.</div></a><div class="stream-meta article-metadata"></div></div><div class=ml-3><a href=https://tlcyzer.github.io target=_blank rel=noopener><img src=/project/tlcyzer/featured_hu3ded59b12af0e577d0f6c9ba658bb523_562259_378ef30f17ca6b5e47e7e7513ebfc489.webp height=128 width=150 alt=TLCyzer loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-python js-id-matplotlib js-id-open-source"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/vork/farrowandball target=_blank rel=noopener>farrow-and-ball</a></div><a href=https://github.com/vork/farrowandball target=_blank rel=noopener class=summary-link><div class=article-style>A set of British paint manufacturer Farrow & Ball inspired color maps for matplotlib. The maps are sorted in categories for different plotting needs (Divergent, Spectral, Base Colors, Misc).</div></a><div class="stream-meta article-metadata"></div></div><div class=ml-3><a href=https://github.com/vork/farrowandball target=_blank rel=noopener><img src=/project/farrow-and-ball/featured_hu52f801fec1c38d0c2216ad5c035fdf92_3125_d53ae3890126512c8a65bc04133e45ac.webp height=119 width=150 alt=farrow-and-ball loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-teaching"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/project/web_brdf_viz/>Online BRDF Visualization</a></div><a href=/project/web_brdf_viz/ class=summary-link><div class=article-style>The bidirectional reflectance distribution function (BRDF) is a function that defines how incoming light is reflected outward. This web-based visualizer uses the Cook-Torrance model and shows how the material looks on a sphere, and simultaneously displays its evaluated BRDF. Especially the different reflective behavior of specular and diffuse materials are easily visible.</div></a><div class="stream-meta article-metadata"></div></div><div class=ml-3><a href=/project/web_brdf_viz/><img src=/project/web_brdf_viz/featured_hu42b9ad4760d45bdb9802a75dede8cd48_93350_6b4559534e2bc784740e9e62bc89dc2e.webp height=114 width=150 alt="Online BRDF Visualization" loading=lazy></a></div></div></div><div class="col-12 isotope-item js-id-teaching"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://infomark.org/ target=_blank rel=noopener>Infomark</a></div><a href=https://infomark.org/ target=_blank rel=noopener class=summary-link><div class=article-style>A free, scalable, modern and open source solution for programming lectures supporting auto-testing/grading of programming assignments scaling to thousands of students and several courses.</div></a><div class="stream-meta article-metadata"></div></div><div class=ml-3><a href=https://infomark.org/ target=_blank rel=noopener><img src=/project/infomark/featured_hu7511ac86c67a21fc108a57c999b43d2d_28368_9c871e6ccc81130dd4abc23d62308ee9.webp height=122 width=150 alt=Infomark loading=lazy></a></div></div></div></div></div></div></div></section><section id=experience class="home-section wg-experience"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Experience</h1></div><div class="col-12 col-lg-8"><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border exp-fill">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://stability.ai/ target=_blank rel=noopener><img src=/media/icons/brands/StabilityAI.svg width=56 height=56 alt="Stability AI" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Scientist</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://stability.ai/ target=_blank rel=noopener>Stability AI</a></div><div class="text-muted exp-meta">Jan 2024 –
Present
<span class=middot-divider></span>
<span>Germany</span></div></div></div><div class=card-text>Research on object acquisition, usage of deep priors to reduce ambiguities, and generative AI.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://unity.com target=_blank rel=noopener><img src=/media/icons/brands/UnityTechnologies.svg width=56 height=56 alt="Unity Technologies" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Senior Research Scientist</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://unity.com target=_blank rel=noopener>Unity Technologies</a></div><div class="text-muted exp-meta">Sep 2022 –
Jan 2024
<span class=middot-divider></span>
<span>Germany</span></div></div></div><div class=card-text>Research on object acquisition and generative AI.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://uni-tuebingen.de/en/ target=_blank rel=noopener><img src=/media/icons/brands/UniTuebingen.svg width=56 height=56 alt="University of Tübingen" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Ph.D. Student</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://uni-tuebingen.de/en/ target=_blank rel=noopener>University of Tübingen</a></div><div class="text-muted exp-meta">Jun 2018 –
Jun 2022
<span class=middot-divider></span>
<span>Tübingen, Germany</span></div></div></div><div class=card-text>Research on deep learning based material acquisition.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://research.google target=_blank rel=noopener><img src=/media/icons/brands/Google.svg width=56 height=56 alt=Google loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Student Researcher</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://research.google target=_blank rel=noopener>Google</a></div><div class="text-muted exp-meta">Jun 2021 –
Apr 2022
<span class=middot-divider></span>
<span>Germany (Remote)</span></div></div></div><div class=card-text>Research on novel techniques for material, geometry and illumination disentanglement.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=http://www.nvidia.com target=_blank rel=noopener><img src=/media/icons/brands/Nvidia.svg width=56 height=56 alt=NVIDIA loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=http://www.nvidia.com target=_blank rel=noopener>NVIDIA</a></div><div class="text-muted exp-meta">Apr 2019 –
Jul 2019
<span class=middot-divider></span>
<span>Westford, MA</span></div></div></div><div class=card-text>Research on casual shape and material acquisition, which resulted in the publication: <a href=publication/cvpr20-two-shot-brdf/>&ldquo;Two-shot Spatially-varying BRDF and Shape Estimation&rdquo;</a></div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://zahlz.com target=_blank rel=noopener><img src=/media/icons/brands/zahlz.svg width=56 height=56 alt=zahlz loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Android Developer</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://zahlz.com target=_blank rel=noopener>zahlz</a></div><div class="text-muted exp-meta">Jul 2015 –
Jun 2017
<span class=middot-divider></span>
<span>Osnabrück</span></div></div></div><div class=card-text>Development of an Android Application for a mobile payment system.</div></div></div></div></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/privacy/>Privacy Policy</a>
&#183;
<a href=/terms/>Legal details</a></p><p class="powered-by copyright-license-text">© 2025 Mark Boss. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js></script><script src=https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.62586ca65ca61821fe707eb9fa6268b7.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js type=module></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin=anonymous></script></body></html>