<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mark Boss</title><link>https://markboss.me/author/mark-boss/</link><atom:link href="https://markboss.me/author/mark-boss/index.xml" rel="self" type="application/rss+xml"/><description>Mark Boss</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 May 2023 18:24:54 +0200</lastBuildDate><image><url>https://markboss.me/author/mark-boss/avatar_hu8228efeb083742449721d94a9befbadd_1942861_270x270_fill_q75_lanczos_center.jpg</url><title>Mark Boss</title><link>https://markboss.me/author/mark-boss/</link></image><item><title>NeRF at CVPR 2023</title><link>https://markboss.me/post/nerf_at_cvpr23/</link><pubDate>Mon, 01 May 2023 18:24:54 +0200</pubDate><guid>https://markboss.me/post/nerf_at_cvpr23/</guid><description>&lt;p>It is now my &lt;a href="https://markboss.me/category/literature-review/">third time&lt;/a> writing a summary of NeRFy things at a conference. This time it is the big one: CVPR. The &lt;a href="https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers" target="_blank" rel="noopener">list of accepted papers&lt;/a> is massive again, with 2359 papers.&lt;/p>
&lt;p>What is even more astounding is that the number of NeRF papers has grown significantly. I scanned the provisional program for potential NeRF titles and manually confirmed a relationship to the NeRF field.&lt;/p>
&lt;p>However, writing a summary is an entirely different task. Frank Dellaert contacted me on Twitter about the CVPR post this year, and he gave me an idea to automate the entire summary using an LLM and creating slides for the posts. He even provided me access to his tools, research database, and a ChatGPT API key. Thank you so much - the idea worked wonders :).&lt;/p>
&lt;p>So, I built a small program: ARCHIVE: Ai assisted Research Conference Human-readable Instant surVEy. It automatically creates a summary (thanks, OpenAI ðŸ˜…) and formats the blog posts below. As the blog post is rather long, I also extended the tool to auto-generate a &lt;a href="https://markboss.me/archive_presentations/cvpr23/">slide deck&lt;/a> which also showcases prominent figures from the papers (automatically extracted). The ESC key displays a slide overview and you can use the arrow keys to move through the presentation (Categories left/right, individual slides up/down). The slide title is also a link to the paper itself. The entire deck is automatically generated by the tool (including extracting images and figure descriptions).&lt;/p>
&lt;p>I am extending the tool to provide other use cases, but this will be left for another blog post.&lt;/p>
&lt;p>While I reread all summaries, my little tool might get something wrong, and I did not detect it during the final read. If this happened or I missed any paper, please send me a DM on Twitter &lt;a href="https://twitter.com/markb_boss" target="_blank" rel="noopener">@markb_boss&lt;/a> or via mail.&lt;/p>
&lt;p>Here we go again!&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: &lt;em>images below belong to the cited papers, and the copyright belongs to the authors or the organization that published these papers. Below I use key figures or videos from some papers under the fair use clause of copyright law.&lt;/em>&lt;/p>
&lt;h2 id="nerf">NeRF&lt;/h2>
&lt;p>Mildenhall &lt;em>et al.&lt;/em> introduced NeRF at ECCV 2020 in the now seminal &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">Neural Radiance Field paper&lt;/a>. As mentioned in the introduction, fantastic progress in research in this field has been made, and this is my third time covering it. CVPR 2023 is no exception, and NeRF finds applications in many new areas.&lt;/p>
&lt;p>What NeRF does, in short, is similar to the task of computed tomography scans (CT). In CT scans, a rotating head measures densities from X-rays. However, knowing where that measurement should be placed in 3D is challenging. NeRF also tries to estimate where a surface and view-dependent radiance is located in space. This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images. With many posed images, photorealistic novel view synthesis is possible.&lt;/p>
&lt;h3 id="fundamentals">Fundamentals&lt;/h3>
&lt;figure>
&lt;video src="https://dynibar.github.io/static/videos/new-teaser_3d.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Performance improvements with &lt;a href="https://dynibar.github.io" target="_blank" rel="noopener">DynIBaR&lt;/a> on complex dynamic novel view synthesis.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>These papers address more fundamental problems of view-synthesis with NeRF methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14001v1" target="_blank" rel="noopener">Grid-guided Neural Radiance Fields for Large Urban Scenes&lt;/a>&lt;/strong>: The authors propose a new methodology for high fidelity rendering in large urban scenes using a multiresolution ground feature plane representation in combination with an MLP-based neural radiance field (NeRF). This allows for the benefits of both a lightweight NeRF and joint-optimized ground planes, resulting in photorealistic novel views with fine details.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.04452v2" target="_blank" rel="noopener">Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos&lt;/a>&lt;/strong>: ReRF is introduced as a compact neural representation for long-duration dynamic scenes enabling real-time free-view video rendering, using a global coordinate-based tiny MLP as the feature decoder. A compact grid-based approach is utilized to handle large motions in interframe features. Improved compression and faster, higher-quality video generation were demonstrated using ReRF.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.08476v1" target="_blank" rel="noopener">SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory&lt;/a>&lt;/strong>: Neural Lightweight View Synthesis uses temporal consistency to render novel views faster than traditional techniques. A low-resolution feature map is generated first, and a lightweight 2D neural renderer is applied to generate the output image at target resolution leveraging the features of preceding and current frames.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.16386v1" target="_blank" rel="noopener">Compressing Volumetric Radiance Fields to 1 MB&lt;/a>&lt;/strong>: VQRF introduces a framework for compressing volumetric radiance fields by pruning grid models and applying vector quantization to improve compactness, resulting in a 100x compression ratio with minimal loss in visual quality. The proposed approach is generalizable across multiple volumetric structures and facilitates the use of volumetric radiance fields in real-world applications.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.07871v1" target="_blank" rel="noopener">DINER: Disorder-Invariant Implicit Neural Representation&lt;/a>&lt;/strong>: DINER overcomes the spectral limitations of implicit neural representations using a hash table. This allows the network to handle arbitrary ordering of input signals and generalize better across different tasks. The authors demonstrate the superiority of their approach compared to state-of-the-art algorithms across various tasks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.07418v1" target="_blank" rel="noopener">FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization&lt;/a>&lt;/strong>: FreeNeRF proposes a baseline for few-shot novel view synthesis with sparse inputs using frequency regularization on NeRF&amp;rsquo;s inputs and densities. This leads to superior performance compared to existing complicated methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.10537v1" target="_blank" rel="noopener">Learning Neural Duplex Radiance Fields for Real-Time View Synthesis&lt;/a>&lt;/strong>: Duplex Mesh Neural Radiance Fields (DM-NeRF) bakes neural radiance fields into mesh representations for better rendering performance and screen-space convolution. DM-NeRF distills and compresses the radiance information on a two-layer mesh structure to allow fast and accurate rendering with minimal MLP evaluations for each pixel. They show improved performance on standard datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2306.03092v2" target="_blank" rel="noopener">Neuralangelo: High-Fidelity Neural Surface Reconstruction&lt;/a>&lt;/strong>: Neuralangelo uses a multi-resolution 3D hash grid to learn representations for dense 3D surface structures from multi-view images and videos. It also uses numerical gradients and coarse-to-fine optimization for higher-fidelity surface reconstruction.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12562v2" target="_blank" rel="noopener">PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices&lt;/a>&lt;/strong>: PermutoSDF expands on Neural Radiance-Density field by using permutohedral lattice to encode the SDF and achieve faster optimization and high-frequency detail retrieval. The authors&amp;rsquo; regularization scheme is additionally crucial to high-frequency geometric detail recovery, while novel view rendering (using sphere tracing) is also achieved at a high fps rate.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.04268v1" target="_blank" rel="noopener">Multi-Space Neural Radiance Fields&lt;/a>&lt;/strong>: MS-NeRF represents scenes using parallel feature fields in sub-spaces to handle reflective and refractive objects. It is a modification of existing NeRF methods with small computational overheads, providing better rendering performance for complex light paths through mirrored objects.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rivas-Manzaneque_NeRFLight_Fast_and_Light_Neural_Radiance_Fields_Using_a_Shared_CVPR_2023_paper.html" target="_blank" rel="noopener">NeRFLight: Fast and Light Neural Radiance Fields using a Shared Feature Grid&lt;/a>&lt;/strong>: The authors propose a lightweight method for real-time view synthesis via a decoupled grid-based NeRF approach. The approach uses multiple density decoders that share one common feature grid. This results in a model that achieves real-time performance while maintaining high-quality models.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yoon_Cross-Guided_Optimization_of_Radiance_Fields_With_Multi-View_Image_Super-Resolution_for_CVPR_2023_paper.html" target="_blank" rel="noopener">Cross-Guided Optimization of Radiance Fields with Multi-View Image Super-Resolution for High-Resolution Novel View Synthesis&lt;/a>&lt;/strong>: The paper proposes a differentiable framework for cross-guided optimization of single-image super-resolution and radiance fields for high-resolution novel view synthesis. By performing multi-view image super-resolution during radiance fields optimization, train-view images obtain multi-view consistency and high-frequency details, leading to better performance in novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.14340v2" target="_blank" rel="noopener">HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization&lt;/a>&lt;/strong>: HelixSurf combines traditional multi-view stereo with neural implicit surface learning to improve scene geometry reconstruction. The method uses intermediate predictions from one strategy to guide the learning of the other in an iterative process.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.01735v2" target="_blank" rel="noopener">Neural Fourier Filter Bank&lt;/a>&lt;/strong>: The authors propose a grid-based paradigm for spatial decomposition, which is optimized to store the information both spatially and frequency-wise. The method uses adaptive sine activation features, and the network is composed of sine activation fully connected layers, which learn to decompose signals over different scales and frequencies progressively. The proposed method shows improved performance over the state-of-the-art techniques.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13791v1" target="_blank" rel="noopener">Progressively Optimized Local Radiance Fields for Robust View Synthesis&lt;/a>&lt;/strong>: The authors propose an algorithm to reconstruct a large-scale scene&amp;rsquo;s radiance field from a single video. They do so by jointly estimating camera poses and a radiance field in a progressive manner. Local radiance fields are trained to handle large and unbounded scenes within a temporal window.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2208.00277v5" target="_blank" rel="noopener">MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures&lt;/a>&lt;/strong>: PolyNeRF is a new approach for Neural Radiance Fields that replaces ray marching with polygon rasterization algorithms. This allows for fast and efficient rendering while maintaining high quality.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15060v1" target="_blank" rel="noopener">TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering&lt;/a>&lt;/strong>: The authors introduce a pipeline for creating textured meshes from a single smartphone by first using RGB-D aided structure from motion. This is followed by neural implicit surface reconstruction and differentiable rendering to generate finetuned texture maps that are closer to the original scene.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.05937v1" target="_blank" rel="noopener">Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction&lt;/a>&lt;/strong>: S-MPI improves MPI by approximating the 3D scene of the image with plane structures that produce high-quality results for both RGBA layers and plane poses. Instead of an MPI, S-MPI accounts for non-planar surfaces and multi-view consistency. A transformer-based network architecture is deployed to produce expressive S-MPI layers and corresponding masks, poses, and RGBA contexts.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2204.06552v3" target="_blank" rel="noopener">Neural Vector Fields for Implicit Surface Representation and Inference&lt;/a>&lt;/strong>: Vector Fields (VF) are proposed as a new implicit 3D shape representation, yielding faster convergence rates, higher data fidelity, and superior geometric detail compared to traditional distance-based approaches. VF&amp;rsquo;s significant advantage over existing methods is that the insertion of a singularity in the field wherever geometry changes occur enables learning of discontinuous surfaces and interior boundaries.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.11601v1" target="_blank" rel="noopener">Towards Better Gradient Consistency for Neural Signed Distance Functions via Level Set Alignment&lt;/a>&lt;/strong>: This paper proposes a level set alignment loss to better the accuracy of neural signed distance function (SDF) inference from point clouds or multi-view images. Their approach constrains gradients at queries to ensure better gradient consistency across the field. They demonstrate the effectiveness of the method through various benchmarks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.05187v1" target="_blank" rel="noopener">WIRE: Wavelet Implicit Neural Representations&lt;/a>&lt;/strong>: WIRE is a Wavelet-based Implicit neural REpresentation that uses a continuous complex Gabor wavelet activation function, allowing it to achieve high accuracy and robustness. The authors show that WIRE outperforms other INR models in image denoising, super-resolution, computed tomography reconstruction, and novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.08788v2" target="_blank" rel="noopener">MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs&lt;/a>&lt;/strong>: MixNeRF improves the efficiency of NeRF by using a mixture of distributions to estimate a ray&amp;rsquo;s RGB colors and a new training objective based on ray depth estimation. It outperforms other state-of-the-art methods and is more efficient in terms of both training and inference.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.08706v1" target="_blank" rel="noopener">Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections&lt;/a>&lt;/strong>: NeuS-HSR is a surface reconstruction framework based on implicit neural rendering, which can deal with high specular reflections of objects when captured through glasses. The framework parameterizes the object surface as an implicit signed distance function and decomposes the rendered image into a target object and auxiliary plane appearance to generate auxiliary plane appearances. NeuS-HSR outperforms state-of-the-art approaches in reconstructing target surfaces accurately and robustly against high specular reflections.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.14173v1" target="_blank" rel="noopener">NeuralUDF: Learning Unsigned Distance Fields for Multi-view Reconstruction of Surfaces with Arbitrary Topologies&lt;/a>&lt;/strong>: NeuralUDF introduces a new method for reconstructing arbitrary-topology surfaces from 2D images with volume rendering. By using Unsigned Distance Functions (UDFs) and a new density function, NeuralUDF enables high-quality reconstruction of non-closed shapes, achieving comparable performance to Signed Distance Function (SDF) based methods for closed surfaces.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2209.15511v2" target="_blank" rel="noopener">Sphere-Guided Training of Neural Implicit Surfaces&lt;/a>&lt;/strong>: SphereGuided jointly trains a neural distance function with a coarse sphere-based triangular reconstruction to improve sampling efficiency in high-frequency detail regions.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.17968v1" target="_blank" rel="noopener">VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence Normalization&lt;/a>&lt;/strong>: VDN-NeRF is a new method that normalizes the geometry of neural radiance fields (NeRF) by distilling invariant information encoded in the fields. This technique improves the synthesis of 3D scenes with dynamic lighting or non-Lambertian surfaces and minimizes shape-radiance ambiguity.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.12012v1" target="_blank" rel="noopener">NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from Multi-view Images&lt;/a>&lt;/strong>: NeAT is a new neural rendering framework that represents 3D surfaces as a level set of a signed distance function with a validity branch. This allows for learning implicit surfaces with arbitrary topologies from multi-view images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.08971v1" target="_blank" rel="noopener">SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Reconstruction of Indoor Scenes&lt;/a>&lt;/strong>: SurfelNeRF combines explicit geometric surfel representations with NeRF rendering to enable efficient online reconstruction and high-quality rendering. The method also includes a differentiable rasterization scheme for rendering the neural surfel radiance fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.14086v2" target="_blank" rel="noopener">ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision&lt;/a>&lt;/strong>: ShadowNeuS proposes to model shadow rays, which result from light sources in order to reconstruct an SDF neural model from single RGB views or corresponding shadow information when full scene sampling is not available. The approach allows the effective reconstruction of 3D models beyond the line of sight.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.03361v2" target="_blank" rel="noopener">Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision&lt;/a>&lt;/strong>: Nerflets are introduced as a local scene representation. Each Nerflet represents local information regarding object type, location, and orientation within a scene. By joint optimization of Nerflet parameters, efficient and structure-aware 3D scene representation can be obtained without global modeling.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15484v1" target="_blank" rel="noopener">Regularize implicit neural representation by itself&lt;/a>&lt;/strong>: INRR is introduced as a regularizer for the Implicit Neural Representation (INR). INRR measures the similarity between rows/columns of a matrix and integrates the smoothness of the Laplacian matrix by parameterizing learned Dirichlet Energy with a small INR. The method aims to improve the generalization of INR for signal representation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.00833v1" target="_blank" rel="noopener">RobustNeRF: Ignoring Distractors with Robust Losses&lt;/a>&lt;/strong>: Robust NeRF suggests incorporating an outlier rejection component to NeRF training, removing moving objects and ephemeral elements from the training data. The method is a simple optimization problem and works with existing NeRF frameworks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2209.00082v2" target="_blank" rel="noopener">Multi-View Reconstruction using Signed Ray Distance Functions (SRDF)&lt;/a>&lt;/strong>: This paper introduces a new optimization framework for multi-view 3D shape reconstructions using a novel volumetric shape representation that combines differentiable rendering with local depth predictions to yield pixel-wise geometric accuracy. The approach optimizes the depths of an implicit-parameterized shape representation at each 3D location. The method outperforms existing approaches for geometry estimation over standard 3D benchmarks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_Self-Supervised_Super-Plane_for_Neural_3D_Reconstruction_CVPR_2023_paper.html" target="_blank" rel="noopener">Self-supervised Super-plane for Neural 3D Reconstruction&lt;/a>&lt;/strong>: S3PRecon introduces self-supervised super-plane constraints for neural implicit surface representation methods to handle texture-less planar regions without any annotated datasets. An iterative training scheme of grouping pixels and optimizing the reconstruction network via a super-plane constraint is used to achieve better performance than using ground truth plane segmentation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.05594v1" target="_blank" rel="noopener">PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces&lt;/a>&lt;/strong>: PET-NeuS improves NeuS&amp;rsquo;s MLP sign distance field parametrization by representing the signed distance field using triplanes and MLPs in a mixture. This results in a more expressive data structure with noise. PET-NeuS ameliorates this noise by using a new learnable positional encoding and a self-attention convolution operation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_RefSR-NeRF_Towards_High_Fidelity_and_Super_Resolution_View_Synthesis_CVPR_2023_paper.html" target="_blank" rel="noopener">RefSR-NeRF: Towards High Fidelity and Super Resolution View Synthesis&lt;/a>&lt;/strong>: RefSR-NeRF improves NeRF super-resolution images by first generating a low-resolution image and then using a high-resolution reference image to reconstruct high-frequency details. The authors design a novel lightweight RefSR model for learning the inverse degradation process from NeRF renderings to target HR images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11082v3" target="_blank" rel="noopener">DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>&lt;/strong>: DynIBAR synthesizes realistic views in the presence of large camera and object movements in videos by adapting the image-based rendering method in a scene-motion-aware manner. Efficient and performs well against long videos of complex scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_F2-NeRF_Fast_Neural_Radiance_Field_Training_With_Free_Camera_Trajectories_CVPR_2023_paper.html" target="_blank" rel="noopener">F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories&lt;/a>&lt;/strong>: F^2-NeRF introduces a new warping method, called perspective warping, in the context of grid-based NeRFs, enabling it to handle unbounded scenes. The method shows significant performance improvements compared to other approaches on various free-camera single-object datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.02375v2" target="_blank" rel="noopener">NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction&lt;/a>&lt;/strong>: NeuDA proposes a hierarchical approach to implicit surface reconstruction using anchor grids to capture 3D contexts. By maintaining an adaptive anchor structure, capturing different topological structures is achieved.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_PlenVDB_Memory_Efficient_VDB-Based_Radiance_Fields_for_Fast_Training_and_CVPR_2023_paper.html" target="_blank" rel="noopener">PlenVDB: Memory Efficient VDB-Based Radiance Fields for Fast Training and Rendering&lt;/a>&lt;/strong>: PlenVDB aims to accelerate the training and inference stages in NeRFs by introducing the VDB hierarchical sparsely-filled data structure. This method accomplished results with faster training convergence, compressed data for NeRF models, and faster rendering on commercial hardware.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.07743v1" target="_blank" rel="noopener">SeaThru-NeRF: Neural Radiance Fields in Scattering Media&lt;/a>&lt;/strong>: NeRF in the Fog modifies NeRF to account for the medium&amp;rsquo;s transmission and scattering. The authors use SeaThru&amp;rsquo;s image formation model and propose a suitable architecture. The method can also render clear views, removing the medium between the camera and the scene.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.16630v2" target="_blank" rel="noopener">DINER: Depth-aware Image-based NEural Radiance fields&lt;/a>&lt;/strong>: DINER uses depth information to guide the reconstruction of a volumetric neural radiance field representation for 3D object rendering. DINER achieves higher synthesis quality than the state-of-the-art methods and can capture scenes more completely with greater disparity.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.09069v2" target="_blank" rel="noopener">Masked Wavelet Representation for Compact Neural Radiance Fields&lt;/a>&lt;/strong>: The authors propose a method to compress grid-based neural fields and make them more efficient using wavelet transforms. Their approach includes a trainable masking technique that produces a more compact representation and achieves state-of-the-art performance within a memory budget of 2 MB.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12285v2" target="_blank" rel="noopener">Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for Neural Radiance Fields&lt;/a>&lt;/strong>: Exact-NeRF computes the Integrated Positional Encoding in a pyramid-based, precise analytical approach, rather than an approximated conical one. The paper shows that this new approach outperforms the approximated model when scenes are distant or extended.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.06919v2" target="_blank" rel="noopener">NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer&lt;/a>&lt;/strong>: NeRFLiX trains an inter-viewpoint mixer to remove rendering artifacts like noise and blur in existing NeRF models, using a NeRF degradation modeling approach and inter-viewpoint aggregation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.10080v1" target="_blank" rel="noopener">NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering&lt;/a>&lt;/strong>: NeUDF is an extension to signed distance function (SDF) reconstruction algorithms to recover arbitrary shapes with open surfaces. The method utilizes the unsigned distance function (UDF) as the underlying representation and employs two new formulations of weight function and normal regularization strategy for efficient volume rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.06689v4" target="_blank" rel="noopener">TINC: Tree-structured Implicit Neural Compression&lt;/a>&lt;/strong>: TINC proposes a tree-structured approach to compress implicit neural representations of data through partitioned local MLP fitting. The parameter-sharing approach helps to capture both local and non-local correlations across the scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13817v1" target="_blank" rel="noopener">ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field&lt;/a>&lt;/strong>: ABLE-NeRF improves the view-dependent effects of Neural Radiance Fields (NeRF) in volumetric rendering by using a self-attention-based framework along rays and Learnable Embeddings to capture local lighting. The method reduces the artifacts on several materials, resulting in higher-quality rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.12652v1" target="_blank" rel="noopener">Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur&lt;/a>&lt;/strong>: The Hybrid Neural Rendering model uses a combination of neural and image-based representations to render high-fidelity, view-consistent images, even for large-scale scenes with motion blur. Additionally, the authors propose methods to simulate the blur effects and reduce the impact of blurriness during training.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13805v1" target="_blank" rel="noopener">Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container&lt;/a>&lt;/strong>: ReNeuS is a novel method for recovering the 3D geometry of objects in transparent enclosures. It models the scene as two sub-spaces, using an existing method NeuS to represent the inner sub-space. A combination of volume rendering and ray tracing is used to render the model, and then the geometry and appearance are recovered by minimizing differences between real and hybrid-rendered images.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.08057v1" target="_blank" rel="noopener">Real-Time Neural Light Field on Mobile Devices&lt;/a>&lt;/strong>: MobileNeRF introduces a mobile-friendly network architecture that runs in real-time on mobile devices for neural rendering of 3D scenes, with high-resolution generation and similar image quality as NeRF. It requires low latency and small size, saving 15-24 times the storage compared with MobileNeRF.&lt;/p>
&lt;h3 id="priors-and-generative">Priors and Generative&lt;/h3>
&lt;figure>
&lt;video src="https://raw.githubusercontent.com/paintingnature/paintingnature.github.io/master/static/videos/4.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Create realistic 3D landscape synthesis using a single semantic mask using &lt;a href="https://arxiv.org/abs/2302.07224" target="_blank" rel="noopener">Painting 3D Nature in 2D&lt;/a>.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>Priors can either aid in the reconstruction or can be used in a generative manner. For example, in the reconstruction, priors either increase the quality of neural view synthesis or enable reconstructions from sparse image collections.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.07702v2" target="_blank" rel="noopener">Learning 3D-aware Image Synthesis with Unknown Pose Distribution&lt;/a>&lt;/strong>: The PoF3D method frees generative radiance fields from the requirements of 3D pose priors. It equips the generator with an efficient pose learner to infer a pose from a latent code and assigns the discriminator a task to learn pose distribution. The pose-free generator and the pose-aware discriminator are jointly trained in an adversarial manner.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.07224v1" target="_blank" rel="noopener">Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask&lt;/a>&lt;/strong>: The paper introduces a novel approach for 3D-aware image synthesis that can produce photorealistic multi-view consistent color images of natural scenes. The key idea is to use a semantic field as an intermediate representation and convert it to a radiance field using semantic image synthesis models. The method outperforms baseline methods and requires only a single semantic mask input.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11674v2" target="_blank" rel="noopener">Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion&lt;/a>&lt;/strong>: The end-to-end monetizable NeRF (MeNRF) framework generates high-quality 3D reconstructions with accurate pose and appearance from a single image of arbitrary topologies. MeNRF leverages an unconditional 3D-aware generator while using a hybrid inversion scheme to refine the solution via optimization without exploiting multiple views. The network can de-render an image in as few as 10 steps, which is useful for practical applications.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.12746v1" target="_blank" rel="noopener">Local Implicit Ray Function for Generalizable Radiance Field Representation&lt;/a>&lt;/strong>: LIRF is a novel approach to neural rendering that aggregates information from conical frustums to construct each ray resulting in high-quality novel view rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.08247v1" target="_blank" rel="noopener">Multiview Compressive Coding for 3D Reconstruction&lt;/a>&lt;/strong>: MCC is a method for single-view 3D reconstruction that operates on 3D points of single objects or whole scenes. Its efficient size compression allows large-scale training from diverse RGB-D videos for learning a generalizable representation. MCC shows strong generalization to novel objects and objects captured in the wild.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.13223v2" target="_blank" rel="noopener">Generalizable Implicit Neural Representations via Instance Pattern Composers&lt;/a>&lt;/strong>: The authors present a new method for implicit neural representations (INRs) which improves generalization by learning a small set of weights that modulate an early layer of the MLP network while keeping the remaining MLP weights constant. The resulting pattern composition rules enable the network to represent common features across instances.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12046v4" target="_blank" rel="noopener">DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors&lt;/a>&lt;/strong>: DP-NeRF proposes a clean novel framework to handle blurry images for 3D reconstruction. The approach utilizes two physical priors for color consistency and 3D geometric consistency, which are derived from the actual blurring process during image acquisition by the camera. The authors show that the proposed method improves the perceptual quality of the constructed scene in synthetic and real scenes with both camera motion blur and defocus blur.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Song_DIFu_Depth-Guided_Implicit_Function_for_Clothed_Human_Reconstruction_CVPR_2023_paper.html" target="_blank" rel="noopener">DIFu: Depth-Guided Implicit Function for Clothed Human Reconstruction&lt;/a>&lt;/strong>: DIFu is a new method for single image clothed human reconstruction. It uses projected depth information to create a voxel-aligned human reconstruction that can contain pixels with detailed 3D information, such as hair and clothing and estimates occupancies with pixel and voxel-aligned features. The method also includes a texture inference branch for color estimation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.05321v1" target="_blank" rel="noopener">HumanGen: Generating Human Radiance Fields with Explicit Priors&lt;/a>&lt;/strong>: HumanGen is a 3D human generation method that combines various priors from 2D and 3D models of humans using an anchor image. It features a hybrid feature representation, pronged design for geometry and appearance generation, and incorporates off-the-shelf 2D latent editing methods into 3D. The method generates view-consistent radiance fields with detailed geometry and realistic free-view rendering.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.09869v2" target="_blank" rel="noopener">RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation&lt;/a>&lt;/strong>: RenderDiffusion is a diffusion-based model for 3D inference, training and generation. The authors present a novel image denoising method that provides consistency with a 3D intermediate representation. The method enables 2D inpainting for editing 3D scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.00774v1" target="_blank" rel="noopener">Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&lt;/a>&lt;/strong>: A properly trained diffusion model can be used to backpropagate score through the Jacobian of a differentiable renderer. A scene representation example can be a voxel radiance field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15012v1" target="_blank" rel="noopener">3D-Aware Multi-Class Image-to-Image Translation with NeRFs&lt;/a>&lt;/strong>: The paper proposes a new method for 3D-aware multi-class image-to-image (I2I) translation using a combination of a 3D-aware GAN step and a 3D-aware I2I translation step. The authors introduce a new conditional architecture and training strategy for the multi-class GAN and several new techniques for the I2I translation step to improve view-consistency.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.07600v1" target="_blank" rel="noopener">Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures&lt;/a>&lt;/strong>: This paper proposes Latent-NeRF, which uses score distillation adapted to latent diffusion models for text-guided image generation. The authors integrate sketch-shape constraints to control the 3D shape generation process and apply latent score distillation on 3D meshes. Implementation is available at the provided GitHub link.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.11052v3" target="_blank" rel="noopener">ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real Novel View Synthesis via Contrastive Learning&lt;/a>&lt;/strong>: The paper reports that models trained on synthetic data tend to produce sharper but less accurate volume densities. To address this, a geometry-aware contrastive learning approach is introduced, and cross-view attention is adopted. The method helps to render higher quality and better detailed images when working with synthetic-to-real novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13582v1" target="_blank" rel="noopener">SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates&lt;/a>&lt;/strong>: SCADE improves NeRF reconstruction quality by leveraging depth estimates produced with monocular depth estimation models, which can generalize across scenes. It uses a space carving loss to fusing multiple hypothesized depth maps from each view and distilling from them a consistent geometry.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.08067v2" target="_blank" rel="noopener">VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction&lt;/a>&lt;/strong>: VolRecon is introduced as a more generalizable neural implicit scene reconstruction method with Signed Ray Distance Function. It projects multi-view features and combines them with volume features.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14662v1" target="_blank" rel="noopener">OTAvatar: One-shot Talking Face Avatar with Controllable Tri-plane Rendering&lt;/a>&lt;/strong>: OTAvator is a one-shot learning system for 3D facial avatars. It employs tri-plane volumetric rendering with an efficient CNN and disentangles facial identity and motion representing using a decoupling-by-inverting strategy, which allows to create new avatars quickly from as few as one reference portrait.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.16509v2" target="_blank" rel="noopener">HoloDiffusion: Training a 3D Diffusion Model using 2D Images&lt;/a>&lt;/strong>: The authors address the challenge of 3D training data scarcity and the memory complexity of 3D extension by introducing a new diffusion model that can be trained using only 2D images and an image formation model that decouples model memory from spatial memory. The approach is shown to be competitive with existing techniques on the CO3D dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.03267v1" target="_blank" rel="noopener">NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a>&lt;/strong>: NeRDi is a NeRF-based single-view 3D reconstruction method with general image priors from 2D diffusion models, using pre-trained vision-language models for conditional input and depth maps for geometric regularization.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13515v1" target="_blank" rel="noopener">Persistent Nature: A Generative Model of Unbounded 3D Worlds&lt;/a>&lt;/strong>: The authors present a method for the unconditioned synthesis of unbounded nature scenes based on an extendible planar scene layout grid and a panoramic skydome. Renderer can freely move through the space without auto-regression.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.16677v1" target="_blank" rel="noopener">3D Neural Field Generation using Triplane Diffusion&lt;/a>&lt;/strong>: TriDiff is a diffusion-based model for 3D-aware generation of neural fields. TriDiff preprocesses training data by converting meshes to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Training the diffusion model with these representations yields high-quality 3D neural fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shim_Diffusion-Based_Signed_Distance_Fields_for_3D_Shape_Generation_CVPR_2023_paper.html" target="_blank" rel="noopener">Diffusion-Based Signed Distance Fields for 3D Shape Generation&lt;/a>&lt;/strong>: SDF-Diffusion proposes a two-stage generative process by using diffusion models. The first stage generates a low-resolution SDF, which is further processed in the second stage to obtain high-resolution results. The network can generate complex high-resolution 3D shapes using 3D SDF previously used for shape completion tasks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Towards_Unbiased_Volume_Rendering_of_Neural_Implicit_Surfaces_With_Geometry_CVPR_2023_paper.html" target="_blank" rel="noopener">Towards Unbiased Volume Rendering of Neural Implicit Surfaces with Geometry Priors&lt;/a>&lt;/strong>: The paper proposes a new way to render Signed Distance Functions, where the scale factor is dependent on angle to the normal vector of the surface, which leads to a reduction in bias in volume rendering. The authors pre-train a Multi-View Stereo network for supervision at zero crossing intersection points between the implicit surface and the viewing frustum.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.html" target="_blank" rel="noopener">Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild&lt;/a>&lt;/strong>: NPF is a novel Neural Proto-face Field that disentangles the common/specific facial cues to allow precise face modeling. It is able to learn 3D-consistent identity via uncertainty modeling and multi-image priors from photo collections. The disentangled learning methodology predicts superior 3D face shapes and textures compared to the state-of-the-art methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.10440v2" target="_blank" rel="noopener">Magic3D: High-Resolution Text-to-3D Content Creation&lt;/a>&lt;/strong>: Magic3D is a two-stage optimization framework that uses a diffusion prior to obtain a coarse model and accelerates it with a sparse 3D hash grid structure. It further optimizes a textured 3D mesh model to create high-quality 3D mesh models in 40 minutes, which is 2x faster than DreamFusion.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.01206v2" target="_blank" rel="noopener">DiffRF: Rendering-guided 3D Radiance Field Diffusion&lt;/a>&lt;/strong>: DiffRF proposes a novel volumetric radiance field synthesis based on denoising diffusion probabilistic models. The framework is designed to generate radiance fields by rendering a set of posed images with a deviated prior, which contains multi-view consistent priors with good quality for image synthesis. Unlike 3D GANs, the method allows free-view synthesis through learning multi-view consistent priors at inference time.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.12231v2" target="_blank" rel="noopener">DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models&lt;/a>&lt;/strong>: NeRF-Prior improves NeRF training introducing regularizing RGBD patch priors. These priors are learned with a denoising diffusion model and can improve the generalization of reconstructed geometry and color fields to novel scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.04965v1" target="_blank" rel="noopener">Seeing a Rose in Five Thousand Ways&lt;/a>&lt;/strong>: The proposed method in this work is capable of learning object intrinsics (geometry, texture, and material) from a single image of a certain object category, such as roses, and then use this knowledge to generate different images of the same object under changing poses and lighting conditions. The resulting model shows a superior performance across various related tasks compared to existing methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.09787v1" target="_blank" rel="noopener">NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models&lt;/a>&lt;/strong>: NeuralField-LDM is a generative model that can synthesize complex 3D environments using Latent Diffusion Models for efficiency and high-quality 3D content. With a scene autoencoder, voxel grids, and latent-autoencoder, the authors improve upon existing scene generation models and demonstrate potential uses in 3D content creation applications.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.17260v2" target="_blank" rel="noopener">SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene&lt;/a>&lt;/strong>: SinGRAF is a 3D-aware generative model that can generate photorealistic 3D objects with few input images. The 3D GAN architecture enables SinGRAF to produce different realizations of a scene while preserving appearance and varying the layout.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.11424v1" target="_blank" rel="noopener">Polynomial Implicit Neural Representations For Large Diverse Datasets&lt;/a>&lt;/strong>: Poly-INR is a new implicit neural representation technique that replaces sinusoidal positional encoding with polynomial functions. The proposed model eliminates the need for positional encodings and performs comparably to state-of-the-art generative models with far fewer trainable parameters.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.06287v2" target="_blank" rel="noopener">NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds&lt;/a>&lt;/strong>: NeRFVS utilizes holistic priors such as pseudo-depth maps and view coverage information to guide the learning of implicit neural representations of 3D indoor scenes. Robust depth loss and variance loss are proposed to further improve the performance, and these losses are modulated during NeRF optimization according to the view coverage information to reduce the influence of view coverage imbalance. The method achieves high-fidelity free navigation results on indoor scenes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13777v1" target="_blank" rel="noopener">GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images&lt;/a>&lt;/strong>: GM-NeRF synthesizes novel view images for human performers using a geometry-guided attention mechanism and neural rendering. This allows efficient improvement of perceptual quality of synthesis and outperforms state-of-the-art methods in terms of novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.02163v1" target="_blank" rel="noopener">GINA-3D: Learning to Generate Implicit Neural Assets in the Wild&lt;/a>&lt;/strong>: GINA-3D uses camera and LiDAR data to learn 3D assets of vehicles and pedestrians using a generative approach. The method decouples representation learning and generative modeling into two stages with a tri-plane latent structure, which is shown to perform better than existing approaches when evaluated on a large-scale object-centric dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11208v2" target="_blank" rel="noopener">Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars&lt;/a>&lt;/strong>: The authors propose a novel 3D GAN framework for unsupervised learning of high-quality facial avatars from unstructured 2D images. The method introduces a new 3D representation called generative texture-rasterized tri-planes. The proposed representation accurately models deformation and flexibility, enabling fine-grained expression control and animation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.16431v2" target="_blank" rel="noopener">NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360Â° Views&lt;/a>&lt;/strong>: NeuralLift-360 is a technique that generates a 3D object with 360-degree views that correspond well with a reference image, easing workflows for 3D artists and XR designers. It uses a depth-aware NeRF and denoising diffusion models guided by CLIP to provide coherent guidance and can be guided with rough depth estimation in the wild through a ranking loss. The method outperforms existing state-of-the-art baselines.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.17235v1" target="_blank" rel="noopener">NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation&lt;/a>&lt;/strong>: This paper proposes a method to finetune NeRF-GAN models to generate high-fidelity animation of real subjects based on a single image. The method includes 2D loss functions to reduce the identity gap, as well as explicit and implicit 3D regularizations to remove artifacts.&lt;/p>
&lt;h3 id="dynamic">Dynamic&lt;/h3>
&lt;figure>
&lt;video src="https://blendfields.github.io/videos/concatenated/supplementary_expression.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Neural fields to create blends of various facial expressions using &lt;a href="https://arxiv.org/abs/2305.07514" target="_blank" rel="noopener">BlendFields&lt;/a>.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>Capturing dynamic objects is a trend that started in previous conference years. This can either be solved with parametrization or via neural priors.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.07514v1" target="_blank" rel="noopener">BlendFields: Few-Shot Example-Driven Facial Modeling&lt;/a>&lt;/strong>: The authors propose a solution for fine-grained face rendering that blends sparse expressions to infer the appearance of unseen expressions. The fine-grained details are captured as appearance differences between each of the extreme poses in their method. The approach is robust and generalizes well beyond faces.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12499v2" target="_blank" rel="noopener">INSTA - Instant Volumetric Head Avatars&lt;/a>&lt;/strong>: INSTA uses a neural radiance field-based pipeline to reconstruct digital avatars from a single monocular RGB video. The output model, based on a parametric face model, offers high-quality rendering and interactivity. It also performs well in unseen pose conditions.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14124v1" target="_blank" rel="noopener">Towards Scalable Neural Representation for Diverse Videos&lt;/a>&lt;/strong>: D-NeRV is an INR-based framework designed for encoding long and diverse videos. It decouples visual content from motion information while introducing temporal reasoning into the implicit neural network to improve compression results. The proposed model surpasses NeRV and traditional video compression techniques while also achieving higher accuracy on action recognition tasks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.12237v2" target="_blank" rel="noopener">Learning Neural Volumetric Representations of Dynamic Humans in Minutes&lt;/a>&lt;/strong>: The paper presents a novel technique to accelerate the learning process of neural radiance fields for free-viewpoint video reconstruction from sparse multi-view videos. The proposed method uses a part-based voxelized representation and a 2D motion parameterization scheme to increase convergence rates. The method is shown to achieve competitive visual quality with a much faster training process.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12497v3" target="_blank" rel="noopener">MagicPony: Learning Articulated 3D Animals in the Wild&lt;/a>&lt;/strong>: MagicPony is a method for predicting detailed 3D articulated shapes and appearances of animals using only single-view images of the same category. It utilizes a novel implicit-explicit representation that combines the strengths of neural fields and meshes. MagicPony includes a self-supervised visual transformer and a viewpoint sampling technique to improve performance and generalization.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.02633v1" target="_blank" rel="noopener">HNeRV: A Hybrid Neural Representation for Videos&lt;/a>&lt;/strong>: HNeRV uses content-adaptive embeddings and re-designed architecture to outperform existing methods in video regression besides allowing for higher resolution and fewer parameters. The method also shows advantages in video decoding for speed, flexibility, and deployment. HNeRV can be used in downstream tasks like video compression and inpainting.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2306.07579v1" target="_blank" rel="noopener">Parametric Implicit Face Representation for Audio-Driven Facial Reenactment&lt;/a>&lt;/strong>: This work proposes a novel audio-driven facial reenactment framework that uses a parametric, interpretable implicit face representation. It improves audio-to-expression parameters encoding, uses conditional image synthesis, and data augmentation techniques to achieve high-quality results with more fidelity to the identities and speaking styles of speakers.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.14593v1" target="_blank" rel="noopener">NIRVANA: Neural Implicit Representations of Videos with Adaptive Networks and Autoregressive Patch-wise Modeling&lt;/a>&lt;/strong>: NIRVANA adopts a patch-wise approach to video compression where groups of frames are each fitted to separate networks to exploit temporal redundancy. The method uses autoregressive modeling, quantized network parameters, and scaling based on GPU use to achieve joint improvements in quality, speed, and scalability with efficient variable bitrate compression.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.02238v2" target="_blank" rel="noopener">HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling&lt;/a>&lt;/strong>: HyperReel is a 6-DoF video representation with a hyper network for ray-conditioned sample prediction. It has a compact and memory-efficient dynamic volume representation and outperforms existing approaches in visual quality, memory requirements, and frame rate.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.06544v1" target="_blank" rel="noopener">DNeRV: Modeling Inherent Dynamics via Difference Neural Representation for Videos&lt;/a>&lt;/strong>: DNeRV introduces the difference frame as an essential channel for implicit video representation, resulting in state-of-the-art performance in intraprediction and video compression on x264.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.02001v1" target="_blank" rel="noopener">MonoHuman: Animatable Human Neural Field from Monocular Video&lt;/a>&lt;/strong>: MonoHuman proposes a three-part pipeline to generate view-consistent and high-fidelity avatars under arbitrary novel poses. A shared bidirectional deformation module creates a pose-independent, generalizable deformation field, followed by a forward correspondence search module. Finally, a rendering network leverages multi-view consistent features to produce the final avatar. The authors show improved performance over current state-of-the-art methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13825v1" target="_blank" rel="noopener">HandNeRF: Neural Radiance Fields for Animatable Interacting Hands&lt;/a>&lt;/strong>: HandNeRF uses pose estimation to generate a detailed explicit triangle mesh of interacting hands from multi-view images. They design a shared axis space for multiple poses, allowing each pose to add to the view space. A neural feature distillation method is used to enhance image quality while avoiding artifacts. Expensive ground-truth data is used to remove occlusions in the learning process.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14368v1" target="_blank" rel="noopener">FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views&lt;/a>&lt;/strong>: FlexNeRF provides photorealistic free-viewpoint rendering of people in motion from monocular videos. The method handles fast and complex motions under sparse views through a joint optimization approach where canonical time and pose are optimized with pose-dependent motion fields and pose-independent temporal deformations. The approach proposes novel consistency constraints and provides improved performance over existing benchmarks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Flow_Supervision_for_Deformable_NeRF_CVPR_2023_paper.html" target="_blank" rel="noopener">Flow Supervision for Deformable NeRF&lt;/a>&lt;/strong>: The authors present a deformable NeRF method that uses optical flow as supervision, with improvements over baselines that don&amp;rsquo;t use flow supervision. They show that inverting the backward deformation function is not needed for computing scene flows between frames, simplifying the problem.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.13660v1" target="_blank" rel="noopener">NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action&lt;/a>&lt;/strong>: NeMo is a neural motion field optimized to reconstruct 3D human motion from multiple video instances of the same action. The method outperforms existing monocular HMR methods in terms of 2D keypoint detection and achieves better 3D reconstruction compared to baselines on a small MoCap dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14435v1" target="_blank" rel="noopener">NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects&lt;/a>&lt;/strong>: The paper presents a modified version of NeRF called NeRF-DS for rendering novel views from RGB video input with dynamic scenes, which is capable of modeling the reflected color of specular surfaces during motion. NeRF-DS conditions the radiance field function on surface position and orientation in the observation space and uses a mask of moving objects to guide the deformation field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.09311v2" target="_blank" rel="noopener">Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields&lt;/a>&lt;/strong>: The proposed method learns spatiotemporal neural representations for scenes using neural network modules or 4D hash grids for extracting and interpolating features from space-time inputs, achieving state-of-the-art performance and/or 100 times faster training speed.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.02239v2" target="_blank" rel="noopener">Robust Dynamic Radiance Fields&lt;/a>&lt;/strong>: DRF-Net improves the robustness of dynamic radiance field reconstruction by jointly estimating the static and dynamic radiance fields alongside camera parameters. The method demonstrates improved performance on challenging videos compared to state-of-the-art methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Distilling_Neural_Fields_for_Real-Time_Articulated_Shape_Reconstruction_CVPR_2023_paper.html" target="_blank" rel="noopener">Distilling Neural Fields for Real-Time Articulated Shape Reconstruction&lt;/a>&lt;/strong>: The authors present a method for real-time reconstruction of articulated 3D models from video without test-time optimization or manual 3D supervision. The method trains a fast feed-forward network using off-the-shelf video-based dynamic NeRFs as 3D supervision to reconstruct arbitrary deformations represented by articulated bones and blend skinning.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.11113v1" target="_blank" rel="noopener">Implicit Neural Head Synthesis via Controllable Local Deformation Fields&lt;/a>&lt;/strong>: The paper presents a method of generating personalized 3D head avatars from 2D videos, with sharper deformations and greater facial detail compared to existing methods. This is achieved through a novel formulation of multiple implicit deformation fields with local semantic rig-like control and a local control loss, as well as an attention mask mechanism.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.09632v2" target="_blank" rel="noopener">HexPlane: A Fast Representation for Dynamic Scenes&lt;/a>&lt;/strong>: HexPlane proposes a new method to represent dynamic 3D scenes explicitly using six planes of learned features. HexPlane computes color features efficiently for voxels by fusing six vectors. Its combination with a small MLP produces impressive results in novel view synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.03184v1" target="_blank" rel="noopener">Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream&lt;/a>&lt;/strong>: Instant-NVR proposes a monocular tracking and rendering system for complex human-object interactions in real-time. The authors use a hybrid deformation module and an online reconstruction strategy for efficient rendering. The system can capture the dynamic and static radiance fields for image synthesis.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14243v1" target="_blank" rel="noopener">DyLiN: Making Light Field Networks Dynamic&lt;/a>&lt;/strong>: DyLiN is proposed to handle non-rigid deformations in dynamic light fields in a computationally efficient manner. The method is based on learning a deformation field and lifting them into a higher dimensional space for handling discontinuities. CoDyLiN is proposed to handle controllable attributes in addition to non-rigid deformations.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11610v2" target="_blank" rel="noopener">Tensor4D: Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering&lt;/a>&lt;/strong>: Tensor4D uses a 4D tensor decomposition model for capturing dynamic 3D scenes from sparse-view camera rigs or even a monocular camera. The tensor is decomposed hierarchically into three time-aware volumes and nine compact feature planes. The proposed tensor factorization scheme allows for structural motion and detailed changes to be learned from coarse to fine.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.10241v2" target="_blank" rel="noopener">K-Planes: Explicit Radiance Fields in Space, Time, and Appearance&lt;/a>&lt;/strong>: K-Planes is a white-box model for radiance fields in arbitrary dimensions that represents a d-dimensional scene using choose-2 planes, making it easy to add dimension-specific priors. Despite using a linear feature decoder, it yields similar performance to a non-linear black-box MLP decoder with low memory usage and fast optimization. The method achieves state-of-the-art reconstruction fidelity whereby one can easily add temporal smoothness and other structure priors.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.06717v1" target="_blank" rel="noopener">Representing Volumetric Videos as Dynamic MLP Maps&lt;/a>&lt;/strong>: The paper proposes a method for real-time view synthesis of dynamic 3D scenes by representing the radiance field of each frame as a set of shallow MLPs stored as &amp;ldquo;MLP maps,&amp;rdquo; and dynamically predicted by a shared 2D CNN decoder. This achieves high rendering quality with state-of-the-art efficiency and speed.&lt;/p>
&lt;h3 id="editable-and-composable">Editable and Composable&lt;/h3>
&lt;p>
&lt;figure id="figure-the-left-image-shows-the-observation-the-central-one-shows-the-optimization-process-of-onsfhttpsarxivorgabs230608748-and-the-right-the-optimized-light-position-highlighted-as-a-green-dot">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s-tian.github.io/assets/actionosf/light_pose_opt.gif" alt="The left image shows the observation, the central one shows the optimization process of &amp;lt;a href=&amp;#34;https://arxiv.org/abs/2306.08748&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;ONSF&amp;lt;/a&amp;gt;, and on the right, the optimized light position is highlighted as a green dot." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The left image shows the observation, the central one shows the optimization process of &lt;a href="https://arxiv.org/abs/2306.08748" target="_blank" rel="noopener">ONSF&lt;/a>, and the right the optimized light position highlighted as a green dot.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>This section covers NeRFs that propose composing, controlling, or editing methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2306.08748v1" target="_blank" rel="noopener">Multi-Object Manipulation via Object-Centric Neural Scattering Functions&lt;/a>&lt;/strong>: Object-centric neural scattering functions (OSFs) are used as object representations to enable compositional scene re-rendering under object rearrangement and varying lighting conditions. This approach leads to improved model-predictive control performance and generalization in compositional multi-object environments.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.03049v1" target="_blank" rel="noopener">NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds&lt;/a>&lt;/strong>: NeuralEditor is a shape-editing algorithm that works on the explicit point cloud representation of NeRF. It employs K-D tree-guided density-adaptive voxels to perform deterministic integration and optimize the neural network. The resulting point cloud is then used to perform shape editing to achieve state-of-the-art results on shape deformation tasks.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.11966v1" target="_blank" rel="noopener">Removing Objects From Neural Radiance Fields&lt;/a>&lt;/strong>: The paper presents a method for inpainting objects in an already generated NeRF using confidence-based view selection. A user-provided mask is used to overwrite data likely to contain the object, and the NeRF is then re-trained with individual 2D images selected by the view selection procedure.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2203.14402v5" target="_blank" rel="noopener">UV Volumes for Real-time Rendering of Editable Free-view Human Performance&lt;/a>&lt;/strong>: The UV Volumes proposes an approach for rendering human performers in real-time for VR/AR applications. It separates the high-frequency appearance from the 3D volume and encodes them into 2D texture stacks, which allows faster computation with shallower neural networks while maintaining editability and generalization. Other applications, like retexturing, are also made possible using this approach.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.06344v3" target="_blank" rel="noopener">DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization&lt;/a>&lt;/strong>: DPLayer is a differentiable parameterization layer to pick a meaningful region of a mesh for low-distortion parameterization. The method uses a neural segmentation network to learn to select the 3D region, which is then parameterized in 2D.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.html" target="_blank" rel="noopener">SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields&lt;/a>&lt;/strong>: The paper proposes a novel 3D inpainting method for removing unwanted objects from a 3D scene. The method leverages learned 2D image inpainters and a 3D segmentation mask to address the challenges of view consistency and geometric validity. Additionally, the authors introduce a dataset comprised of real-world scenes to evaluate the effectiveness of the proposed method.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.10699v2" target="_blank" rel="noopener">PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields&lt;/a>&lt;/strong>: PaletteNeRF provides an efficient and realistic approach to edit the appearance of neural radiance fields. The appearance can be decomposed into palettes shared across the scene and optimized alongside the basis functions. The method allows efficient editing of the appearance through direct modification of the color palettes. Additionally, compressed semantic features can be introduced for semantic-aware appearance editing.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13232v1" target="_blank" rel="noopener">Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization&lt;/a>&lt;/strong>: LipRF uses a Lipschitz mapping to stylize 3D scenes photorealistically. LipRF couples pre-trained NeRF with 2D photorealistic style transfer and learns 3D styles from views.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Occlusion-Free_Scene_Recovery_via_Neural_Radiance_Fields_CVPR_2023_paper.html" target="_blank" rel="noopener">OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields&lt;/a>&lt;/strong>: The proposed method directly maps occlusion-free scene details from position and viewing angles with Neural Radiance Field. The scheme optimizes both camera parameters and scene reconstruction in the presence of occlusions without the need for labeled external data for training.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.10448v1" target="_blank" rel="noopener">ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects&lt;/a>&lt;/strong>: ReNe is a dataset that contains real-world object scenes captured with one-light-at-a-time (OLAT) conditions. The dataset contains complex geometries and challenging materials. The authors perform an ablation study to identify a lightweight architecture capable of rendering objects under novel light conditions and establish a non-trivial baseline for the new dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.04247v2" target="_blank" rel="noopener">EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points&lt;/a>&lt;/strong>: EditableNeRF models dynamic scenes by detecting key points and weights. Then the key points can be dragged and dropped, allowing for editing from single-camera image sequences.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.10598v3" target="_blank" rel="noopener">StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields&lt;/a>&lt;/strong>: StyleRF is a 3D style transfer method that performs style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes for reliable geometry restoration via volume rendering, with deferred style transformation of 2D feature maps that ensure high-quality zero-shot style transfer across a variety of new styles. The proposed method performs sampling-invariant content transformation to ensure multi-view consistency.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.02766v2" target="_blank" rel="noopener">Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization&lt;/a>&lt;/strong>: Ref-NPR uses a reference image as a style source to stylize a 3D scene using radiance fields, with view interpolation and semantic disambiguation fills occlusion areas with visuals consistent with the stylized reference.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13277v2" target="_blank" rel="noopener">SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field&lt;/a>&lt;/strong>: SinE lets users easily edit NeRFs using semantic strokes or text prompts. Techniques like cyclic constraints with a proxy mesh, color compositing, and feature cluster-based regularization are used to stabilize the process. The authors show examples of both real-world and synthetic data that achieve high-quality multi-view consistency.&lt;/p>
&lt;h3 id="pose-estimation">Pose Estimation&lt;/h3>
&lt;figure>
&lt;video src="https://rover-xingyu.github.io/L2G-NeRF/images/girl.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Pose alignment with &lt;a href="https://arxiv.org/abs/2211.11505" target="_blank" rel="noopener">L2G-NeRF&lt;/a> compared to prior work.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>Estimating the pose of objects or the camera is a fundamental problem in computer vision. This can also be done to improve the quality of scenes with noisy camera poses.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11505v3" target="_blank" rel="noopener">Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields&lt;/a>&lt;/strong>: L2G-NeRF is a bundle-adjustment method for finding accurate camera poses for Neural Radiance Fields in novel view synthesis. The method applies a pixel-wise local alignment, followed by a frame-wise global alignment using differentiable parameter estimation solvers. L2G-NeRF improves reconstruction and outperforms the state-of-the-art.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14478v1" target="_blank" rel="noopener">DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields&lt;/a>&lt;/strong>: DBARF is an extension of BARF for GeNeRFs. The geometric cost feature map used in DBARF has been shown to support self-supervised learning, allowing it to be trained across domains, in contrast to state of the art.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.09682v1" target="_blank" rel="noopener">AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training&lt;/a>&lt;/strong>: AlignNerf combines convolutional layers with MLPs in high-resolution NeRF reconstructions and also includes a novel training strategy and a high-frequency aware loss to improve reconstruction quality, performing better than other state-of-the-art NeRF models in high-frequency detail recovery.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.07388v3" target="_blank" rel="noopener">NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior&lt;/a>&lt;/strong>: The authors propose a novel method for training and rendering NeRF from mobile camera videos. The method works better with the addition of monocular depth prior to the relative motion estimation between frames. The authors show that their method has promising results for mobile camera use cases.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11738v3" target="_blank" rel="noopener">SPARF: Neural Radiance Fields from Sparse and Noisy Poses&lt;/a>&lt;/strong>: SPARF is introduced to allow novel view synthesis from a few input views given noisy camera poses. It exploits multi-view geometry constraints to jointly refine camera poses and estimate the NeRF. SPARF sets a new state-of-the-art in the sparse-view regime on multiple datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12853v1" target="_blank" rel="noopener">BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields&lt;/a>&lt;/strong>: The authors present BAD-NeRF, a bundle-adjusted deblur neural radiance field, which models the physical image formation process to jointly learn camera poses and NeRF parameters and is robust to motion blur. They show superior performance over prior works on synthetic and real datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://arxiv.org/abs/2211.12018" target="_blank" rel="noopener">Level-S2fM: Structure from Motion on Neural Level Set of Implicit Surfaces&lt;/a>&lt;/strong>: This paper introduces Level-S2fM, a neural incremental Structure-from-Motion (SfM) approach that uses coordinate MLPs to estimate camera poses and scene geometry from uncalibrated images. It addresses challenges in optimizing volumetric neural rendering with unknown camera poses and demonstrates promising results in camera pose estimation, scene geometry reconstruction, and neural implicit rendering.&lt;/p>
&lt;h3 id="decomposition">Decomposition&lt;/h3>
&lt;figure>
&lt;video src="https://nv-tlabs.github.io/fegr/assets/FEGR_asset_clip.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>&lt;a href="http://arxiv.org/abs/2304.03266v1" target="_blank" rel="noopener">Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes&lt;/a> allows exporting captured urban objects into any graphics engine.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>In this section, the radiance of NeRF is split into geometry, BRDF, and illumination. This enables consistent relighting under any illumination.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.16617v1" target="_blank" rel="noopener">NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination&lt;/a>&lt;/strong>: The authors of this paper present an inverse rendering pipeline that considers near-field indirect illumination and uses path tracing Monte Carlo sampling. They demonstrate state-of-the-art performance in inter-reflection decomposition by introducing radiance consistency constraints between implicit neural radiance and path tracing results.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_I2-SDF_Intrinsic_Indoor_Scene_Reconstruction_and_Editing_via_Raytracing_in_CVPR_2023_paper.html" target="_blank" rel="noopener">I2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs&lt;/a>&lt;/strong>: I^2-SDF is a neural radiance field-based framework that jointly estimates shapes, incident radiance, and materials for indoor scene reconstruction and editing. The neural radiance field is decomposed into a spatially-varying material through differentiable Monte Carlo raytracing that enables photorealistic scene relighting and editing applications.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15101v2" target="_blank" rel="noopener">DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering&lt;/a>&lt;/strong>: DANI-Net is an inverse rendering framework for uncalibrated photometric stereo problems. It incorporates differentiable shadow handling and anisotropic reflectance modeling in its design.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Complementary_Intrinsics_From_Neural_Radiance_Fields_and_CNNs_for_Outdoor_CVPR_2023_paper.html" target="_blank" rel="noopener">Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting&lt;/a>&lt;/strong>: The proposed framework combines NeRF and CNNs for outdoor scene relighting through intrinsic image decomposition. NeRF provides richer and more reliable pseudo-labels for CNNs training to predict interpretable lighting parameters, which enable realistic relighting.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14092v2" target="_blank" rel="noopener">NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images&lt;/a>&lt;/strong>: NeuFace introduces an approximated BRDF integration and a low-rank prior to create accurate and physically-meaningful 3D facial representations using neural rendering techniques. The method incorporates neural BRDFs into physically based rendering, allowing for the capture of complex facial geometry and appearance clues.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.03266v1" target="_blank" rel="noopener">Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes&lt;/a>&lt;/strong>: The presented method jointly reconstructs geometry, materials, and HDR lighting from posed RGB images. An explicit mesh is used to model high-order lighting effects such as shadows. The method disentangles complex geometry and materials from lighting effects for photorealistic relighting and virtual object insertion.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.10206v4" target="_blank" rel="noopener">Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes&lt;/a>&lt;/strong>: TexIR proposes a Texture-based Lighting representation of indoor scenes that models direct and infinite-bounce indirect lighting. A hybrid lighting representation with precomputed irradiance helps with efficiency and material optimization noise. The method enables mixed-reality applications such as material editing, novel view synthesis, and relighting.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.11900v1" target="_blank" rel="noopener">Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting&lt;/a>&lt;/strong>: The paper presents a 3D human reconstruction framework that includes a visibility field in addition to the occupancy field and the albedo field. A discretized visibility is supplied with coupled 3D depth and 2D image features, and a TransferLoss is proposed to improve the alignment between visibility and occupancy fields. The proposed method improves reconstruction accuracy and achieves accurate relighting comparable to ray-traced ground truth.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14190v1" target="_blank" rel="noopener">WildLight: In-the-wild Inverse Rendering with a Flashlight&lt;/a>&lt;/strong>: The authors propose a photometric approach to inverse rendering for unknown ambient lighting. The approach exploits a smartphone&amp;rsquo;s flashlight to produce a minimal light source and decomposes image intensities into a static appearance that corresponds to ambient flux and a dynamic reflection induced by the flashlight.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.12461v1" target="_blank" rel="noopener">TensoIR: Tensorial Inverse Rendering&lt;/a>&lt;/strong>: TensoIR proposes an inverse rendering approach based on tensor factorization and neural fields, allowing for efficient and physically-based model estimation for multi-view images in unknown lighting conditions. This provides photorealistic novel view synthesis and relighting results.&lt;/p>
&lt;h3 id="other">Other&lt;/h3>
&lt;figure>
&lt;video src="https://neural-lens.github.io/assets/opt_timelapse.mp4" autoplay loop>&lt;/video>
&lt;figcaption>
&lt;p>Neural Fields can be used to learn a differentiable and bijective lens with &lt;a href="https://arxiv.org/abs/2304.04848" target="_blank" rel="noopener">Neural Lens Modeling&lt;/a>.&lt;/p>
&lt;figcaption>
&lt;/figure>
&lt;p>Several works with excellent results in various fields.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.12995v2" target="_blank" rel="noopener">Raw Image Reconstruction with Learned Compact Metadata&lt;/a>&lt;/strong>: The paper proposes a framework to learn a compressed latent representation of raw images containing fewer metadata than commonly compressed raw image files. The method features an sRGB-guided context model with improved entropy estimation strategies. The compressed representation enables the allocation of more bits for important regions.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.17603v1" target="_blank" rel="noopener">NeRF-Supervised Deep Stereo&lt;/a>&lt;/strong>: This paper proposes a novel framework for training deep stereo networks without ground-truth by using a combination of Neural Rendering and NeRF-supervised training. The rendered stereo triplets are used to compensate for occlusions and depth maps as proxy labels. The proposed model shows a significant improvement over existing self-supervised methods on the Middlebury dataset.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.16493v1" target="_blank" rel="noopener">AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation&lt;/a>&lt;/strong>: AnyFlow generates optical flow accurately by representing it as a coordinate-based representation. It can accurately estimate flow from images of various resolutions and performs better in detail preservation of tiny objects than previous models when given low-resolution inputs.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.04848v1" target="_blank" rel="noopener">Neural Lens Modeling&lt;/a>&lt;/strong>: NeuroLens is an end-to-end optimization method for image distortion and vignetting, which can be used for point projection and ray casting. It allows performing pre-capture and post-reconstruction calibration, outperforming standard methods and being easy to use.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.00613v2" target="_blank" rel="noopener">NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation&lt;/a>&lt;/strong>: In NeuWigs, two stages are used to model human hair for virtual reality: the first learns a latent space of 3D hair states and the second performs temporal hair transfer. The learned model outperforms the state-of-the-art and can create new hair animations without additional observations.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.04328v1" target="_blank" rel="noopener">Neural Voting Field for Camera-Space 3D Hand Pose Estimation&lt;/a>&lt;/strong>: NVF unifies the two-stage process used traditionally in hand pose estimation, directly predicting 3D dense local evidence and global hand geometry via point-wise voting of 3D points in the camera frustum to alleviate 2D-to-3D ambiguities.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2304.00341v1" target="_blank" rel="noopener">JacobiNeRF: NeRF Shaping with Mutual Information Gradients&lt;/a>&lt;/strong>: JacobiNeRF learns to encode mutual correlation patterns between entities via maximizing their mutual information and is used to improve label propagation for sparse label regimes, semantic, and instance segmentation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Peters_pCON_Polarimetric_Coordinate_Networks_for_Neural_Scene_Representations_CVPR_2023_paper.html" target="_blank" rel="noopener">pCON: Polarimetric Coordinate Networks for Neural Scene Representations&lt;/a>&lt;/strong>: Polarimetric Coordinate Networks (pCON) is an architecture designed to preserve polarimetric information and address artifacts created by coordinate network architecture when reconstructing three polarimetric quantities of interest. The current state-of-the-art models do not consider preserving physical quantities.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2306.07970v1" target="_blank" rel="noopener">Neural Scene Chronology&lt;/a>&lt;/strong>: The proposed scene representation Space-Time Radiance Field (STRF) can model discrete scene-level changes as piece-wise constant temporal step functions. By using this representation, the proposed method can reconstruct a time-varying 3D scene model from internet imagery while separating scene-level and illumination changes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.04531v2" target="_blank" rel="noopener">ORCa: Glossy Objects as Radiance Field Cameras&lt;/a>&lt;/strong>: The proposed method uses glossy objects to recover the 5D environment radiance field visible to them by conversion into radiance-field cameras, which can be used to create noise-free images in real-time and to synthesize novel viewpoints. This method can also image around occluders in a scene while estimating object geometry, radiance, and the radiance field.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.12886v3" target="_blank" rel="noopener">OReX: Object Reconstruction from Planar Cross-sections Using Neural Fields&lt;/a>&lt;/strong>: OReX uses a Neural Field as an interpolation prior to reconstruct 3D shapes from planar cross-sections. The approach involves iterative estimation architecture and a hierarchical input sampling scheme. A regularization scheme is employed to alleviate gradient ripples. OReX outperforms previous methods and scales well with input size.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.18139v2" target="_blank" rel="noopener">Efficient View Synthesis and 3D-based Multi-Frame Denoising with Multiplane Feature Representations&lt;/a>&lt;/strong>: The authors propose a 3D-based multi-frame denoising method that outperforms 2D-based methods with lower computational requirements. The approach extends the multiplane image framework with a learnable encoder-renderer pair manipulating multiplane representations in feature space for better performance.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2211.11646v3" target="_blank" rel="noopener">NeRF-RPN: A general framework for object detection in NeRFs&lt;/a>&lt;/strong>: NeRF-RPN is an object detection framework that directly operates on NeRF to detect objects in 3D. It exploits a novel voxel representation and multi-scale 3D neural volumetric features to regress the 3D bounding boxes of objects without rendering the NeRF at any viewpoint. A benchmark dataset with both synthetic and real-world data is also provided.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.01838v2" target="_blank" rel="noopener">vMAP: Vectorised Object Mapping for Neural Field SLAM&lt;/a>&lt;/strong>: vMAP is a dense SLAM system using MLPs to represent objects, enabling efficient, incrementally-built models without 3D priors. The authors show this approach to be more efficient and effective than previous neural field SLAM systems.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2302.08504v1" target="_blank" rel="noopener">PersonNeRF: Personalized Reconstruction from Photo Collections&lt;/a>&lt;/strong>: PersonNeRF is a method that builds a customized neural volumetric 3D model from a collection of photos of a subject captured across multiple years, enables the rendering of the subject with arbitrary combinations of viewpoint, body pose, and appearance. It addresses the issue of sparse observations by recovering a canonical T-pose neural volumetric representation of the subject that allows for changing appearance across different observations but uses a shared pose-dependent motion field across all observations.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Worchel_Differentiable_Shadow_Mapping_for_Efficient_Inverse_Graphics_CVPR_2023_paper.html" target="_blank" rel="noopener">Differentiable Shadow Mapping for Efficient Inverse Graphics&lt;/a>&lt;/strong>: The authors show that pre-filtered shadow mapping can be combined with existing differentiable rasterizers to allow efficient shadow computation for inverse graphics problems. Such a technique has faster convergence than differentiable light transport simulation and allows for better results in implicit 3D reconstruction.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chang_Depth_Estimation_From_Indoor_Panoramas_With_Neural_Scene_Representation_CVPR_2023_paper.html" target="_blank" rel="noopener">Depth Estimation from Indoor Panoramas with Neural Scene Representation&lt;/a>&lt;/strong>: The proposed method for depth estimation from multi-view indoor panoramic images with the Neural Radiance Field technology outperforms previous works by a large margin in quantitative and qualitative evaluations. Two networks were developed to learn the Signed Distance Function for depth measurement and the Radiance Field from panoramas, respectively, as well as a novel spherical position embedding scheme. A geometric consistency loss leveraging surface normal further refines depth estimation.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2305.19590v2" target="_blank" rel="noopener">Neural Kernel Surface Reconstruction&lt;/a>&lt;/strong>: This paper introduces enhancement over the Neural Kernel Field (NKF) method. The new method, SparseVox, uses compactly supported kernel functions, making it robust to noise, trainable with any dataset of dense oriented points, and capable of reconstructing millions of points in a few seconds. SparseVox also outperforms NKF for reconstructing scenes, objects, and outdoor environments.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.00304v4" target="_blank" rel="noopener">Renderable Neural Radiance Map for Visual Navigation&lt;/a>&lt;/strong>: The Renderable Neural Radiance Map (RNR-Map) is a grid structure that stores latent codes and extracts the radiance field of images using the grid positions. This structure serves as a visual guideline for efficient navigation and localization. It provides data for camera tracking, visual localization, and curved scenarios navigation that are fast and robust to environmental changes and actuation noises.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.08556v1" target="_blank" rel="noopener">NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis&lt;/a>&lt;/strong>: SPARTN uses NeRFs to synthetically inject corrective noise into visual robotic manipulation policies, eliminating the need for expert supervision or additional interaction. It improves success rates by 2.8x over imitation learning without augmentation and even outperforms some methods that use online supervision.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.04823v2" target="_blank" rel="noopener">GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields&lt;/a>&lt;/strong>: GazeNeRF is a 3D-aware method for gaze redirection that models the face and eye volumes separately and projects them onto a 2D image by finetuning using a 3D rotation matrix.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.14158v1" target="_blank" rel="noopener">BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects&lt;/a>&lt;/strong>: Neural Object Field (NOF) combines 6-DoF tracking and 3D reconstruction for arbitrary objects, with learning and optimization done at the same time. It produces a high-fidelity reconstruction of objects even without visual textures and performs well in sequences with large pose changes.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2301.08730v2" target="_blank" rel="noopener">Novel-view Acoustic Synthesis&lt;/a>&lt;/strong>: ViGAS learns to synthesize the sound of an arbitrary point in 3D space from audio-visual input. The authors introduce a novel-view acoustic synthesis task, which is addressed by learning to reason about the spatial acoustics cues. To enable this work, two large-scale multi-view datasets have been collected: one synthetic and one real.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.07653v2" target="_blank" rel="noopener">NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images&lt;/a>&lt;/strong>: NEF is a learned implicit curve representation based on neural networks. It is optimized with view-based rendering loss and is able to output 3D feature curves without relying on 3D geometric operators or cross-view correspondence. On a synthetic benchmark, NEF outperforms existing methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.15126v1" target="_blank" rel="noopener">NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation&lt;/a>&lt;/strong>: NeuralPCI introduces Neural Field Interpolation, an end-to-end 4D spatio-temporal neural field for 3D point cloud interpolation, extrapolation, morphing, and auto-labeling to handle both indoor and outdoor scenarios&amp;rsquo; large non-linear motions. It achieves state-of-the-art performance on the DHB (Dynamic Human Bodies) and NL-Drive datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.02493v3" target="_blank" rel="noopener">Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields&lt;/a>&lt;/strong>: CaFi-Net can produce a canonical coordinate-based implicit representation of an object category without the need for pre-aligned datasets, using a Siamese network architecture for category-level canonicalization. It extracts features from radiance fields and estimates canonical fields with consistent 3D pose. The method is tested on a dataset of 1300 NeRF models across 13 object categories and compared to point cloud-based methods.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2303.13014v1" target="_blank" rel="noopener">Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention&lt;/a>&lt;/strong>: The proposed S-Ray model combines semantic understanding of radiance and cross-view attention mechanisms to learn from multiple scenes efficiently while providing generalizable results.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2206.11896v3" target="_blank" rel="noopener">EventNeRF: Neural Radiance Fields from a Single Colour Event Camera&lt;/a>&lt;/strong>: The authors present an approach to dense and photorealistic view synthesis from a single color event stream. This is done with a neural radiance field trained in a self-supervised way using a tailored ray sampling strategy. The resulting method produces significantly denser and more visually appealing renderings than existing methods while being robust in challenging scenarios.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://arxiv.org/abs/2212.09802v1" target="_blank" rel="noopener">Panoptic Lifting for 3D Scene Understanding with Neural Fields&lt;/a>&lt;/strong>: Panoptic Lifting presents a new method to learn 3D panoptic representations of in-the-wild scenes using a neural field representation trained on machine-generated 2D panoptic segmentation masks. The method accounts for inconsistencies in 2D instance identifiers and incorporates improvements to make it more robust to noisy labels. The approach shows improvement in scene-level PQ over state-of-the-art methods on several datasets.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://openaccess.thecvf.com/content/CVPR2023/html/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.html" target="_blank" rel="noopener">Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving&lt;/a>&lt;/strong>: ImplicitO reports an implicit representation of the occupancy of the street with a flow factor. This approach reduces computational cost as the motion planner can directly query it and avoids preoccupying the infrastructure with unnecessary computational work. The model also uses a global attention mechanism.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>That was many papers, and compared to the previous &lt;a href="https://dellaert.github.io/NeRF22/" target="_blank" rel="noopener">CVPR&lt;/a> (57 papers), the paper count has now grown to 175. CVPR 23 alone had more papers than NeurIPS, ECCV, and CVPR 2022 combined (140).&lt;/p>
&lt;p>I have the feeling my little ARCHIVE tool will come in handy this year.&lt;/p>
&lt;p>Apart from the sheer number of papers, there are prominent trends in the papers. To no one&amp;rsquo;s surprise, leveraging NeRFs for anything generative is a substantial new area, and it shows that NeRF and its extension are a mighty tool for this task. I also noticed many papers in the decomposition area - very close to my heart. This is great, and here also, NeRFs mainly play the role of a tool in the reconstruction process. There are also several papers in the SLAM field. So while Frank Dellaert originally termed it as the &lt;a href="https://dellaert.github.io/NeRF/" target="_blank" rel="noopener">NeRF Explosion&lt;/a>, we may now enter the time of the NeRFusion, where NeRF becomes a building block in many different areas.&lt;/p>
&lt;p>Especially with the current speed of research and new trendy fields constantly emerging, it is exciting to see where NeRF will head next :).&lt;/p></description></item><item><title>Neural Reflectance Decomposition</title><link>https://markboss.me/publication/phdthesis/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/phdthesis/</guid><description/></item><item><title>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</title><link>https://markboss.me/publication/2022-samurai/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2022-samurai/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/LlYuGDjXp-8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Our previous methods such as &lt;a href="https://markboss.me/publication/2021-nerd/">NeRD&lt;/a> and &lt;a href="https://markboss.me/publication/2021-neural-pil/">Neural-PIL&lt;/a> achieve the decomposition of images under varying illumination into shape, BRDF, and illumination. However, both methods require near-perfect known poses. In challenging scenes recovering poses is challenging and traditional methods fail with objects captured under varying illuminations and locations.&lt;/p>
&lt;h2 id="method">Method&lt;/h2>
&lt;figure id="figure-the-samurai-architecture">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The SAMURAI architecture." srcset="
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_f0fbbf61d7097a49e26b7975fffcfb94.webp 400w,
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_9e4d72244c945061262711b0f3a8d3cd.webp 760w,
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_f0fbbf61d7097a49e26b7975fffcfb94.webp"
width="760"
height="248"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
The SAMURAI architecture.
&lt;/figcaption>&lt;/figure>
&lt;p>In &lt;a href="#figure-the-samurai-architecture">&lt;strong>FIGURE 1&lt;/strong>&lt;/a> we visualize the SAMURAI architecture, which jointly optimizes the camera extrinsic and intrinsic parameters per image, the global shape and BRDF, as well as the per-image illumination latent variables. Here, we leverage &lt;a href="https://markboss.me/publication/2021-neural-pil/">Neural-PIL&lt;/a> for the rendering and prior on natural illuminations.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="samurai-dataset">SAMURAI-Dataset&lt;/h4>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=duck">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/duck.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=fireengine">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/fireengine.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=garbagetruck">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/garbagetruck.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=keywest">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/keywest.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=pumpkin">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/pumpkin.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=rccar">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/rccar.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=robot">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/robot.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>An open-source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening</title><link>https://markboss.me/publication/2022-tlcyzer/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2022-tlcyzer/</guid><description>&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</title><link>https://markboss.me/publication/2021-neural-pil/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2021-neural-pil/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/p5cKaNwVp4M" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Besides the general &lt;a href="https://dellaert.github.io/NeRF/" target="_blank" rel="noopener">NeRF Explosion&lt;/a> of 2020, a subfield of introducing explicit material representations in to neural volume representation emerged with papers such as &lt;a href="https://markboss.me/publication/2021-nerd/">NeRD&lt;/a>, &lt;a href="https://pratulsrinivasan.github.io/nerv/" target="_blank" rel="noopener">NeRV&lt;/a>, &lt;a href="https://arxiv.org/abs/2008.03824" target="_blank" rel="noopener">Neural Reflectance Fields for Appearance Acquisition&lt;/a>, &lt;a href="https://kai-46.github.io/PhySG-website/" target="_blank" rel="noopener">PhySG&lt;/a> or &lt;a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/" target="_blank" rel="noopener">NeRFactor&lt;/a>. The way illumination is represented varies drastically between the methods. Either the methods focus on single-point lights such as in &lt;a href="https://arxiv.org/abs/2008.03824" target="_blank" rel="noopener">Neural Reflectance Fields for Appearance Acquisition&lt;/a>, it is assumed to be known (NeRV), it is extracted from a trained NeRF as an illumination map (NeRFactor), or it is represented as Spherical Gaussians (NeRD and PhySG). It is also worth pointing out that nearly all methods focus on a single illumination per scene, except NeRD.&lt;/p>
&lt;p>While NeRD enabled decomposition from multiple views under different illumination, SGs only allowed for rather diffuse illuminations. Inspired from &lt;a href="https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf" target="_blank" rel="noopener">Pre-integrated Lighting&lt;/a> from real-time rendering, we transfer this concept to a neural network, which handles the integration and can represent illuminations from a manifold of natural illuminations.&lt;/p>
&lt;h2 id="method">Method&lt;/h2>
&lt;figure id="figure-the-neural-pil-architecture">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The Neural-PIL architecture." srcset="
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_f65ed7f0eae768f8d2fe04858fedb038.webp 400w,
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_c8d406d520e0d639062f4ba09d83e4b1.webp 760w,
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_f65ed7f0eae768f8d2fe04858fedb038.webp"
width="375"
height="253"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
The Neural-PIL architecture.
&lt;/figcaption>&lt;/figure>
&lt;p>In &lt;a href="#figure-the-neural-pil-architecture">&lt;strong>FIGURE 1&lt;/strong>&lt;/a> visualizes the Neural-PIL architecture, which is inspired by &lt;a href="https://marcoamonteiro.github.io/pi-GAN-website/" target="_blank" rel="noopener">pi-GAN&lt;/a>. As seen, the mapping networks are used on the embedding $z^l$, which describes the general content of the environment map and the roughness $b_r$, which defines how rough and therefore how blurry the environment should be.&lt;/p>
&lt;figure id="figure-pre-integrated-lighting-visualzed">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Pre-integrated Lighting visualzed." srcset="
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_9b482253ec45a6df9381bef693190cf0.webp 400w,
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_71abc23863cc4b37e7472a0d5c76b9fa.webp 760w,
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_9b482253ec45a6df9381bef693190cf0.webp"
width="760"
height="108"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Pre-integrated Lighting visualzed.
&lt;/figcaption>&lt;/figure>
&lt;p>Visually this can be seen in &lt;a href="#figure-pre-integrated-lighting-visualzed">&lt;strong>FIGURE 2&lt;/strong>&lt;/a>. If the BRDF, shown on the left for each pair, becomes rougher, the illuminations from a larger area get integrated. The result is a blurrier environment map.&lt;/p>
&lt;p>As a joint decomposition of illumination, shape, and appearance is a challenging, ill-posed task, we introduce priors to the BRDF and illumination. The illumination should only lie on a smooth manifold of natural illumination and the BRDF on possible materials. Here, we introduce a Smooth Manifold Auto-Encoder (SMAE).&lt;/p>
&lt;figure id="figure-smooth-manifold-auto-encoder">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Smooth-Manifold-Auto-Encoder." srcset="
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_712e77003ea0bdb06796212f28cc6030.webp 400w,
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_ee450154c756a0c55d12b6eab27b418a.webp 760w,
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_712e77003ea0bdb06796212f28cc6030.webp"
width="408"
height="249"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Smooth-Manifold-Auto-Encoder.
&lt;/figcaption>&lt;/figure>
&lt;p>Inspired by Berthelot et al. - &lt;a href="https://arxiv.org/abs/1807.07543" target="_blank" rel="noopener">Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer&lt;/a>, we introduce the interpolation in latent space during training, we further introduce three additional losses which further aid in a smooth manifold formation. The smoothness loss encourages a smooth gradient w.r.t. to the interpolation factor and therefore achieves a smooth interpolation between two points in the latent space. The cyclic loss enforces that the encoder and decoder perform the same step by re-encoding the decoded interpolated embeddings and ensuring the re-encoded latent vectors are the same as the initial ones. Lastly, we add a discriminator trained on the examples from the dataset as real ones and the interpolated ones as fake and try to fool it with our interpolated embeddings. With these three losses, a smooth latent space is formed, which allows for an easy introduction in our framework, where the corresponding networks are frozen, and only the latent space is optimized.&lt;/p>
&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>NeRD: Neural Reflectance Decomposition from Image Collections</title><link>https://markboss.me/publication/2021-nerd/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2021-nerd/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/IM9OgMwHNTI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>NeRD is a novel method that can decompose image collections from multiple views taken under varying or fixed illumination conditions. The object can be rotated, or the camera can turn around the object. The result is a neural volume with an explicit representation of the appearance and illumination in the form of the BRDF and Spherical Gaussian (SG) environment illumination.&lt;/p>
&lt;p>The method is based on the general structure of &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">NeRF&lt;/a>. However, NeRF encodes the scene to an implicit BRDF representation where a Multi-Layer-Perceptron (MLP) is queried for outgoing view directions at every point. Extracting information from NeRF is therefore not easily done, and the inference time for novel views takes around 30 seconds. Also, NeRF is not capable of relighting an object under any illumination. By introducing physically-based representations for lighting and appearance, NeRD can relighting an object, and information can be extracted from the neural volume. After our extraction process, the result is a regular texture mesh that can be rendered in real-time. See our &lt;a href="#results">results&lt;/a> where we provide a web-based interactive renderer.&lt;/p>
&lt;h3 id="method">Method&lt;/h3>
&lt;p>Decomposing the scene requires that the integral over the hemisphere from the rendering equation is decomposed into its parts. Here, we use a simplified version without self-emittance.
$$L_o(x,\omega_o) = \int_\Omega L_i(x,\omega_i) f_r(x,\omega_i,\omega_o) (\omega_i \cdot n) d\omega_i$$
Here, $L_o$ is the outgoing radiance for a point $x$ in the direction $\omega_o$. This radiance is calculated by integrating all influences over the hemisphere $\Omega$, which are based on the incoming light $L_i$ for each direction $\omega_i$. The surface behavior is expressed as the BRDF $f_r$, which describes how incoming light $\omega_i$ is directed to the outgoing direction $\omega_o$. Lastly, a cosine term is used $(\omega_i \cdot n)$, which reduces the received light based on the angle towards the light source.&lt;/p>
&lt;p>The inverse of this integral is highly ambiguous, and we use several approximations to solve it. We do not use any interreflections or shadowing, which means that we do not compute the incoming radiance recursively. Additionally, our illumination is expressed as SG, which reduces a full continuous integral to - in our case - 24 evaluation of the environment SGs.&lt;/p>
&lt;!-- Our image formation is now expressed as:
$$L_o(x,\omega_o) \approx \sum^{24}_{m=1} \rho_d(\omega_o,\Gamma,x) + \rho_s(\omega_o,\Gamma,x)$$
Where $\Gamma$ defines the parameters for the SGs, $\rho_d$ defines the evaluation for the diffuse part of the BRDF, and $\rho_s$ for the specular component. -->
&lt;figure id="figure-steps-of-the-query-process-in-a-the-volume-is-constructed-by-tracing-rays-from-each-camera-into-the-scene-then-samples-are-placed-along-with-the-rays-in-b-and-based-on-the-density-at-each-sampling-point-additional-samples-are-placed-in-c-the-samples-are-then-evaluated-into-a-brdf-which-is-re-rendered-using-the-jointly-optimized-illumination-in-d">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d)." srcset="
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1bbd1c3ac16b86cda9f44a5e77471f36.webp 400w,
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_49a191e91129f1c86af0cceaab5489f7.webp 760w,
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1bbd1c3ac16b86cda9f44a5e77471f36.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d).
&lt;/figcaption>&lt;/figure>
&lt;p>Inspired by NeRF the method uses two MLPs, which encode the each position in the volume $\textbf{x} = (x,y,z)$ to a volume density $\sigma$ and a color or BRDF parameters. &lt;a href="#figure-steps-of-the-query-process-in-a-the-volume-is-consturcted-by-tracing-rays-from-each-camera-into-the-scene-then-samples-are-placed-along-the-rays-in-b-and-based-on-the-density-at-each-sampling-point-additional-samples-are-placed-in-c-the-samples-are-then-evaluated-into-a-brdf-which-is-re-rendered-using-the-jointly-optimized-illumination-in-d">Figure 1&lt;/a> shows an overview of this optimization process.&lt;/p>
&lt;!-- Similar to NeRF, two networks are trained in conjunction. A training and inference step consists of first creating a rough sampling pattern using our *sampling network*, which learns the object's rough shape. Samp -->
&lt;!--
&lt;figure id="figure-overview-of-the-sampling-network">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Overview of the sampling network." srcset="
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_35d1538a7833f8bc008f37225f81cbca.webp 400w,
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_58e3423051f5c144464c778a5662a18d.webp 760w,
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_35d1538a7833f8bc008f37225f81cbca.webp"
width="712"
height="340"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Overview of the sampling network.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-overview-of-the-decomposition-network">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Overview of the decomposition network." srcset="
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_23296c69e2599015e5425e33d6db0bfb.webp 400w,
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_a1c6d74b033f6ca373eefbf110124f8a.webp 760w,
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_23296c69e2599015e5425e33d6db0bfb.webp"
width="760"
height="347"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Overview of the decomposition network.
&lt;/figcaption>&lt;/figure>
-->
&lt;!--
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CyC6PutoJO8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
-->
&lt;h3 id="results">Results&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="real-world">Real-world&lt;/h4>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=gnome">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/gnome/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=goldcape">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/goldcape/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h4 id="real-world-preliminary-in-the-wild">Real-world (Preliminary in the wild)&lt;/h4>
&lt;p>The images from the Statue of Liberty are collected from Flickr, Unsplash, Youtube and Vimeo videos. In total about 120 images are used for training from various phones, cameras and drones. Overall even the COLMAP registration is not perfect due to the simplistic shared camera model.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=statue">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/statue/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h5 id="synthetic-examples">Synthetic Examples&lt;/h5>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=car">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/car/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=chair">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/chair/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=globe">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/globe/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h3 id="copyright">Copyright&lt;/h3>
&lt;p>This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to &lt;a href="mailto:pubs-permissions@ieee.org">pubs-permissions@ieee.org&lt;/a>.&lt;/p></description></item><item><title>Two-shot Spatially-varying BRDF and Shape Estimation</title><link>https://markboss.me/publication/cvpr20-two-shot-brdf/</link><pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/cvpr20-two-shot-brdf/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CyC6PutoJO8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="results">Results&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="aksoy-et-al---a-dataset-of-flash-and-ambient-illumination-pairs-from-the-crowd">Aksoy et al. - A Dataset of Flash and Ambient Illumination Pairs from the Crowd&lt;/h4>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf0">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf0/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf1">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf1/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf2">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf2/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf3">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf3/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf4">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf4/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf5">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf5/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf6">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf6/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf7">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf7/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf8">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf8/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf9">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf9/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf10">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf10/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf11">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf11/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf12">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf12/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf13">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf13/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf14">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf14/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf15">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf15/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h5 id="real-world-examples">Real-world Examples&lt;/h5>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw0">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/rw0/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw1">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/rw1/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw2">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/rw2/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw3">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/rw3/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h5 id="synthetic-examples">Synthetic Examples&lt;/h5>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn0">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/syn0/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn1">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/syn1/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn2">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/syn2/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn3">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/syn3/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h3 id="copyright">Copyright&lt;/h3>
&lt;p>This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to &lt;a href="mailto:pubs-permissions@ieee.org">pubs-permissions@ieee.org&lt;/a>.&lt;/p></description></item><item><title>Single Image BRDF Parameter Estimation with a Conditional Adversarial Network</title><link>https://markboss.me/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/</guid><description/></item><item><title>Deep Dual Loss BRDF Parameter Estimation</title><link>https://markboss.me/publication/deep-dual-loss-brdf-parameter-estimation/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/deep-dual-loss-brdf-parameter-estimation/</guid><description/></item></channel></rss>