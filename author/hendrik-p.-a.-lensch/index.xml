<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hendrik P. A. Lensch | Mark Boss</title><link>https://markboss.me/author/hendrik-p.-a.-lensch/</link><atom:link href="https://markboss.me/author/hendrik-p.-a.-lensch/index.xml" rel="self" type="application/rss+xml"/><description>Hendrik P. A. Lensch</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 28 Sep 2022 00:00:00 +0000</lastBuildDate><image><url>https://markboss.me/media/icon_hube1743d0a4940c76c300ec8349475861_6659_512x512_fill_lanczos_center_3.png</url><title>Hendrik P. A. Lensch</title><link>https://markboss.me/author/hendrik-p.-a.-lensch/</link></image><item><title>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</title><link>https://markboss.me/publication/2022-samurai/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2022-samurai/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/LlYuGDjXp-8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Our previous methods such as &lt;a href="https://markboss.me/publication/2021-nerd/">NeRD&lt;/a> and &lt;a href="https://markboss.me/publication/2021-neural-pil/">Neural-PIL&lt;/a> achieve the decomposition of images under varying illumination into shape, BRDF, and illumination. However, both methods require near-perfect known poses. In challenging scenes recovering poses is challenging and traditional methods fail with objects captured under varying illuminations and locations.&lt;/p>
&lt;h2 id="method">Method&lt;/h2>
&lt;figure id="figure-the-samurai-architecture">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The SAMURAI architecture." srcset="
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_f0fbbf61d7097a49e26b7975fffcfb94.webp 400w,
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_9e4d72244c945061262711b0f3a8d3cd.webp 760w,
/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2022-samurai/SAMURAI_hu39fb25fb710281ded4fb528a11595723_111404_f0fbbf61d7097a49e26b7975fffcfb94.webp"
width="760"
height="248"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
The SAMURAI architecture.
&lt;/figcaption>&lt;/figure>
&lt;p>In &lt;a href="#figure-the-samurai-architecture">&lt;strong>FIGURE 1&lt;/strong>&lt;/a> we visualize the SAMURAI architecture, which jointly optimizes the camera extrinsic and intrinsic parameters per image, the global shape and BRDF, as well as the per-image illumination latent variables. Here, we leverage &lt;a href="https://markboss.me/publication/2021-neural-pil/">Neural-PIL&lt;/a> for the rendering and prior on natural illuminations.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="samurai-dataset">SAMURAI-Dataset&lt;/h4>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=duck">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/duck.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=fireengine">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/fireengine.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=garbagetruck">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/garbagetruck.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=keywest">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/keywest.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=pumpkin">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/pumpkin.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=rccar">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/rccar.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/samurai-results/render.html?scene=robot">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/samurai-results/assets/models/robot.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>An open-source smartphone app for the quantitative evaluation of thin-layer chromatographic analyses in medicine quality screening</title><link>https://markboss.me/publication/2022-tlcyzer/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2022-tlcyzer/</guid><description>&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</title><link>https://markboss.me/publication/2021-neural-pil/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2021-neural-pil/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/p5cKaNwVp4M" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Besides the general &lt;a href="https://dellaert.github.io/NeRF/" target="_blank" rel="noopener">NeRF Explosion&lt;/a> of 2020, a subfield of introducing explicit material representations in to neural volume representation emerged with papers such as &lt;a href="https://markboss.me/publication/2021-nerd/">NeRD&lt;/a>, &lt;a href="https://pratulsrinivasan.github.io/nerv/" target="_blank" rel="noopener">NeRV&lt;/a>, &lt;a href="https://arxiv.org/abs/2008.03824" target="_blank" rel="noopener">Neural Reflectance Fields for Appearance Acquisition&lt;/a>, &lt;a href="https://kai-46.github.io/PhySG-website/" target="_blank" rel="noopener">PhySG&lt;/a> or &lt;a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/" target="_blank" rel="noopener">NeRFactor&lt;/a>. The way illumination is represented varies drastically between the methods. Either the methods focus on single-point lights such as in &lt;a href="https://arxiv.org/abs/2008.03824" target="_blank" rel="noopener">Neural Reflectance Fields for Appearance Acquisition&lt;/a>, it is assumed to be known (NeRV), it is extracted from a trained NeRF as an illumination map (NeRFactor), or it is represented as Spherical Gaussians (NeRD and PhySG). It is also worth pointing out that nearly all methods focus on a single illumination per scene, except NeRD.&lt;/p>
&lt;p>While NeRD enabled decomposition from multiple views under different illumination, SGs only allowed for rather diffuse illuminations. Inspired from &lt;a href="https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf" target="_blank" rel="noopener">Pre-integrated Lighting&lt;/a> from real-time rendering, we transfer this concept to a neural network, which handles the integration and can represent illuminations from a manifold of natural illuminations.&lt;/p>
&lt;h2 id="method">Method&lt;/h2>
&lt;figure id="figure-the-neural-pil-architecture">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="The Neural-PIL architecture." srcset="
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_f65ed7f0eae768f8d2fe04858fedb038.webp 400w,
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_c8d406d520e0d639062f4ba09d83e4b1.webp 760w,
/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/NeuralPIL_hu4172eccdb630b879972ccb5c83230ade_17271_f65ed7f0eae768f8d2fe04858fedb038.webp"
width="375"
height="253"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
The Neural-PIL architecture.
&lt;/figcaption>&lt;/figure>
&lt;p>In &lt;a href="#figure-the-neural-pil-architecture">&lt;strong>FIGURE 1&lt;/strong>&lt;/a> visualizes the Neural-PIL architecture, which is inspired by &lt;a href="https://marcoamonteiro.github.io/pi-GAN-website/" target="_blank" rel="noopener">pi-GAN&lt;/a>. As seen, the mapping networks are used on the embedding $z^l$, which describes the general content of the environment map and the roughness $b_r$, which defines how rough and therefore how blurry the environment should be.&lt;/p>
&lt;figure id="figure-pre-integrated-lighting-visualzed">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Pre-integrated Lighting visualzed." srcset="
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_9b482253ec45a6df9381bef693190cf0.webp 400w,
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_71abc23863cc4b37e7472a0d5c76b9fa.webp 760w,
/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/PreintegratedIllumViz_hu8b613da489dbe18525fd7f969d376644_20971_9b482253ec45a6df9381bef693190cf0.webp"
width="760"
height="108"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Pre-integrated Lighting visualzed.
&lt;/figcaption>&lt;/figure>
&lt;p>Visually this can be seen in &lt;a href="#figure-pre-integrated-lighting-visualzed">&lt;strong>FIGURE 2&lt;/strong>&lt;/a>. If the BRDF, shown on the left for each pair, becomes rougher, the illuminations from a larger area get integrated. The result is a blurrier environment map.&lt;/p>
&lt;p>As a joint decomposition of illumination, shape, and appearance is a challenging, ill-posed task, we introduce priors to the BRDF and illumination. The illumination should only lie on a smooth manifold of natural illumination and the BRDF on possible materials. Here, we introduce a Smooth Manifold Auto-Encoder (SMAE).&lt;/p>
&lt;figure id="figure-smooth-manifold-auto-encoder">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Smooth-Manifold-Auto-Encoder." srcset="
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_712e77003ea0bdb06796212f28cc6030.webp 400w,
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_ee450154c756a0c55d12b6eab27b418a.webp 760w,
/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-neural-pil/SMAE_hu6144a3c158b55ef5327a5cd6fa3058f9_21047_712e77003ea0bdb06796212f28cc6030.webp"
width="408"
height="249"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Smooth-Manifold-Auto-Encoder.
&lt;/figcaption>&lt;/figure>
&lt;p>Inspired by Berthelot et al. - &lt;a href="https://arxiv.org/abs/1807.07543" target="_blank" rel="noopener">Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer&lt;/a>, we introduce the interpolation in latent space during training, we further introduce three additional losses which further aid in a smooth manifold formation. The smoothness loss encourages a smooth gradient w.r.t. to the interpolation factor and therefore achieves a smooth interpolation between two points in the latent space. The cyclic loss enforces that the encoder and decoder perform the same step by re-encoding the decoded interpolated embeddings and ensuring the re-encoded latent vectors are the same as the initial ones. Lastly, we add a discriminator trained on the examples from the dataset as real ones and the interpolated ones as fake and try to fool it with our interpolated embeddings. With these three losses, a smooth latent space is formed, which allows for an easy introduction in our framework, where the corresponding networks are frozen, and only the latent space is optimized.&lt;/p>
&lt;!-- ### Copyright
This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to [pubs-permissions@ieee.org](mailto:pubs-permissions@ieee.org). --></description></item><item><title>NeRD: Neural Reflectance Decomposition from Image Collections</title><link>https://markboss.me/publication/2021-nerd/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/2021-nerd/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/IM9OgMwHNTI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>NeRD is a novel method that can decompose image collections from multiple views taken under varying or fixed illumination conditions. The object can be rotated, or the camera can turn around the object. The result is a neural volume with an explicit representation of the appearance and illumination in the form of the BRDF and Spherical Gaussian (SG) environment illumination.&lt;/p>
&lt;p>The method is based on the general structure of &lt;a href="https://www.matthewtancik.com/nerf" target="_blank" rel="noopener">NeRF&lt;/a>. However, NeRF encodes the scene to an implicit BRDF representation where a Multi-Layer-Perceptron (MLP) is queried for outgoing view directions at every point. Extracting information from NeRF is therefore not easily done, and the inference time for novel views takes around 30 seconds. Also, NeRF is not capable of relighting an object under any illumination. By introducing physically-based representations for lighting and appearance, NeRD can relighting an object, and information can be extracted from the neural volume. After our extraction process, the result is a regular texture mesh that can be rendered in real-time. See our &lt;a href="#results">results&lt;/a> where we provide a web-based interactive renderer.&lt;/p>
&lt;h3 id="method">Method&lt;/h3>
&lt;p>Decomposing the scene requires that the integral over the hemisphere from the rendering equation is decomposed into its parts. Here, we use a simplified version without self-emittance.
$$L_o(x,\omega_o) = \int_\Omega L_i(x,\omega_i) f_r(x,\omega_i,\omega_o) (\omega_i \cdot n) d\omega_i$$
Here, $L_o$ is the outgoing radiance for a point $x$ in the direction $\omega_o$. This radiance is calculated by integrating all influences over the hemisphere $\Omega$, which are based on the incoming light $L_i$ for each direction $\omega_i$. The surface behavior is expressed as the BRDF $f_r$, which describes how incoming light $\omega_i$ is directed to the outgoing direction $\omega_o$. Lastly, a cosine term is used $(\omega_i \cdot n)$, which reduces the received light based on the angle towards the light source.&lt;/p>
&lt;p>The inverse of this integral is highly ambiguous, and we use several approximations to solve it. We do not use any interreflections or shadowing, which means that we do not compute the incoming radiance recursively. Additionally, our illumination is expressed as SG, which reduces a full continuous integral to - in our case - 24 evaluation of the environment SGs.&lt;/p>
&lt;!-- Our image formation is now expressed as:
$$L_o(x,\omega_o) \approx \sum^{24}_{m=1} \rho_d(\omega_o,\Gamma,x) + \rho_s(\omega_o,\Gamma,x)$$
Where $\Gamma$ defines the parameters for the SGs, $\rho_d$ defines the evaluation for the diffuse part of the BRDF, and $\rho_s$ for the specular component. -->
&lt;figure id="figure-steps-of-the-query-process-in-a-the-volume-is-constructed-by-tracing-rays-from-each-camera-into-the-scene-then-samples-are-placed-along-with-the-rays-in-b-and-based-on-the-density-at-each-sampling-point-additional-samples-are-placed-in-c-the-samples-are-then-evaluated-into-a-brdf-which-is-re-rendered-using-the-jointly-optimized-illumination-in-d">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d)." srcset="
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1bbd1c3ac16b86cda9f44a5e77471f36.webp 400w,
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_49a191e91129f1c86af0cceaab5489f7.webp 760w,
/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/methodoverview_hu30910665284931ace4f57faa1e01d828_73318_1bbd1c3ac16b86cda9f44a5e77471f36.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Steps of the query process. In a) the volume is constructed by tracing rays from each camera into the scene. Then samples are placed along with the rays in b), and based on the density at each sampling point, additional samples are placed in c). The samples are then evaluated into a BRDF, which is re-rendered using the jointly optimized illumination in d).
&lt;/figcaption>&lt;/figure>
&lt;p>Inspired by NeRF the method uses two MLPs, which encode the each position in the volume $\textbf{x} = (x,y,z)$ to a volume density $\sigma$ and a color or BRDF parameters. &lt;a href="#figure-steps-of-the-query-process-in-a-the-volume-is-consturcted-by-tracing-rays-from-each-camera-into-the-scene-then-samples-are-placed-along-the-rays-in-b-and-based-on-the-density-at-each-sampling-point-additional-samples-are-placed-in-c-the-samples-are-then-evaluated-into-a-brdf-which-is-re-rendered-using-the-jointly-optimized-illumination-in-d">Figure 1&lt;/a> shows an overview of this optimization process.&lt;/p>
&lt;!-- Similar to NeRF, two networks are trained in conjunction. A training and inference step consists of first creating a rough sampling pattern using our *sampling network*, which learns the object's rough shape. Samp -->
&lt;!--
&lt;figure id="figure-overview-of-the-sampling-network">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Overview of the sampling network." srcset="
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_35d1538a7833f8bc008f37225f81cbca.webp 400w,
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_58e3423051f5c144464c778a5662a18d.webp 760w,
/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/sampling_hu196f0511f68819ecb55f8ee2b8043621_32888_35d1538a7833f8bc008f37225f81cbca.webp"
width="712"
height="340"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Overview of the sampling network.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-overview-of-the-decomposition-network">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Overview of the decomposition network." srcset="
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_23296c69e2599015e5425e33d6db0bfb.webp 400w,
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_a1c6d74b033f6ca373eefbf110124f8a.webp 760w,
/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://markboss.me/publication/2021-nerd/decomposition_huf360bbca0ddfda6b4bca5b3617a09fb5_64813_23296c69e2599015e5425e33d6db0bfb.webp"
width="760"
height="347"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Overview of the decomposition network.
&lt;/figcaption>&lt;/figure>
-->
&lt;!--
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CyC6PutoJO8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
-->
&lt;h3 id="results">Results&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="real-world">Real-world&lt;/h4>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=gnome">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/gnome/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=goldcape">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/goldcape/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h4 id="real-world-preliminary-in-the-wild">Real-world (Preliminary in the wild)&lt;/h4>
&lt;p>The images from the Statue of Liberty are collected from Flickr, Unsplash, Youtube and Vimeo videos. In total about 120 images are used for training from various phones, cameras and drones. Overall even the COLMAP registration is not perfect due to the simplistic shared camera model.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=statue">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/statue/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h5 id="synthetic-examples">Synthetic Examples&lt;/h5>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=car">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/car/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=chair">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/chair/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/nerd-results/render.html?scene=globe">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/nerd-results/assets/models/globe/input.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h3 id="copyright">Copyright&lt;/h3>
&lt;p>This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to &lt;a href="mailto:pubs-permissions@ieee.org">pubs-permissions@ieee.org&lt;/a>.&lt;/p></description></item><item><title>Two-shot Spatially-varying BRDF and Shape Estimation</title><link>https://markboss.me/publication/cvpr20-two-shot-brdf/</link><pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/cvpr20-two-shot-brdf/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CyC6PutoJO8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="results">Results&lt;/h3>
&lt;div class="alert alert-note">
&lt;div>
Click the images for an interactive 3D visualization.
&lt;/div>
&lt;/div>
&lt;h4 id="aksoy-et-al---a-dataset-of-flash-and-ambient-illumination-pairs-from-the-crowd">Aksoy et al. - A Dataset of Flash and Ambient Illumination Pairs from the Crowd&lt;/h4>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf0">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf0/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf1">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf1/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf2">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf2/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf3">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf3/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf4">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf4/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf5">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf5/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf6">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf6/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf7">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf7/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf8">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf8/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf9">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf9/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf10">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf10/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf11">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf11/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf12">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf12/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf13">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf13/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf14">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf14/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=fnf15">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/fnf15/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h5 id="real-world-examples">Real-world Examples&lt;/h5>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw0">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/rw0/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw1">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/rw1/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw2">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/rw2/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=rw3">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/rw3/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h5 id="synthetic-examples">Synthetic Examples&lt;/h5>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn0">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/syn0/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn1">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/syn1/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn2">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/syn2/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox data-type="iframe" data-src="https://markboss.me/files/cvpr20-results/renderer.html?mat=syn3">
&lt;img style="overflow: hidden;" src="https://markboss.me/files/cvpr20-results/predictions/syn3/input_flash.jpg" loading="lazy">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;h3 id="copyright">Copyright&lt;/h3>
&lt;p>This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to &lt;a href="mailto:pubs-permissions@ieee.org">pubs-permissions@ieee.org&lt;/a>.&lt;/p></description></item><item><title>Single Image BRDF Parameter Estimation with a Conditional Adversarial Network</title><link>https://markboss.me/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/single-image-brdf-parameter-estimation-with-conditional-adversarial-network/</guid><description/></item><item><title>Deep Dual Loss BRDF Parameter Estimation</title><link>https://markboss.me/publication/deep-dual-loss-brdf-parameter-estimation/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://markboss.me/publication/deep-dual-loss-brdf-parameter-estimation/</guid><description/></item></channel></rss>